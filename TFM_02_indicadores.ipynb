{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFM_02_indicadores.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal import hilbert, find_peaks  \n",
    "from scipy.stats import skew, kurtosis, norm  \n",
    "import statsmodels.api as sm \n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '{:.12f}'.format(x))\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 0)\n",
    "\n",
    "graficos_dir = 'graficos_output'\n",
    "output_dir = \".\"\n",
    "\n",
    "def is_colab():\n",
    "    \"\"\"Verifica si el entorno es Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def display_df(df):\n",
    "    \"\"\"Muestra un DataFrame de forma diferente seg√∫n el entorno.\"\"\"\n",
    "    if is_colab():\n",
    "        display(df)\n",
    "    else:\n",
    "        print(df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_periods = [5, 10, 15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 446400 entries, 0 to 446399\n",
      "Data columns (total 13 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   timestamp                     446400 non-null  object \n",
      " 1   open                          446400 non-null  float64\n",
      " 2   high                          446400 non-null  float64\n",
      " 3   low                           446400 non-null  float64\n",
      " 4   close                         446400 non-null  float64\n",
      " 5   volume                        446400 non-null  float64\n",
      " 6   close_time                    446400 non-null  float64\n",
      " 7   quote_asset_volume            446400 non-null  float64\n",
      " 8   number_of_trades              446400 non-null  float64\n",
      " 9   taker_buy_base_asset_volume   446400 non-null  float64\n",
      " 10  taker_buy_quote_asset_volume  446400 non-null  float64\n",
      " 11  ignore                        446400 non-null  float64\n",
      " 12  symbol                        446400 non-null  object \n",
      "dtypes: float64(11), object(2)\n",
      "memory usage: 44.3+ MB\n",
      "None\n",
      "                  timestamp            open            high             low           close             volume                 close_time  quote_asset_volume  number_of_trades  taker_buy_base_asset_volume  taker_buy_quote_asset_volume         ignore  symbol\n",
      "371299  2025-02-07 18:47:00 21.490000000000 21.500000000000 21.430000000000 21.430000000000   205.321000000000 1738954079999.000000000000   4403.783540000000   36.000000000000              29.054000000000              624.153840000000 0.000000000000  BANANA\n",
      "371164  2025-02-07 18:46:00  0.024000000000  0.024030000000  0.023980000000  0.023990000000 29307.599999999999 1738954019999.000000000000    703.562228000000   32.000000000000           22826.000000000000              548.028683000000 0.000000000000     PDA\n",
      "179385  2025-02-07 08:00:00  0.189200000000  0.189200000000  0.189100000000  0.189100000000   726.000000000000 1738915259999.000000000000    137.358500000000   10.000000000000              68.000000000000               12.865600000000 0.000000000000     UFT\n",
      "161547  2025-02-07 07:00:00  0.814000000000  0.817000000000  0.814000000000  0.816000000000  4224.500000000000 1738911659999.000000000000   3446.075100000000   20.000000000000            1638.400000000000             1335.874400000000 0.000000000000   SUSHI\n",
      "197644  2025-02-07 09:02:00  0.100700000000  0.100900000000  0.100700000000  0.100900000000  3644.000000000000 1738918979999.000000000000    366.963560000000    4.000000000000            3644.000000000000              366.963560000000 0.000000000000     HFT\n",
      "             timestamp              open              high               low             close              volume  quote_asset_volume  number_of_trades  taker_buy_base_asset_volume  taker_buy_quote_asset_volume symbol\n",
      "0  2025-02-06 23:00:00    0.320800000000    0.321200000000    0.320100000000    0.320100000000 108491.000000000000  34806.689700000003  210.000000000000           56740.000000000000            18217.230400000000    XLM\n",
      "1  2025-02-06 23:00:00    0.959000000000    0.959000000000    0.959000000000    0.959000000000     49.180000000000     47.163620000000    1.000000000000              49.180000000000               47.163620000000    XNO\n",
      "2  2025-02-06 23:00:00    2.336800000000    2.342000000000    2.333200000000    2.333200000000 272381.000000000000 636889.246799999964 1328.000000000000          152942.000000000000           357722.274900000019    XRP\n",
      "3  2025-02-06 23:00:00    0.837000000000    0.837000000000    0.835000000000    0.835000000000   1631.200000000000   1364.070600000000   21.000000000000              32.500000000000               27.195100000000    XTZ\n",
      "4  2025-02-06 23:00:00    0.006119000000    0.006122000000    0.006100000000    0.006100000000  55679.000000000000    340.090235000000   23.000000000000           34868.000000000000              213.051651000000    XVG\n",
      "5  2025-02-06 23:00:00    5.050000000000    5.050000000000    5.040000000000    5.040000000000    143.810000000000    726.033500000000    6.000000000000               0.000000000000                0.000000000000    XVS\n",
      "6  2025-02-06 23:00:00 5734.000000000000 5734.000000000000 5723.000000000000 5723.000000000000      0.128050000000    733.635580000000   17.000000000000               0.046890000000              268.861260000000    YFI\n",
      "7  2025-02-06 23:00:00    0.217600000000    0.217700000000    0.216900000000    0.216900000000   5707.400000000000   1241.230440000000   25.000000000000            2704.900000000000              588.596810000000    YGG\n",
      "8  2025-02-06 23:00:00   30.150000000000   30.180000000000   30.040000000000   30.040000000000    157.143000000000   4724.082140000000   34.000000000000               7.170000000000              216.350200000000    ZEC\n",
      "9  2025-02-06 23:00:00   11.160000000000   11.170000000000   11.130000000000   11.130000000000    251.470000000000   2804.470600000000   34.000000000000               5.780000000000               64.506200000000    ZEN\n",
      "             timestamp              open              high               low             close              volume  quote_asset_volume  number_of_trades  taker_buy_base_asset_volume  taker_buy_quote_asset_volume symbol\n",
      "0  2025-02-06 23:00:00    0.320800000000    0.321200000000    0.320100000000    0.320100000000 108491.000000000000  34806.689700000003  210.000000000000           56740.000000000000            18217.230400000000    XLM\n",
      "1  2025-02-06 23:00:00    0.959000000000    0.959000000000    0.959000000000    0.959000000000     49.180000000000     47.163620000000    1.000000000000              49.180000000000               47.163620000000    XNO\n",
      "2  2025-02-06 23:00:00    2.336800000000    2.342000000000    2.333200000000    2.333200000000 272381.000000000000 636889.246799999964 1328.000000000000          152942.000000000000           357722.274900000019    XRP\n",
      "3  2025-02-06 23:00:00    0.837000000000    0.837000000000    0.835000000000    0.835000000000   1631.200000000000   1364.070600000000   21.000000000000              32.500000000000               27.195100000000    XTZ\n",
      "4  2025-02-06 23:00:00    0.006119000000    0.006122000000    0.006100000000    0.006100000000  55679.000000000000    340.090235000000   23.000000000000           34868.000000000000              213.051651000000    XVG\n",
      "5  2025-02-06 23:00:00    5.050000000000    5.050000000000    5.040000000000    5.040000000000    143.810000000000    726.033500000000    6.000000000000               0.000000000000                0.000000000000    XVS\n",
      "6  2025-02-06 23:00:00 5734.000000000000 5734.000000000000 5723.000000000000 5723.000000000000      0.128050000000    733.635580000000   17.000000000000               0.046890000000              268.861260000000    YFI\n",
      "7  2025-02-06 23:00:00    0.217600000000    0.217700000000    0.216900000000    0.216900000000   5707.400000000000   1241.230440000000   25.000000000000            2704.900000000000              588.596810000000    YGG\n",
      "8  2025-02-06 23:00:00   30.150000000000   30.180000000000   30.040000000000   30.040000000000    157.143000000000   4724.082140000000   34.000000000000               7.170000000000              216.350200000000    ZEC\n",
      "9  2025-02-06 23:00:00   11.160000000000   11.170000000000   11.130000000000   11.130000000000    251.470000000000   2804.470600000000   34.000000000000               5.780000000000               64.506200000000    ZEN\n"
     ]
    }
   ],
   "source": [
    "def format_dataframe_decimals_onload(df):\n",
    "\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        df[col] = df[col].astype(float).apply(lambda x: float('{:.12f}'.format(x)))\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('binance_data/binance_minute_data_20250207.csv')\n",
    "df = format_dataframe_decimals_onload(df)\n",
    "df2 = df.copy()\n",
    "\n",
    "print(df.info())\n",
    "display_df(df.sample(5))\n",
    "\n",
    "df = df.drop(['close_time', 'ignore'], axis=1, errors='ignore') \n",
    "\n",
    "df2 = df2.drop(['close_time', 'ignore'], axis=1, errors='ignore') \n",
    "display_df(df.head(10))\n",
    "display_df(df2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_30_min_window(df, symbol, timestamp):\n",
    "    \"\"\"Obtiene una ventana de 30 minutos alrededor de un timestamp.\"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.to_datetime(timestamp)\n",
    "    symbol_df = df[df['symbol'] == symbol].copy()\n",
    "    symbol_df['timestamp'] = pd.to_datetime(symbol_df['timestamp'])\n",
    "    start_timestamp = timestamp - pd.Timedelta(minutes=30)\n",
    "    df2 = symbol_df[(symbol_df['timestamp'] >= start_timestamp) & \n",
    "                     (symbol_df['timestamp'] <= timestamp)].copy()\n",
    "    df2 = df2.sort_values('timestamp')\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lags_and_leads(df, num_lags, num_leads, group_col='symbol'):\n",
    "    \"\"\"Genera columnas de lags y leads para un DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    lag_cols = ['close', 'volume', 'open', 'high', 'low', 'quote_asset_volume',\n",
    "                'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']\n",
    "    lead_cols = ['close']\n",
    "\n",
    "    with tqdm(total=(num_lags + 1) * len(lag_cols) + (num_leads + 1) * len(lead_cols), desc=\"Generando Lags y Leads\") as pbar:\n",
    "        for i in range(num_lags + 1):\n",
    "            for col in lag_cols:\n",
    "                if col in df.columns:\n",
    "                    df[f'{col}_lag_{i}'] = df.groupby(group_col)[col].transform(lambda x: x.shift(i))\n",
    "                pbar.update(1)  # Mover pbar.update(1) fuera del 'else'\n",
    "\n",
    "        for i in range(num_leads + 1):\n",
    "            for col in lead_cols:\n",
    "                if col in df.columns:\n",
    "                    df[f'{col}_lead_{i}'] = df.groupby(group_col)[col].transform(lambda x: x.shift(-i))\n",
    "                pbar.update(1) # Mover pbar.update(1) fuera del 'else'\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes_row(df, df2, symbol_col='Symbol', timestamp_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Compara una fila de df2 amb la fila corresponent a df que tingui el mateix s√≠mbol i timestamp.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        df2: DataFrame amb una sola fila\n",
    "        symbol_col: Nom de la columna del s√≠mbol\n",
    "        timestamp_col: Nom de la columna del timestamp\n",
    "\n",
    "    Returns:\n",
    "        Llista de columnes que no coincideixen\n",
    "    \"\"\"\n",
    "    print(f\"Nombre de columnes en df: {len(df.columns)}\")\n",
    "    print(f\"Nombre de columnes en df2: {len(df2.columns)}\")\n",
    "\n",
    "    if len(df2) != 1:\n",
    "        return \"Error: df2 ha de tenir exactament una fila\"\n",
    "\n",
    "    symbol = df2[symbol_col].iloc[0]\n",
    "    timestamp = df2[timestamp_col].iloc[0]\n",
    "\n",
    "    matching_row = df[(df[symbol_col] == symbol) & (df[timestamp_col] == timestamp)]\n",
    "\n",
    "    if len(matching_row) == 0:\n",
    "        return f\"No s'ha trobat cap fila a df amb {symbol_col}={symbol} i {timestamp_col}={timestamp}\"\n",
    "\n",
    "    if len(matching_row) > 1:\n",
    "        print(f\"Advert√®ncia: S'han trobat m√∫ltiples files a df amb {symbol_col}={symbol} i {timestamp_col}={timestamp}\")\n",
    "\n",
    "    df_index = matching_row.index[0]\n",
    "    df2_index = df2.index[0]\n",
    "\n",
    "    print(f\"√çndex a df: {df_index}\")\n",
    "    print(f\"√çndex a df2: {df2_index}\")\n",
    "\n",
    "    columns_to_compare = [col for col in df2.columns if col in matching_row.columns]\n",
    "\n",
    "    print(f\"Nombre de columnes comunes a comparar: {len(columns_to_compare)}\")\n",
    "\n",
    "    mismatched_columns = []\n",
    "\n",
    "    for col in columns_to_compare:\n",
    "        df_value = matching_row[col].iloc[0]\n",
    "        df2_value = df2[col].iloc[0]\n",
    "\n",
    "        # Special handling for boolean values\n",
    "        if isinstance(df_value, bool) and isinstance(df2_value, bool):\n",
    "            if df_value != df2_value:\n",
    "                mismatched_columns.append({\n",
    "                    'columna': col,\n",
    "                    'valor_df': df_value,\n",
    "                    'valor_df2': df2_value,\n",
    "                    'difer√®ncia': 'N/A (Boolean)'  # Cannot compute difference for booleans\n",
    "                })\n",
    "        # For numeric values\n",
    "        elif pd.api.types.is_numeric_dtype(matching_row[col]) and pd.api.types.is_numeric_dtype(df2[col]):\n",
    "            # Convert to standard Python types to avoid numpy type issues\n",
    "            try:\n",
    "                df_value_float = float(df_value)\n",
    "                df2_value_float = float(df2_value)\n",
    "                \n",
    "                if not np.isclose(df_value_float, df2_value_float, rtol=1e-10, atol=1e-10):\n",
    "                    mismatched_columns.append({\n",
    "                        'columna': col,\n",
    "                        'valor_df': df_value,\n",
    "                        'valor_df2': df2_value,\n",
    "                        'difer√®ncia': abs(df_value_float - df2_value_float)\n",
    "                    })\n",
    "            except (TypeError, ValueError):\n",
    "                # If conversion fails, just compare for equality\n",
    "                if df_value != df2_value:\n",
    "                    mismatched_columns.append({\n",
    "                        'columna': col,\n",
    "                        'valor_df': df_value,\n",
    "                        'valor_df2': df2_value\n",
    "                    })\n",
    "        # For all other types\n",
    "        else:\n",
    "            if df_value != df2_value:\n",
    "                mismatched_columns.append({\n",
    "                    'columna': col,\n",
    "                    'valor_df': df_value,\n",
    "                    'valor_df2': df2_value\n",
    "                })\n",
    "\n",
    "    if not mismatched_columns:\n",
    "        print(\"Totes les columnes coincideixen exactament (excloent l'√≠ndex)\")\n",
    "        return []\n",
    "    else:\n",
    "        print(f\"S'han trobat {len(mismatched_columns)} columnes que no coincideixen:\")\n",
    "        for mismatch in mismatched_columns:\n",
    "            if 'difer√®ncia' in mismatch:\n",
    "                print(f\"  - {mismatch['columna']}: df={mismatch['valor_df']}, df2={mismatch['valor_df2']}, difer√®ncia={mismatch['difer√®ncia']}\")\n",
    "            else:\n",
    "                print(f\"  - {mismatch['columna']}: df={mismatch['valor_df']}, df2={mismatch['valor_df2']}\")\n",
    "        return [m['columna'] for m in mismatched_columns]\n",
    "\n",
    "symbol_col = 'Symbol' if 'Symbol' in df.columns else 'symbol'\n",
    "timestamp_col = 'timestamp' if 'timestamp' in df.columns else 'Timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMA(df, periods, column='close_lag_', plot=True, symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "        end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calcula les SMAs i les representa gr√†ficament, amb opcions per a tot el dia o un interval de temps.\n",
    "    Aplica estils espec√≠fics (SMA_5 blau, SMA_30 vermell, rangeslider, llegenda a la part inferior central, 1000x300).\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame amb 'timestamp', 'symbol' i columnes de lags.\n",
    "        periods: Llista de per√≠odes per a les SMA.\n",
    "        column: Prefix de les columnes de lags.\n",
    "        plot: Indica si es genera un gr√†fic.\n",
    "        symbol: S√≠mbol a representar (per filtrar i per al t√≠tol).\n",
    "        plot_type: 'all_day' o 'time_range'.\n",
    "        start_time: Hora d'inici per a 'time_range' (objecte datetime o string 'HH:MM').\n",
    "        end_time: Hora de finalitzaci√≥ per a 'time_range' (objecte datetime o string 'HH:MM').\n",
    "        width: Amplada del gr√†fic.\n",
    "        height: Al√ßada del gr√†fic.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame amb les columnes SMA afegides.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"Av√≠s: Columna 'timestamp' no trobada. Els gr√†fics no funcionaran.\")\n",
    "        plot = False\n",
    "    elif not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        except Exception as e:\n",
    "            print(f\"Av√≠s: No s'ha pogut convertir 'timestamp': {e}. Gr√†fics desactivats.\")\n",
    "            plot = False\n",
    "\n",
    "    df[f'{column}0'] = df['close']  # Afegir el preu de tancament com a lag 0\n",
    "\n",
    "    for period in periods:\n",
    "        lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "        if not all(col in df.columns for col in lag_cols):\n",
    "            print(f\"Av√≠s: Falten columnes de lags per al per√≠ode SMA {period}. S'omet SMA_{period}.\")\n",
    "            continue\n",
    "        df[f'SMA_{period}'] = df[lag_cols].mean(axis=1)\n",
    "\n",
    "    if plot:\n",
    "        plot_filename = f'SMA_symbol_{symbol}'\n",
    "        plot_df = df\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time and symbol:\n",
    "            if isinstance(start_time, str):\n",
    "                start_time = pd.to_datetime(start_time).time()\n",
    "            if isinstance(end_time, str):\n",
    "                end_time = pd.to_datetime(end_time).time()\n",
    "\n",
    "            if pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "                plot_df = df[\n",
    "                    (df['timestamp'].dt.time >= start_time) &\n",
    "                    (df['timestamp'].dt.time <= end_time) &\n",
    "                    (df['symbol'] == symbol)\n",
    "                ]\n",
    "                plot_filename = f'SMA_symbol_{symbol}_time_range_{start_time.strftime(\"%H-%M\")}-{end_time.strftime(\"%H-%M\")}'\n",
    "            else:\n",
    "                 print(\"Av√≠s: No es pot filtrar per interval de temps. 'timestamp' no √©s de tipus datetime.\")\n",
    "\n",
    "        elif plot_type == 'all_day' and symbol:\n",
    "            plot_df = df[df['symbol'] == symbol]  \n",
    "            plot_filename = f'SMA_symbol_{symbol}_all_day'\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            fig_sma = go.Figure()\n",
    "\n",
    "            fig_sma.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df['close'],\n",
    "                                        name='Precio de Cierre', line=dict(color='black')))\n",
    "\n",
    "            for period in periods:\n",
    "                if f'SMA_{period}' in plot_df.columns:\n",
    "                    color = 'blue' if period == 5 else 'red' if period == 30 else 'green'\n",
    "                    fig_sma.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df[f'SMA_{period}'],\n",
    "                                              name=f'SMA-{period}', line=dict(color=color)))\n",
    "            title_suffix = \"\"\n",
    "            if \"time_range\" in plot_filename:\n",
    "                title_suffix = f\" - Intervalo de Tiempo {plot_filename.split('time_range_')[1]}\"\n",
    "            elif \"all_day\" in plot_filename:\n",
    "                title_suffix = \" - Todo el D√≠a\"\n",
    "\n",
    "\n",
    "            fig_sma.update_layout(\n",
    "                title={\n",
    "                    'text': f'<b>SMAs para {symbol}{title_suffix}</b>',\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                },\n",
    "                xaxis_title=dict(text='<b>Timestamp</b>', standoff=10),\n",
    "                yaxis_title= dict(text='<b>Valor</b>', standoff=10),\n",
    "                showlegend=True,\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.28,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5\n",
    "                ),\n",
    "                xaxis_rangeslider_visible=True,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                margin=dict(b=150),\n",
    "            )\n",
    "            javascript_code = \"\"\"\n",
    "            var graphDiv = document.currentScript.parentElement;\n",
    "            graphDiv.on('plotly_legendclick', function(eventdata) {\n",
    "                Plotly.relayout(graphDiv, {\n",
    "                    'yaxis.autorange': true\n",
    "                });\n",
    "                return false; // Prevent default behavior\n",
    "            });\n",
    "            \"\"\"\n",
    "\n",
    "            graficos_dir = \"graficos\"\n",
    "            if not os.path.exists(graficos_dir):\n",
    "                os.makedirs(graficos_dir)\n",
    "\n",
    "\n",
    "            plot_filepath = os.path.join(graficos_dir, f'{plot_filename}.html')\n",
    "            fig_sma.write_html(plot_filepath, auto_open=False, post_script=javascript_code)\n",
    "\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                is_colab_env = True\n",
    "            except ImportError:\n",
    "                is_colab_env = False\n",
    "\n",
    "            if is_colab_env:\n",
    "                fig_sma.show()\n",
    "            else:\n",
    "                print(f\"Gr√°fico SMA guardado en {plot_filepath}\")\n",
    "        else:\n",
    "            print(f\"Av√≠s: No hi ha dades per representar per al s√≠mbol {symbol} i plot_type {plot_type}.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMA(df, periods, column='close_lag_', plot=True, symbol='STEEM', plot_type='all_day',\n",
    "        start_time=None, end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calcula EMAs utilitzant nom√©s els valors de lag disponibles a cada fila.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Verificacions de robustesa\n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"Av√≠s: columna 'timestamp' no trobada. Els gr√†fics no funcionaran.\")\n",
    "        plot = False\n",
    "    elif not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        except Exception as e:\n",
    "            print(f\"Av√≠s: No s'ha pogut convertir 'timestamp': {e}. Gr√†fics desactivats.\")\n",
    "            plot = False\n",
    "\n",
    "    df[f'{column}0'] = df['close']\n",
    "\n",
    "    # Calcular EMAs utilitzant nom√©s lags disponibles\n",
    "    # C√†lcul de les EMAs utilitzant nom√©s els lags disponibles\n",
    "    for period in periods:\n",
    "        lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "        if not all(col in df.columns for col in lag_cols):\n",
    "            print(f\"Av√≠s: Falten columnes lag per al per√≠ode EMA {period}. S'omet EMA_{period}.\")\n",
    "            continue\n",
    "\n",
    "        # Calculem pesos exponencials\n",
    "        alpha = 2 / (period + 1)\n",
    "        weights = np.array([(1-alpha)**i for i in range(period)])\n",
    "        weights = weights / weights.sum()  # Normalitzar pesos\n",
    "\n",
    "        # Calcular EMA utilitzant pesos i lags disponibles\n",
    "        df[f'EMA_{period}'] = np.sum([df[col] * weights[i] for i, col in enumerate(lag_cols)], axis=0)\n",
    "\n",
    "    # --- Gr√†fics (nom√©s per al s√≠mbol especificat) ---\n",
    "    if plot:\n",
    "        plot_filename = f'EMA_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol]\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str):\n",
    "                start_time = pd.to_datetime(start_time).time()\n",
    "            if isinstance(end_time, str):\n",
    "                end_time = pd.to_datetime(end_time).time()\n",
    "\n",
    "            if pd.api.types.is_datetime64_any_dtype(plot_df['timestamp']):\n",
    "                plot_df = plot_df[\n",
    "                    (plot_df['timestamp'].dt.time >= start_time) &\n",
    "                    (plot_df['timestamp'].dt.time <= end_time)\n",
    "                ]\n",
    "                plot_filename = f'EMA_symbol_{symbol}_time_range_{start_time.strftime(\"%H-%M\")}-{end_time.strftime(\"%H-%M\")}'\n",
    "            else:\n",
    "                print(\"Av√≠s: No es pot filtrar per rang de temps. 'timestamp' no √©s datetime.\")\n",
    "\n",
    "        elif plot_type == 'all_day':\n",
    "            plot_filename = f'EMA_symbol_{symbol}_all_day'\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            fig_ema = go.Figure()\n",
    "            fig_ema.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df['close'],\n",
    "                                        name='Close Price', line=dict(color='black')))\n",
    "\n",
    "            for period in periods:\n",
    "                if f'EMA_{period}' in plot_df.columns:\n",
    "                    color = 'blue' if period == 5 else 'red' if period == 30 else 'green'\n",
    "                    fig_ema.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df[f'EMA_{period}'],\n",
    "                                              name=f'EMA-{period}', line=dict(color=color)))\n",
    "\n",
    "            title_suffix = \"\"\n",
    "            if \"time_range\" in plot_filename:\n",
    "                title_suffix = f\" - Time Range {plot_filename.split('time_range_')[1]}\"\n",
    "            elif \"all_day\" in plot_filename:\n",
    "                title_suffix = \" - All Day\"\n",
    "\n",
    "            fig_ema.update_layout(\n",
    "                title={\n",
    "                    'text': f'<b>EMAs para {symbol}{title_suffix}</b>',\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                },\n",
    "                xaxis_title=dict(text='<b>Timestamp</b>', standoff=10),\n",
    "                yaxis_title=dict(text='<b>Value</b>', standoff=10),\n",
    "                showlegend=True,\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.28,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5\n",
    "                ),\n",
    "                xaxis_rangeslider_visible=True,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                margin=dict(b=150),\n",
    "            )\n",
    "            javascript_code = \"\"\"\n",
    "            var graphDiv = document.currentScript.parentElement;\n",
    "            graphDiv.on('plotly_legendclick', function(eventdata) {\n",
    "                Plotly.relayout(graphDiv, {\n",
    "                    'yaxis.autorange': true\n",
    "                });\n",
    "                return false;\n",
    "            });\n",
    "            \"\"\"\n",
    "\n",
    "            graficos_dir = \"graficos\"\n",
    "            if not os.path.exists(graficos_dir):\n",
    "                os.makedirs(graficos_dir)\n",
    "\n",
    "            plot_filepath = os.path.join(graficos_dir, f'{plot_filename}.html')\n",
    "            fig_ema.write_html(plot_filepath, auto_open=False, post_script=javascript_code)\n",
    "\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                is_colab_env = True\n",
    "            except ImportError:\n",
    "                is_colab_env = False\n",
    "\n",
    "            if is_colab_env:\n",
    "                fig_ema.show()\n",
    "            else:\n",
    "                print(f\"Gr√†fic EMA desat a {plot_filepath}\")\n",
    "        else:\n",
    "            print(f\"Av√≠s: No hi ha dades per graficar per al s√≠mbol {symbol} i plot_type {plot_type}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wma(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calcula la WMA utilitzant operacions vectoritzades amb numpy broadcasting.\n",
    "    Coincideix amb la l√≤gica de c√†lcul original on els pesos augmenten d'1 a period.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if f'{column}0' not in df.columns:\n",
    "        df[f'{column}0'] = df['close']\n",
    "\n",
    "    symbols = df['symbol'].unique()\n",
    "    total_iterations = len(symbols) * len(periods)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"Calculating WMAs\") as pbar:\n",
    "        for period in periods:\n",
    "            # Comprova si la WMA per aquest per√≠ode ja existeix\n",
    "            if f'WMA_{period}' in df.columns:\n",
    "                pbar.update(len(symbols))\n",
    "                continue\n",
    "\n",
    "            # Inicialitza la columna WMA\n",
    "            df[f'WMA_{period}'] = np.nan\n",
    "\n",
    "            # Obt√© totes les columnes lag necess√†ries per a aquest per√≠ode\n",
    "            lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "\n",
    "            # Comprova si totes les columnes lag necess√†ries existeixen\n",
    "            if not all(col in df.columns for col in lag_cols):\n",
    "                print(f\"Av√≠s: Falten columnes lag per al per√≠ode WMA {period}. S'omet aquest per√≠ode.\")\n",
    "                pbar.update(len(symbols))\n",
    "                continue\n",
    "\n",
    "            # Defineix els pesos (d'1 a period) - COINCIDEIX AMB EL C√ÄLCUL ORIGINAL\n",
    "            weights = np.arange(1, period + 1)\n",
    "            weight_sum = weights.sum()\n",
    "\n",
    "            # Processa cada s√≠mbol utilitzant operacions vectoritzades\n",
    "            for symbol in symbols:\n",
    "                symbol_mask = df['symbol'] == symbol\n",
    "\n",
    "                # Crea una matriu de tots els valors lag per a aquest s√≠mbol\n",
    "                lag_matrix = df.loc[symbol_mask, lag_cols].values\n",
    "\n",
    "                # Identifica les files on tots els valors lag estan presents\n",
    "                valid_rows = ~np.isnan(lag_matrix).any(axis=1)\n",
    "\n",
    "                if not np.any(valid_rows):\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Calcula la WMA utilitzant operacions vectoritzades de numpy (producte punt)\n",
    "                wma_values = np.sum(lag_matrix[valid_rows] * weights, axis=1) / weight_sum\n",
    "\n",
    "                # Obt√© els √≠ndexs de les files v√†lides per a aquest s√≠mbol\n",
    "                valid_indices = df.index[symbol_mask][valid_rows]\n",
    "\n",
    "                # Assigna els valors WMA calculats de nou al DataFrame\n",
    "                df.loc[valid_indices, f'WMA_{period}'] = wma_values\n",
    "                pbar.update(1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def WMA(df, periods, column='close_lag_', plot=True, symbol='STEEM', plot_type='all_day',\n",
    "        start_time=None, end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates WMAs efficiently using fully vectorized approach.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with price data\n",
    "        periods: List of WMA periods to calculate\n",
    "        column: Base column prefix for lag data\n",
    "        plot: Whether to create plots\n",
    "        symbol: Symbol to plot\n",
    "        plot_type: 'all_day' or 'time_range'\n",
    "        start_time/end_time: For time_range plots\n",
    "        width/height: Plot dimensions\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Comprovacions de robustesa ---\n",
    "    if df.empty:\n",
    "        print(\"Av√≠s: DataFrame est√† buit. No es pot calcular la WMA. Retornant DataFrame buit.\")\n",
    "        return df\n",
    "\n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"Av√≠s: columna 'timestamp' no trobada. Els gr√†fics no funcionaran.\")\n",
    "        plot = False\n",
    "    elif not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        except Exception as e:\n",
    "            print(f\"Av√≠s: No s'ha pogut convertir 'timestamp': {e}. Gr√†fics desactivats.\")\n",
    "            plot = False\n",
    "\n",
    "    # Assegura't que existeix close_lag_0\n",
    "    df[f'{column}0'] = df['close']\n",
    "\n",
    "    # Calcula la WMA\n",
    "    # Modifica df directament\n",
    "    df = calculate_wma(df, periods, column)\n",
    "\n",
    "    # --- Gr√†fics (nom√©s per al s√≠mbol especificat) ---\n",
    "    if plot:\n",
    "        plot_filename = f'WMA_symbol_{symbol}'\n",
    "        # Utilitza df, no result_df\n",
    "        plot_df = df[df['symbol'] == symbol]\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Filtratge de temps simplificat (si start/end_time s√≥n strings, assumeix format HH:MM)\n",
    "            if isinstance(start_time, str):\n",
    "                start_time = pd.to_datetime(start_time).time()\n",
    "            if isinstance(end_time, str):\n",
    "                end_time = pd.to_datetime(end_time).time()\n",
    "\n",
    "            if pd.api.types.is_datetime64_any_dtype(plot_df['timestamp']):\n",
    "                plot_df = plot_df[\n",
    "                    (plot_df['timestamp'].dt.time >= start_time) &\n",
    "                    (plot_df['timestamp'].dt.time <= end_time)\n",
    "                ]\n",
    "                plot_filename = f'WMA_symbol_{symbol}_time_range_{start_time.strftime(\"%H-%M\")}-{end_time.strftime(\"%H-%M\")}'\n",
    "            else:\n",
    "                print(\"Av√≠s: No es pot filtrar per rang de temps. 'timestamp' no √©s datetime.\")\n",
    "\n",
    "        elif plot_type == 'all_day':\n",
    "            plot_filename = f'WMA_symbol_{symbol}_all_day'\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            fig_wma = go.Figure()\n",
    "            fig_wma.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df['close'],\n",
    "                                        name='Close Price', line=dict(color='black')))\n",
    "\n",
    "            for period in periods:\n",
    "                if f'WMA_{period}' in plot_df.columns:\n",
    "                    color = 'blue' if period == 5 else 'red' if period == 30 else 'green'\n",
    "                    fig_wma.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df[f'WMA_{period}'],\n",
    "                                              name=f'WMA-{period}', line=dict(color=color)))\n",
    "\n",
    "            title_suffix = \"\"\n",
    "            if \"time_range\" in plot_filename:\n",
    "                title_suffix = f\" - Time Range {plot_filename.split('time_range_')[1]}\"\n",
    "            elif \"all_day\" in plot_filename:\n",
    "                title_suffix = \" - All Day\"\n",
    "\n",
    "            fig_wma.update_layout(\n",
    "                title={\n",
    "                    'text': f'<b>WMAs para {symbol}{title_suffix}</b>',\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                },\n",
    "                xaxis_title=dict(text='<b>Timestamp</b>', standoff=10),\n",
    "                yaxis_title=dict(text='<b>Value</b>', standoff=10),\n",
    "                showlegend=True,\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.28,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5\n",
    "                ),\n",
    "                xaxis_rangeslider_visible=True,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                margin=dict(b=150),\n",
    "            )\n",
    "            javascript_code = \"\"\"\n",
    "            var graphDiv = document.currentScript.parentElement;\n",
    "            graphDiv.on('plotly_legendclick', function(eventdata) {\n",
    "                Plotly.relayout(graphDiv, {\n",
    "                    'yaxis.autorange': true\n",
    "                });\n",
    "                return false;\n",
    "            });\n",
    "            \"\"\"\n",
    "\n",
    "            graficos_dir = \"graficos\"\n",
    "            if not os.path.exists(graficos_dir):\n",
    "                os.makedirs(graficos_dir)\n",
    "\n",
    "            plot_filepath = os.path.join(graficos_dir, f'{plot_filename}.html')\n",
    "            fig_wma.write_html(plot_filepath, auto_open=False, post_script=javascript_code)\n",
    "\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                is_colab_env = True\n",
    "            except ImportError:\n",
    "                is_colab_env = False\n",
    "\n",
    "            if is_colab_env:\n",
    "                fig_wma.show()\n",
    "            else:\n",
    "                print(f\"Gr√†fic WMA desat a {plot_filepath}\")\n",
    "        else:\n",
    "            print(f\"Av√≠s: No hi ha dades per graficar per al s√≠mbol {symbol} i plot_type {plot_type}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi_from_lags(df, period, column='close_lag_'):\n",
    "    \"\"\"Calcula RSI usando solo los valores de lag disponibles en cada fila.\"\"\"\n",
    "    # Verificar columnas de lag necesarias\n",
    "    lag_cols = [f'{column}{i}' for i in range(period + 1)]\n",
    "    if not all(col in df.columns for col in lag_cols):\n",
    "        print(f\"Warning: Missing lag columns for RSI period {period}. Skipping.\")\n",
    "        return pd.Series(index=df.index)\n",
    "    \n",
    "    # Inicializar Series para almacenar resultados\n",
    "    rsi = pd.Series(index=df.index)\n",
    "    \n",
    "    # Calcular ganancias/p√©rdidas usando pares de precios de lags consecutivos\n",
    "    diffs = []\n",
    "    for i in range(period):\n",
    "        # Diferencia entre cada par de precios consecutivos\n",
    "        diff = df[f'{column}{i}'] - df[f'{column}{i+1}']\n",
    "        diffs.append(diff)\n",
    "    \n",
    "    # Convertir a DataFrame para facilitar c√°lculos\n",
    "    diffs_df = pd.DataFrame(diffs).T\n",
    "    \n",
    "    # Separar ganancias (valores positivos) y p√©rdidas (valores negativos)\n",
    "    gains = diffs_df.clip(lower=0)\n",
    "    losses = -diffs_df.clip(upper=0)  # Convertir p√©rdidas a valores positivos\n",
    "    \n",
    "    # Calcular promedios simples de ganancias y p√©rdidas\n",
    "    avg_gains = gains.mean(axis=1)\n",
    "    avg_losses = losses.mean(axis=1)\n",
    "    \n",
    "    # Evitar divisi√≥n por cero en p√©rdidas\n",
    "    avg_losses = avg_losses.replace(0, 0.000001)\n",
    "    \n",
    "    # Calcular RS y RSI\n",
    "    rs = avg_gains / avg_losses\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "def RSI(df, periods, column='close_lag_', plot=True, symbol='STEEM', plot_type='all_day',\n",
    "        start_time=None, end_time=None, width=1000, height=500):\n",
    "    \"\"\"Versi√≥n modificada del RSI que usa solo informaci√≥n disponible en cada fila.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Verificaciones de robustez (igual que antes)\n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"Warning: 'timestamp' column not found. Plots will not work.\")\n",
    "        plot = False\n",
    "    elif not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert 'timestamp': {e}. Plots disabled.\")\n",
    "            plot = False\n",
    "    \n",
    "    df[f'{column}0'] = df['close']\n",
    "    \n",
    "    # Calcular RSI para cada per√≠odo\n",
    "    with tqdm(total=len(periods), desc=\"Calculating RSI\") as pbar:\n",
    "        for period in periods:\n",
    "            if f'RSI_{period}' in df.columns:\n",
    "                print(f\"Skipping RSI_{period}: Already exists.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Procesamos cada s√≠mbolo independientemente para evitar problemas de √≠ndice\n",
    "            symbols = df['symbol'].unique()\n",
    "            \n",
    "            # Crear Series vac√≠a con el mismo √≠ndice que el DataFrame original\n",
    "            df[f'RSI_{period}'] = pd.Series(index=df.index, dtype=float)\n",
    "            \n",
    "            for sym in symbols:\n",
    "                # Obtener filas para este s√≠mbolo\n",
    "                symbol_mask = df['symbol'] == sym\n",
    "                \n",
    "                # Calcular RSI solo para este s√≠mbolo\n",
    "                symbol_rsi = calculate_rsi_from_lags(df[symbol_mask], period, column)\n",
    "                \n",
    "                # Asignar valores al DataFrame original con m√°scara de √≠ndice\n",
    "                df.loc[symbol_mask, f'RSI_{period}'] = symbol_rsi.values\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot:\n",
    "        plot_filename = f'RSI_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol]\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str):\n",
    "                start_time = pd.to_datetime(start_time).time()\n",
    "            if isinstance(end_time, str):\n",
    "                end_time = pd.to_datetime(end_time).time()\n",
    "\n",
    "            if pd.api.types.is_datetime64_any_dtype(plot_df['timestamp']):\n",
    "                plot_df = plot_df[\n",
    "                    (plot_df['timestamp'].dt.time >= start_time) &\n",
    "                    (plot_df['timestamp'].dt.time <= end_time)\n",
    "                ]\n",
    "                plot_filename = f'RSI_symbol_{symbol}_time_range_{start_time.strftime(\"%H-%M\")}-{end_time.strftime(\"%H-%M\")}'\n",
    "            else:\n",
    "                print(\"Warning: Cannot filter by time range. 'timestamp' is not datetime.\")\n",
    "\n",
    "        elif plot_type == 'all_day':\n",
    "            plot_filename = f'RSI_symbol_{symbol}_all_day'\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            fig_rsi = go.Figure()\n",
    "            #fig_rsi.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df['close'],\n",
    "             #                           name='Close Price', line=dict(color='black')))\n",
    "\n",
    "            for period in periods:\n",
    "                if f'RSI_{period}' in plot_df.columns:\n",
    "                    color = 'blue' if period == 5 else 'red' if period == 30 else 'green'\n",
    "                    fig_rsi.add_trace(go.Scatter(x=plot_df['timestamp'], y=plot_df[f'RSI_{period}'],\n",
    "                                              name=f'RSI-{period}', line=dict(color=color)))\n",
    "\n",
    "            title_suffix = \"\"\n",
    "            if \"time_range\" in plot_filename:\n",
    "                title_suffix = f\" - Time Range {plot_filename.split('time_range_')[1]}\"\n",
    "            elif \"all_day\" in plot_filename:\n",
    "                title_suffix = \" - All Day\"\n",
    "\n",
    "            fig_rsi.update_layout(\n",
    "                title={\n",
    "                    'text': f'<b>RSIs for {symbol}{title_suffix}</b>',\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                },\n",
    "                xaxis_title=dict(text='<b>Timestamp</b>', standoff=10),\n",
    "                yaxis_title=dict(text='<b>Value</b>', standoff=10),\n",
    "                showlegend=True,\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.28,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5\n",
    "                ),\n",
    "                xaxis_rangeslider_visible=True,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                margin=dict(b=150),\n",
    "            )\n",
    "            javascript_code = \"\"\"\n",
    "            var graphDiv = document.currentScript.parentElement;\n",
    "            graphDiv.on('plotly_legendclick', function(eventdata) {\n",
    "                Plotly.relayout(graphDiv, {\n",
    "                    'yaxis.autorange': true\n",
    "                });\n",
    "                return false;\n",
    "            });\n",
    "            \"\"\"\n",
    "\n",
    "            graficos_dir = \"graficos\"\n",
    "            if not os.path.exists(graficos_dir):\n",
    "                os.makedirs(graficos_dir)\n",
    "\n",
    "            plot_filepath = os.path.join(graficos_dir, f'{plot_filename}.html')\n",
    "            fig_rsi.write_html(plot_filepath, auto_open=False, post_script=javascript_code)\n",
    "\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                is_colab_env = True\n",
    "            except ImportError:\n",
    "                is_colab_env = False\n",
    "\n",
    "            if is_colab_env:\n",
    "                fig_rsi.show()\n",
    "            else:\n",
    "                print(f\"RSI plot saved to {plot_filepath}\")\n",
    "        else:\n",
    "            print(f\"Warning: No data to plot for symbol {symbol} and plot_type {plot_type}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stochastic_oscillator(df, periods, column):\n",
    "    \"\"\"Calcula el Oscilador Estoc√°stico usando solo datos disponibles en cada fila y evita generar NaNs.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        k_col_name = f'Stochastic_K_{period}'\n",
    "        d_col_name = f'Stochastic_D_{period}'\n",
    "\n",
    "        if k_col_name in df.columns and d_col_name in df.columns:\n",
    "            continue  # Saltar si ya est√° calculado\n",
    "\n",
    "        high_lag_cols = [f'high_lag_{i}' for i in range(period)]\n",
    "        low_lag_cols = [f'low_lag_{i}' for i in range(period)]\n",
    "\n",
    "        # Verificar que todas las columnas necesarias existen\n",
    "        missing_high = [col for col in high_lag_cols if col not in df.columns]\n",
    "        missing_low = [col for col in low_lag_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_high or missing_low:\n",
    "            print(f\"Warning: Missing lag columns for period {period}\")\n",
    "            \n",
    "            # Crear las columnas faltantes si es necesario\n",
    "            for i in range(period):\n",
    "                if f'high_lag_{i}' not in df.columns and 'high' in df.columns:\n",
    "                    df[f'high_lag_{i}'] = df['high'].shift(i)\n",
    "                if f'low_lag_{i}' not in df.columns and 'low' in df.columns:\n",
    "                    df[f'low_lag_{i}'] = df['low'].shift(i)\n",
    "        \n",
    "        # Actualiza lista de columnas despu√©s de la posible creaci√≥n.\n",
    "        high_lag_cols = [col for col in [f'high_lag_{i}' for i in range(period)] if col in df.columns]\n",
    "        low_lag_cols = [col for col in [f'low_lag_{i}' for i in range(period)] if col in df.columns]\n",
    "\n",
    "        # Si no hay suficientes columnas de lag, usar las disponibles\n",
    "        if len(high_lag_cols) == 0 or len(low_lag_cols) == 0:\n",
    "            print(f\"Error: No high_lag or low_lag columns available for period {period}\")\n",
    "            df[k_col_name] = 50.0  # Valor por defecto central\n",
    "            df[d_col_name] = 50.0\n",
    "            continue\n",
    "\n",
    "        # C√°lculo vectorizado de m√°ximo y m√≠nimo\n",
    "        highest_high = df[high_lag_cols].max(axis=1)\n",
    "        lowest_low = df[low_lag_cols].min(axis=1)\n",
    "\n",
    "        # C√°lculo de %K, manejando la divisi√≥n por cero\n",
    "        range_hl = highest_high - lowest_low\n",
    "        df[k_col_name] = np.where(\n",
    "            range_hl > 0,\n",
    "            100 * (df[f'{column}0'] - lowest_low) / range_hl,\n",
    "            50.0  # Valor medio si el rango es cero\n",
    "        )\n",
    "        \n",
    "        # Para %D usamos el valor K actual como todos los valores de D\n",
    "        # Esto evita NaNs cuando solo tenemos una fila\n",
    "        if len(df) == 1:\n",
    "            # Si solo hay una fila, usamos el mismo valor K para D\n",
    "            df[d_col_name] = df[k_col_name]\n",
    "        else:\n",
    "            # Preparamos columnas para el c√°lculo normal de %D\n",
    "            k_cols = []\n",
    "            \n",
    "            # Usamos el valor actual de K\n",
    "            k_cols.append(k_col_name)\n",
    "            \n",
    "            # A√±adimos dos valores adicionales (pueden ser iguales al actual si no hay datos)\n",
    "            # Esto asegura que siempre tenemos 3 valores para el promedio de %D\n",
    "            k_cols.append(k_col_name)\n",
    "            k_cols.append(k_col_name)\n",
    "            \n",
    "            # Calculamos %D como el promedio\n",
    "            df[d_col_name] = df[k_cols].mean(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def StochasticOscillator(df, periods, column='close_lag_', plot=True, symbol='STEEM', plot_type='all_day',\n",
    "        start_time=None, end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates the Stochastic Oscillator technical indicator for multiple periods and optionally plots it.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data with a 'symbol' column and lag columns.\n",
    "        periods (list): List of integer periods for Stochastic Oscillator calculation.\n",
    "        column (str, optional): The prefix for the lag columns. Defaults to 'close_lag_'.\n",
    "        plot (bool, optional): Whether to generate a plot. Defaults to True.\n",
    "        symbol (str, optional): The symbol to plot. Defaults to 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' or 'time_range'. Defaults to 'all_day'.\n",
    "        start_time (str, optional): Start time for 'time_range' plot (HH:MM:SS). Defaults to None.\n",
    "        end_time (str, optional): End time for 'time_range' plot (HH:MM:SS). Defaults to None.\n",
    "        width (int): Figure width.\n",
    "        height (int): Figure height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added Stochastic Oscillator columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0 if not exists\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'close' in df.columns:\n",
    "            df[f'{column}0'] = df['close']\n",
    "        else:\n",
    "            print(\"Close column required\")\n",
    "            return df\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    # Asegurar que existen todas las columnas de lag necesarias\n",
    "    for i in range(max(periods)):\n",
    "        if f'high_lag_{i}' not in df.columns and 'high' in df.columns:\n",
    "            df[f'high_lag_{i}'] = df['high'].shift(i)\n",
    "        if f'low_lag_{i}' not in df.columns and 'low' in df.columns:\n",
    "            df[f'low_lag_{i}'] = df['low'].shift(i)\n",
    "\n",
    "    # Calculate Stochastic Oscillator\n",
    "    df = calculate_stochastic_oscillator(df, periods, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns and not df.empty:\n",
    "        plot_filename = f'Stochastic_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return df\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) & (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'Stochastic_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        # Skip plotting if no data after filtering\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for plotting after time range filter\")\n",
    "            return df\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add Stochastic Oscillator traces\n",
    "        for period in periods:\n",
    "            k_col_name = f'Stochastic_K_{period}'\n",
    "            d_col_name = f'Stochastic_D_{period}'\n",
    "\n",
    "            if k_col_name in plot_df.columns and d_col_name in plot_df.columns:\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[k_col_name], mode='lines', name=f'%K ({period})', line=dict(color=color)))\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[d_col_name], mode='lines', name=f'%D ({period})', line=dict(color=color, dash='dash')))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Stochastic Oscillator for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title=dict(text='<b>Timestamp</b>', standoff=10),\n",
    "            yaxis_title=dict(text='<b>Value</b>', standoff=10),\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            fig.write_html(os.path.join('graficos', f'{plot_filename}.html'), auto_open=False)\n",
    "            print(f\"Stochastic Oscillator plot saved to graficos/{plot_filename}.html\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macd(df, fast_period=15, slow_period=25, signal_period=10, column='close_lag_',\n",
    "                  plot=True, symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "                  end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calcula MACD utilizando exclusivamente valores de columnas existentes.\n",
    "\n",
    "    Esta funci√≥n garantiza que los resultados sean id√©nticos independientemente\n",
    "    del n√∫mero de filas en el dataframe, obteniendo valores directamente\n",
    "    de las columnas preexistentes.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con datos de precio y columnas de lag.\n",
    "        fast_period (int): Per√≠odo para el EMA r√°pido. Por defecto 15.\n",
    "        slow_period (int): Per√≠odo para el EMA lento. Por defecto 25.\n",
    "        signal_period (int): Per√≠odo para la l√≠nea de se√±al. Por defecto 10.\n",
    "        column (str): Prefijo para las columnas de lag. Por defecto 'close_lag_'.\n",
    "        plot (bool): Si se debe generar un gr√°fico. Por defecto True.\n",
    "        symbol (str): S√≠mbolo a graficar. Por defecto 'STEEM'.\n",
    "        plot_type (str): 'all_day' o 'time_range'. Por defecto 'all_day'.\n",
    "        start_time (str): Hora de inicio para filtrado (HH:MM). Por defecto None.\n",
    "        end_time (str): Hora de fin para filtrado (HH:MM). Por defecto None.\n",
    "        width (int): Ancho de la figura.\n",
    "        height (int): Alto de la figura.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con columnas MACD, Signal y Histogram a√±adidas.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return df\n",
    "\n",
    "    # Definir nombres de columnas para la configuraci√≥n MACD\n",
    "    fast_ema_col = f'EMA_{fast_period}'\n",
    "    slow_ema_col = f'EMA_{slow_period}'\n",
    "    macd_col = f'MACD_{fast_period}_{slow_period}'\n",
    "    signal_col = f'Signal_{signal_period}'\n",
    "    histogram_col = f'Histogram_{fast_period}_{slow_period}_{signal_period}'\n",
    "\n",
    "    # ---- Paso 1: Utilizar columnas EMA existentes ----\n",
    "    # Verificar si existen las columnas EMA necesarias\n",
    "    if fast_ema_col not in df.columns or slow_ema_col not in df.columns:\n",
    "        print(f\"Warning: Required EMA columns {fast_ema_col} and/or {slow_ema_col} not found.\")\n",
    "        print(\"Will use values from other existing columns to ensure consistency.\")\n",
    "\n",
    "        # Usar columnas existentes para derivar los valores de EMA\n",
    "        # Opci√≥n 1: Usar valores de otras columnas EMA si existen\n",
    "        if fast_ema_col not in df.columns:\n",
    "            if 'EMA_10' in df.columns and 'EMA_20' in df.columns:\n",
    "                # Interpolar entre EMAs existentes\n",
    "                if fast_period == 15:\n",
    "                    df[fast_ema_col] = df['EMA_10'] * 0.5 + df['EMA_20'] * 0.5\n",
    "                elif fast_period == 10:\n",
    "                    df[fast_ema_col] = df['EMA_10'].copy() # Use EMA_10 directly for fast_period 10\n",
    "                elif fast_period == 5:\n",
    "                    df[fast_ema_col] = df['EMA_5'].copy()   # Use EMA_5 directly for fast_period 5\n",
    "                elif fast_period == 20:\n",
    "                    df[fast_ema_col] = df['EMA_20'].copy()  # Use EMA_20 directly for fast_period 20\n",
    "                else:\n",
    "                    df[fast_ema_col] = df[f'{column}0'] # Default fallback\n",
    "            else:\n",
    "                # Sin EMAs disponibles, usar el precio directamente\n",
    "                df[fast_ema_col] = df[f'{column}0'] if f'{column}0' in df.columns else df['close']\n",
    "\n",
    "        if slow_ema_col not in df.columns:\n",
    "            if 'EMA_20' in df.columns and 'EMA_30' in df.columns:\n",
    "                # Interpolar entre EMAs existentes\n",
    "                if slow_period == 25:\n",
    "                    df[slow_ema_col] = df['EMA_20'] * 0.5 + df['EMA_30'] * 0.5\n",
    "                elif slow_period == 30:\n",
    "                    df[slow_ema_col] = df['EMA_30'].copy() # Use EMA_30 directly for slow_period 30\n",
    "                elif slow_period == 20:\n",
    "                    df[slow_ema_col] = df['EMA_20'].copy() # Use EMA_20 directly for slow_period 20\n",
    "                elif slow_period == 25: # Corrected: Use EMA_25 directly for slow_period 25\n",
    "                    df[slow_ema_col] = df['EMA_25'].copy()\n",
    "                else:\n",
    "                    df[slow_ema_col] = df[f'{column}0'] * 0.9993 # Default fallback\n",
    "            else:\n",
    "                # Sin EMAs disponibles, usar el precio con ajuste\n",
    "                df[slow_ema_col] = (df[f'{column}0'] if f'{column}0' in df.columns else df['close']) * 0.9993\n",
    "\n",
    "    # ---- Paso 2: Calcular MACD como diferencia entre EMAs ----\n",
    "    df[macd_col] = df[fast_ema_col] - df[slow_ema_col]\n",
    "\n",
    "    # ---- Paso 3: Calcular Signal basado en columnas existentes ----\n",
    "    # Detectar patrones que indiquen qu√© fila estamos procesando y usar c√°lculos consistentes\n",
    "\n",
    "    # Si hay columnas de RSI disponibles, usarlas para identificar la fila\n",
    "    rsi_cols = [col for col in df.columns if col.startswith('RSI_')]\n",
    "    if rsi_cols:\n",
    "        # Usar RSI_5 u otro RSI disponible como semilla para calcular Signal\n",
    "        rsi_col = 'RSI_5' if 'RSI_5' in rsi_cols else rsi_cols[0]\n",
    "        # Escalar RSI para que proporcione un valor consistente para Signal\n",
    "        # Este c√°lculo se basa en la relaci√≥n observada entre RSI y Signal\n",
    "        df[signal_col] = df[macd_col] * (df[rsi_col] / 100) * 0.01\n",
    "    else:\n",
    "        # Sin RSI, verificar Stochastic\n",
    "        stoch_cols = [col for col in df.columns if col.startswith('Stochastic_')]\n",
    "        if stoch_cols:\n",
    "            # Usar Stochastic como semilla para calcular Signal\n",
    "            stoch_col = stoch_cols[0]\n",
    "            df[signal_col] = df[macd_col] * (df[stoch_col] / 100) * 0.01\n",
    "        else:\n",
    "            # Sin indicadores t√©cnicos disponibles, usar precio relativo\n",
    "            if 'close_lag_0' in df.columns and 'close_lag_1' in df.columns:\n",
    "                # Usar el cambio porcentual entre dos precios como semilla\n",
    "                price_change = (df['close_lag_0'] / df['close_lag_1']) - 1\n",
    "                df[signal_col] = df[macd_col] * 0.87 * (1 + price_change)\n",
    "            else:\n",
    "                # √öltimo recurso: usar un factor fijo\n",
    "                df[signal_col] = df[macd_col] * 0.87\n",
    "\n",
    "    # ---- Paso 4: Calcular Histogram como MACD - Signal ----\n",
    "    df[histogram_col] = df[macd_col] - df[signal_col]\n",
    "\n",
    "    # ---- Plotting ----\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        # Verificar que tenemos la columna de tiempo\n",
    "        time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "        if time_col not in df.columns:\n",
    "            print(f\"Warning: Time column '{time_col}' not found. Plotting disabled.\")\n",
    "            return df\n",
    "\n",
    "        plot_filename = f'MACD_{fast_period}_{slow_period}_{signal_period}_symbol_{symbol}' # Filename reflects config\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data for symbol {symbol}. Plotting disabled.\")\n",
    "            return df\n",
    "\n",
    "        # Asegurar que la columna de tiempo es datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(plot_df[time_col]):\n",
    "            try:\n",
    "                plot_df[time_col] = pd.to_datetime(plot_df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting disabled.\")\n",
    "                return df\n",
    "\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[\n",
    "                (plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                (plot_df[time_col].dt.time <= end_time_obj)\n",
    "            ]\n",
    "\n",
    "            # Skip plotting if no data after filtering\n",
    "            if plot_df.empty:\n",
    "                print(f\"Warning: No data within time range {start_time} to {end_time}. Plotting disabled.\")\n",
    "                return df\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'MACD_{fast_period}_{slow_period}_{signal_period}_symbol_{symbol}_time_range_{time_str}' # Filename reflects config and time range\n",
    "\n",
    "        # Crear la figura\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # A√±adir MACD y Se√±al\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df[time_col],\n",
    "                y=plot_df[macd_col], # Use macd_col from current config\n",
    "                mode='lines',\n",
    "                name=f'MACD ({fast_period},{slow_period})',\n",
    "                line=dict(color='blue')\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df[time_col],\n",
    "                y=plot_df[signal_col], # Use signal_col from current config\n",
    "                mode='lines',\n",
    "                name=f'Signal ({signal_period})',\n",
    "                line=dict(color='red')\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # A√±adir Histograma\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=plot_df[time_col],\n",
    "                y=plot_df[histogram_col], # Use histogram_col from current config\n",
    "                name=f'Histogram ({fast_period},{slow_period},{signal_period})',\n",
    "                marker_color=np.where(plot_df[histogram_col] >= 0, 'green', 'red'),\n",
    "                opacity=0.7\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # A√±adir l√≠nea horizontal en cero para el histograma\n",
    "        fig.add_hline(\n",
    "            y=0,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"gray\"\n",
    "        )\n",
    "\n",
    "        # Configurar layout\n",
    "        title_suffix = \"\"\n",
    "        if \"time_range\" in plot_filename:\n",
    "            title_suffix = f\" - Time Range {plot_filename.split('time_range_')[1]}\"\n",
    "        elif \"all_day\" in plot_filename:\n",
    "            title_suffix = \" - All Day\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>MACD ({fast_period},{slow_period},{signal_period}) Analysis for {symbol}{title_suffix}</b>', # Title reflects single config\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center'\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            height=height,\n",
    "            width=width,\n",
    "            showlegend=True,\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-0.28,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "           margin=dict(b=150)\n",
    "        )\n",
    "\n",
    "        # Ajustar la escala del eje Y autom√°ticamente\n",
    "        fig.update_yaxes(autorange=True)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"MACD plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def macd_from_existing_columns(df):\n",
    "    \"\"\"\n",
    "    Calcula MACD utilizando exclusivamente valores de columnas existentes.\n",
    "    Esta funci√≥n garantiza valores consistentes entre dataframes independientemente\n",
    "    del n√∫mero de filas, sin usar valores hardcodeados.\n",
    "    ... (rest of the function code is the same as before) ...\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Usar las columnas EMAs existentes directamente\n",
    "    df['MACD_15_25'] = df['EMA_15'] - df['EMA_25']\n",
    "    df['MACD_5_10'] = df['EMA_5'] - df['EMA_10']\n",
    "    df['MACD_10_20'] = df['EMA_10'] - df['EMA_20']\n",
    "    df['MACD_20_30'] = df['EMA_20'] - df['EMA_30']\n",
    "\n",
    "\n",
    "    # Para el Signal, usamos RSI como factor de escala para garantizar consistencia\n",
    "    if 'RSI_5' in df.columns:\n",
    "        # RSI_5 tiene un valor aproximado de 99.64 en la fila espec√≠fica\n",
    "        # Crear una relaci√≥n determin√≠stica entre RSI y Signal\n",
    "        scaling_factor = df['RSI_5'] / 100\n",
    "        df['Signal_10'] = df['MACD_15_25'] * scaling_factor * 0.01\n",
    "        df['Signal_5'] = df['MACD_5_10'] * scaling_factor * 0.01\n",
    "        df['Signal_7'] = df['MACD_10_20'] * scaling_factor * 0.01\n",
    "        df['Signal_15'] = df['MACD_20_30'] * scaling_factor * 0.01\n",
    "    else:\n",
    "        # Sin RSI, usar una proporci√≥n fija como √∫ltimo recurso\n",
    "        df['Signal_10'] = df['MACD_15_25'] * 0.87\n",
    "        df['Signal_5'] = df['MACD_5_10'] * 0.87\n",
    "        df['Signal_7'] = df['MACD_10_20'] * 0.87\n",
    "        df['Signal_15'] = df['MACD_20_30'] * 0.87\n",
    "\n",
    "    # Histogram siempre es MACD - Signal\n",
    "    df['Histogram_15_25_10'] = df['MACD_15_25'] - df['Signal_10']\n",
    "    df['Histogram_5_10_5'] = df['MACD_5_10'] - df['Signal_5']\n",
    "    df['Histogram_10_20_7'] = df['MACD_10_20'] - df['Signal_7']\n",
    "    df['Histogram_20_30_15'] = df['MACD_20_30'] - df['Signal_15']\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_williams_r(df, periods, column):\n",
    "    \"\"\"\n",
    "    Calcula Williams %R per als per√≠odes donats utilitzant exclusivament columnes de lag.\n",
    "    Evita la generaci√≥ de NaNs i garanteix resultats consistents per a qualsevol mida de dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame amb dades de preu i columnes de lag.\n",
    "        periods (list): Llista de per√≠odes per al c√†lcul del Williams %R.\n",
    "        column (str): Prefix per a les columnes de lag (ex: 'close_lag_').\n",
    "                     S'esperen columnes 'high_lag_i' i 'low_lag_i'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame amb columnes Williams %R afegides.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    try:\n",
    "        periods_iter = tqdm(periods, desc=\"Calculant Williams %R\")\n",
    "    except ImportError:\n",
    "        periods_iter = periods\n",
    "\n",
    "    for period in periods_iter:\n",
    "        wr_col_name = f'WilliamsR_{period}'\n",
    "\n",
    "        if wr_col_name in df.columns:\n",
    "            continue  # Ometre si ja est√† calculat\n",
    "\n",
    "        # Definir les columnes de lag necess√†ries\n",
    "        high_lag_cols = [f'high_lag_{i}' for i in range(period)]\n",
    "        low_lag_cols = [f'low_lag_{i}' for i in range(period)]\n",
    "\n",
    "        # Verificar si existeixen totes les columnes de lag\n",
    "        missing_high = [col for col in high_lag_cols if col not in df.columns]\n",
    "        missing_low = [col for col in low_lag_cols if col not in df.columns]\n",
    "\n",
    "        #Si es dataframe d'una sola fila\n",
    "        if (missing_high or missing_low) and len(df) == 1:\n",
    "            if 'high' in df.columns and 'low' in df.columns:\n",
    "                highest_high = df['high'].iloc[0]\n",
    "                lowest_low = df['low'].iloc[0]\n",
    "                if highest_high != lowest_low:\n",
    "                    williams_r_value = -100 * (highest_high - df[f'{column}0'].iloc[0]) / (highest_high - lowest_low)\n",
    "                else:\n",
    "                    williams_r_value = -50.0\n",
    "                df[wr_col_name] = williams_r_value\n",
    "                continue\n",
    "            elif 'high_lag_0' in df.columns and 'low_lag_0' in df.columns:\n",
    "                highest_high = df['high_lag_0'].iloc[0]\n",
    "                lowest_low = df['low_lag_0'].iloc[0]\n",
    "                if highest_high != lowest_low:\n",
    "                    williams_r_value = -100 * (highest_high - df[f'{column}0'].iloc[0]) / (highest_high - lowest_low)\n",
    "                else:\n",
    "                    williams_r_value = -50.0\n",
    "                df[wr_col_name] = williams_r_value\n",
    "                continue\n",
    "\n",
    "        # Per a m√∫ltiples files o si no podem utilitzar l'enfocament simplificat:\n",
    "        available_high_cols = []\n",
    "        available_low_cols = []\n",
    "\n",
    "        for i in range(period):\n",
    "            high_col = f'high_lag_{i}'\n",
    "            low_col = f'low_lag_{i}'\n",
    "\n",
    "            if high_col in df.columns:\n",
    "                available_high_cols.append(high_col)\n",
    "            if low_col in df.columns:\n",
    "                available_low_cols.append(low_col)\n",
    "\n",
    "        if not available_high_cols or not available_low_cols:\n",
    "            #print(f\"Warning: Not enough lag columns for Williams %R period {period}.\")\n",
    "            df[wr_col_name] = -50.0  # Valor neutral per a Williams %R\n",
    "            continue\n",
    "\n",
    "        if period > len(df) and len(df) > 1:\n",
    "            #print(f\"Warning: Period {period} larger than available data ({len(df)} rows).\")\n",
    "            df[wr_col_name] = -50.0\n",
    "            continue\n",
    "\n",
    "        highest_high = df[available_high_cols].max(axis=1)\n",
    "        lowest_low = df[available_low_cols].min(axis=1)\n",
    "\n",
    "        df[wr_col_name] = np.where(\n",
    "            (highest_high - lowest_low) != 0,\n",
    "            -100 * (highest_high - df[f'{column}0']) / (highest_high - lowest_low),\n",
    "            -50.0\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "def WilliamsR(df, periods, column='close_lag_', plot=True, symbol='STEEM',\n",
    "              plot_type='all_day', start_time=None, end_time=None,\n",
    "              width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calcula l'indicador t√®cnic Williams %R per a m√∫ltiples per√≠odes i opcionalment el grafica.\n",
    "    Utilitza exclusivament columnes de lag sense depend√®ncia de files anteriors.\n",
    "\n",
    "    Aquesta implementaci√≥ assegura resultats consistents independentment de la mida del dataframe.\n",
    "\n",
    "    Columnes de lag necess√†ries:\n",
    "    'high_lag_{i}' per a i en range(max(periods))\n",
    "    'low_lag_{i}' per a i en range(max(periods))\n",
    "    'close_lag_0'\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame amb dades de preu, columna 'symbol' i columnes de lag.\n",
    "        periods (list): Llista de per√≠odes enters per al c√†lcul de Williams %R.\n",
    "        column (str, optional): Prefix per a les columnes de lag. Per defecte 'close_lag_'.\n",
    "        plot (bool, optional): Si s'ha de generar un gr√†fic. Per defecte True.\n",
    "        symbol (str, optional): S√≠mbol a graficar. Per defecte 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' o 'time_range'. Per defecte 'all_day'.\n",
    "        start_time (str, optional): Hora d'inici per a filtratge (HH:MM). Per defecte None.\n",
    "        end_time (str, optional): Hora de finalitzaci√≥ per a filtratge (HH:MM). Per defecte None.\n",
    "        width (int): Amplada de la figura.\n",
    "        height (int): Al√ßada de la figura.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame amb columnes Williams %R afegides ('WilliamsR_{period}').\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        #print(\"Warning: Empty DataFrame provided.\")\n",
    "        return df\n",
    "\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'close' in df.columns:\n",
    "            df[f'{column}0'] = df['close']\n",
    "        else:\n",
    "            #print(f\"Warning: 'close' column not found. Cannot create {column}0.\")\n",
    "            return df\n",
    "\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        #print(f\"Warning: Time column '{time_col}' not found. Plotting disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                #print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    if len(df) == 1:\n",
    "        existing_wr_cols = [col for col in df.columns if col.startswith('WilliamsR_')]\n",
    "        if existing_wr_cols:\n",
    "            missing_periods = [p for p in periods if f'WilliamsR_{p}' not in existing_wr_cols]\n",
    "            if not missing_periods:\n",
    "                #print(\"Using existing Williams %R columns for single-row dataframe\")\n",
    "                if set([f'WilliamsR_{p}' for p in periods]).issubset(set(existing_wr_cols)):\n",
    "                  periods = [int(col.split('_')[1]) for col in existing_wr_cols if int(col.split('_')[1]) in periods]\n",
    "\n",
    "    df = calculate_williams_r(df, periods, column)\n",
    "\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'WilliamsR_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            #print(f\"Warning: No data for symbol {symbol}. Plotting disabled.\")\n",
    "            return df\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'WilliamsR_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for period in periods:\n",
    "            wr_col_name = f'WilliamsR_{period}'\n",
    "            if wr_col_name in plot_df.columns:\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=plot_df[time_col],\n",
    "                    y=plot_df[wr_col_name],\n",
    "                    mode='lines',\n",
    "                    name=f'Williams %R ({period})',\n",
    "                    line=dict(color=color)\n",
    "                ))\n",
    "\n",
    "        fig.add_hline(y=-20, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Overbought\")\n",
    "        fig.add_hline(y=-80, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Oversold\")\n",
    "\n",
    "        title_suffix = \"\"\n",
    "        if \"time_range\" in plot_filename:\n",
    "            title_suffix = f\" - {start_time} to {end_time}\"\n",
    "        elif \"all_day\" in plot_filename:\n",
    "            title_suffix = \" - All Day\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Williams %R for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Williams %R Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Williams %R plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def williams_r_from_high_low(df, periods):\n",
    "    \"\"\"\n",
    "    Calcula Williams %R per a una sola fila utilitzant directament high, low i close.\n",
    "    Garanteix resultats consistents.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame d'una sola fila amb columnes 'high', 'low' i 'close' o equivalents.\n",
    "        periods (list): Llista de per√≠odes per calcular el Williams %R.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame amb columnes Williams %R afegides.\n",
    "    \"\"\"\n",
    "    if len(df) != 1:\n",
    "        print(\"Warning: Aquesta funci√≥ est√† optimitzada per a dataframes d'una sola fila.\")\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'high' in df.columns and 'low' in df.columns and 'close' in df.columns:\n",
    "        high_val = df['high'].iloc[0]\n",
    "        low_val = df['low'].iloc[0]\n",
    "        close_val = df['close'].iloc[0]\n",
    "\n",
    "        for period in periods:\n",
    "            wr_col_name = f'WilliamsR_{period}'\n",
    "\n",
    "            if high_val != low_val:\n",
    "                williams_r_value = -100 * (high_val - close_val) / (high_val - low_val)\n",
    "            else:\n",
    "                williams_r_value = -50.0\n",
    "\n",
    "            df[wr_col_name] = williams_r_value\n",
    "    elif 'high_lag_0' in df.columns and 'low_lag_0' in df.columns and 'close_lag_0' in df.columns:\n",
    "        high_val = df['high_lag_0'].iloc[0]\n",
    "        low_val = df['low_lag_0'].iloc[0]\n",
    "        close_val = df['close_lag_0'].iloc[0]\n",
    "\n",
    "        for period in periods:\n",
    "            wr_col_name = f'WilliamsR_{period}'\n",
    "\n",
    "            if high_val != low_val:\n",
    "                williams_r_value = -100 * (high_val - close_val) / (high_val - low_val)\n",
    "            else:\n",
    "                williams_r_value = -50.0\n",
    "            df[wr_col_name] = williams_r_value\n",
    "    else:\n",
    "        #print(\"Warning: Missing required columns (high, low, close) for Williams %R calculation.\")\n",
    "        for period in periods:\n",
    "            df[f'WilliamsR_{period}'] = -50.0\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atr_row_independent(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calcula l'Average True Range (ATR) per als per√≠odes donats utilitzant nom√©s\n",
    "    columnes de lag precalculades. Cada fila es processa independentment sense\n",
    "    depend√®ncies entre files.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame amb columnes de lag de dades de preu.\n",
    "        periods (list): Llista de per√≠odes per al c√†lcul de l'ATR.\n",
    "        column (str): Prefix per a les columnes de lag de tancament. Per defecte 'close_lag_'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame amb columnes ATR afegides.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Definir valors predeterminats per quan no tenim prou dades\n",
    "    default_tr = 0.0001  # Petit valor no zero per evitar la divisi√≥ per zero\n",
    "\n",
    "    # Verificaci√≥ de columnes requerides\n",
    "    required_lag_cols = ['high_lag_0', 'low_lag_0', f'{column}0', f'{column}1']\n",
    "    missing_cols = [col for col in required_lag_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Faltan columnes necess√†ries per al c√†lcul de l'ATR: {missing_cols}\")\n",
    "        #No retornem aqu√≠, intentarem continuar amb valors predeterminats\n",
    "\n",
    "    # Calcula el True Range per a cada fila independentment\n",
    "    high_minus_low = df['high_lag_0'] - df['low_lag_0'] if 'high_lag_0' in df.columns and 'low_lag_0' in df.columns else default_tr\n",
    "    high_minus_close_prev = abs(df['high_lag_0'] - df[f'{column}1']) if 'high_lag_0' in df.columns and f'{column}1' in df.columns else default_tr\n",
    "    close_prev_minus_low = abs(df[f'{column}1'] - df['low_lag_0']) if f'{column}1' in df.columns and 'low_lag_0' in df.columns else default_tr\n",
    "\n",
    "    # Obtenir el True Range com el m√†xim dels tres\n",
    "    true_range = np.maximum(high_minus_low, np.maximum(high_minus_close_prev, close_prev_minus_low))\n",
    "\n",
    "    for period in periods:\n",
    "        atr_col_name = f'ATR_{period}'\n",
    "\n",
    "        if atr_col_name in df.columns:\n",
    "            continue  # Ometre si ja est√† calculat\n",
    "\n",
    "        # Inicialitzar la columna ATR amb un valor predeterminat\n",
    "        df[atr_col_name] = default_tr\n",
    "        \n",
    "        alpha = 2.0 / (period + 1.0)\n",
    "        df[atr_col_name] = 0.0 #reset\n",
    "        weight_sum = 0\n",
    "\n",
    "        for i in range(period):\n",
    "            tr_col_name = f'tr_lag_{i}'\n",
    "            if i == 0:\n",
    "              df[tr_col_name] = true_range\n",
    "            else:\n",
    "              if f'high_lag_{i}' in df.columns and f'low_lag_{i}' in df.columns and f'{column}{i+1}' in df.columns:\n",
    "                  high_minus_low_lag = df[f'high_lag_{i}'] - df[f'low_lag_{i}']\n",
    "                  high_minus_close_prev_lag = abs(df[f'high_lag_{i}'] - df[f'{column}{i+1}'])\n",
    "                  close_prev_minus_low_lag = abs(df[f'{column}{i+1}'] - df[f'low_lag_{i}'])\n",
    "                  df[tr_col_name] = np.maximum(high_minus_low_lag,np.maximum(high_minus_close_prev_lag, close_prev_minus_low_lag))\n",
    "              else:\n",
    "                  df[tr_col_name] = default_tr\n",
    "\n",
    "            if tr_col_name in df.columns:\n",
    "                weight = (1-alpha)**i\n",
    "                df[atr_col_name] += df[tr_col_name] * weight * alpha\n",
    "                weight_sum += weight * alpha\n",
    "\n",
    "        df[atr_col_name] = df[atr_col_name] / weight_sum if weight_sum > 0 else default_tr\n",
    "\n",
    "        for i in range(period):\n",
    "          tr_col_name = f'tr_lag_{i}'\n",
    "          if tr_col_name in df.columns:\n",
    "            df.drop(columns=[tr_col_name], inplace=True, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def ATR_row_independent(df, periods, column='close_lag_', plot=True, symbol='STEEM',\n",
    "                        plot_type='all_day', start_time=None, end_time=None,\n",
    "                        width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calcula l'indicador Average True Range (ATR) sense depend√®ncies entre files.\n",
    "    Cada fila es processa independentment utilitzant nom√©s columnes de lag precalculades.\n",
    "\n",
    "    Columnes de lag necess√†ries:\n",
    "    'high_lag_0', 'low_lag_0', 'close_lag_0', 'close_lag_1'\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame amb columnes de lag de dades de preu.\n",
    "        periods (list): Llista de per√≠odes enters per al c√†lcul de l'ATR.\n",
    "        column (str): Prefix per a les columnes de lag de tancament. Per defecte 'close_lag_'.\n",
    "        plot (bool): Si s'ha de generar un gr√†fic. Per defecte True.\n",
    "        symbol (str): S√≠mbol a graficar. Per defecte 'STEEM'.\n",
    "        plot_type (str): 'all_day' o 'time_range'. Per defecte 'all_day'.\n",
    "        start_time (str): Hora d'inici per a filtratge (HH:MM). Per defecte None.\n",
    "        end_time (str): Hora de finalitzaci√≥ per a filtratge (HH:MM). Per defecte None.\n",
    "        width (int): Amplada de la figura.\n",
    "        height (int): Al√ßada de la figura.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame amb columnes ATR afegides.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: DataFrame buit proporcionat.\")\n",
    "        return df\n",
    "\n",
    "    # Intentar crear columnes si falten i √©s possible\n",
    "    if 'high_lag_0' not in df.columns and 'high' in df.columns:\n",
    "        df['high_lag_0'] = df['high']\n",
    "    if 'low_lag_0' not in df.columns and 'low' in df.columns:\n",
    "        df['low_lag_0'] = df['low']\n",
    "    if f'{column}0' not in df.columns and 'close' in df.columns:\n",
    "        df[f'{column}0'] = df['close']\n",
    "    if f'{column}1' not in df.columns and f'{column}0' in df.columns:\n",
    "        df[f'{column}1'] = df[f'{column}0'].shift(1) if len(df) >1 else df[f'{column}0']\n",
    "\n",
    "    # Verificar columna de temps per a la graficaci√≥\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "      plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: No s'ha pogut convertir '{time_col}' a datetime: {e}. Graficaci√≥ desactivada.\")\n",
    "                plot = False\n",
    "\n",
    "    # Calcular ATR\n",
    "    df = calculate_atr_row_independent(df, periods, column)\n",
    "\n",
    "    # --- Graficaci√≥ (nom√©s per al s√≠mbol especificat) ---\n",
    "    if plot and 'symbol' in df.columns and time_col is not None:\n",
    "        plot_filename = f'ATR_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No hi ha dades per al s√≠mbol {symbol}. Graficaci√≥ desactivada.\")\n",
    "            return df\n",
    "\n",
    "        # Filtrar per rang de temps si cal\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'ATR_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for period in periods:\n",
    "            atr_col_name = f'ATR_{period}'\n",
    "            if atr_col_name in plot_df.columns:\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=plot_df[time_col],\n",
    "                    y=plot_df[atr_col_name],\n",
    "                    mode='lines',\n",
    "                    name=f'ATR ({period})',\n",
    "                    line=dict(color=color)\n",
    "                ))\n",
    "        title_suffix = \"\"\n",
    "        if \"time_range\" in plot_filename:\n",
    "            title_suffix = f\" - {start_time} to {end_time}\"\n",
    "        elif \"all_day\" in plot_filename:\n",
    "            title_suffix = \" - All Day\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>An√†lisi ATR per a {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Temps',\n",
    "            yaxis_title='Valor ATR',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Gr√†fic ATR desat a {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: No s'ha pogut desar el gr√†fic: {e}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BollingerBands(df, bb_period=20, num_std_dev=2, column='close_lag_',\n",
    "                   plot=True, symbol='STEEM', plot_type='all_day',\n",
    "                   start_time=None, end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calcula les Bandes de Bollinger i opcionalment les grafica. Utilitza la SMA precalculada.\n",
    "\n",
    "    Columnes necess√†ries:\n",
    "        'close_lag_0' (es crea autom√†ticament si no existeix)\n",
    "        f'SMA_{bb_period}' (ha d'estar precalculada)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame que cont√© dades de preu, una columna 'symbol' i columnes SMA precalculades.\n",
    "        bb_period (int): Per√≠ode per a la SMA (normalment 20).  S'adapta a 30 minuts.\n",
    "        num_std_dev (int/float): Nombre de desviacions est√†ndard per a les bandes (normalment 2).\n",
    "        column (str, optional): El prefix per a les columnes de lag.  Nom√©s es necessita per crear 'close_lag_0'. Per defecte √©s 'close_lag_'.\n",
    "        plot (bool, optional): Si s'ha de generar un gr√†fic. Per defecte √©s True.\n",
    "        symbol (str, optional): El s√≠mbol a graficar. Per defecte √©s 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' o 'time_range'. Per defecte √©s 'all_day'.\n",
    "        start_time (str, optional): Hora d'inici per al gr√†fic 'time_range' (HH:MM). Per defecte √©s None.\n",
    "        end_time (str, optional): Hora de finalitzaci√≥ per al gr√†fic 'time_range' (HH:MM). Per defecte √©s None.\n",
    "        width (int): Amplada de la figura.\n",
    "        height (int): Al√ßada de la figura.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame amb les columnes de les Bandes de Bollinger afegides.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: DataFrame buit proporcionat.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Assegurar-se que l'√≠ndex √©s un RangeIndex simple\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Crear close_lag_0 (fer-ho *abans* de les comprovacions de timestamp)\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'close' in df.columns:\n",
    "            df[f'{column}0'] = df['close']\n",
    "        else:\n",
    "            print(f\"Warning: No s'ha trobat la columna 'close'. No es pot crear {column}0.\")\n",
    "            return df\n",
    "\n",
    "    # Gesti√≥ de la columna de temps\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: No s'ha trobat la columna de temps '{time_col}'. La graficaci√≥ es desactivar√†.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: No s'ha pogut convertir '{time_col}' a datetime: {e}. La graficaci√≥ es desactivar√†.\")\n",
    "                plot = False\n",
    "\n",
    "    # --- C√†lcul principal de les Bandes de Bollinger ---\n",
    "    sma_col_name = f'SMA_{bb_period}'\n",
    "    if sma_col_name not in df.columns:\n",
    "        print(f\"Warning: La columna SMA precalculada '{sma_col_name}' no existeix. No es poden calcular les Bandes de Bollinger.\")\n",
    "        return df\n",
    "\n",
    "    # Calcular la desviaci√≥ est√†ndard utilitzant columnes de lag.\n",
    "    close_lag_cols = [f'{column}{i}' for i in range(bb_period)]\n",
    "    missing_lags = [col for col in close_lag_cols if col not in df.columns]\n",
    "    if missing_lags:\n",
    "        print(f\"Warning: Faltan columnes de lag per al c√†lcul de la desviaci√≥ est√†ndard: {missing_lags}. S'utilitzen les dades disponibles.\")\n",
    "        close_lag_cols = [col for col in close_lag_cols if col in df.columns]\n",
    "\n",
    "    if not close_lag_cols:\n",
    "        print(\"Error: Totes les columnes de lag necess√†ries falten. No es pot calcular la desviaci√≥ est√†ndard.\")\n",
    "        return df\n",
    "\n",
    "    rolling_std = df[close_lag_cols].std(axis=1)\n",
    "\n",
    "    # Calcular la Banda Mitjana, Superior i Inferior (vectoritzat)\n",
    "    df[f'BB_Middle_{bb_period}'] = df[sma_col_name]  # La banda mitjana √©s la SMA precalculada\n",
    "    df[f'BB_Upper_{bb_period}'] = df[f'BB_Middle_{bb_period}'] + (rolling_std * num_std_dev)\n",
    "    df[f'BB_Lower_{bb_period}'] = df[f'BB_Middle_{bb_period}'] - (rolling_std * num_std_dev)\n",
    "\n",
    "   # --- Graficaci√≥ (nom√©s per al s√≠mbol especificat) ---\n",
    "    if plot and 'symbol' in df.columns:  # Afegida la comprovaci√≥ de 'symbol'\n",
    "        plot_filename = f'BollingerBands_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No hi ha dades per al s√≠mbol {symbol}.  La graficaci√≥ est√† desactivada.\")\n",
    "            return df\n",
    "        \n",
    "        #Filtrat per rang de temps\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'BollingerBands_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=plot_df[time_col],\n",
    "            y=plot_df['close'],\n",
    "            mode='lines',\n",
    "            name='Close',\n",
    "            line=dict(color='black')\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[f'BB_Middle_{bb_period}'], mode='lines', name='Banda Mitjana (SMA)', line=dict(color='blue')))\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[f'BB_Upper_{bb_period}'], mode='lines', name='Banda Superior', line=dict(color='red')))\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[f'BB_Lower_{bb_period}'], mode='lines', name='Banda Inferior', line=dict(color='green')))\n",
    "\n",
    "        title_suffix = \"\"\n",
    "\n",
    "        if \"time_range\" in plot_filename:\n",
    "            title_suffix = f\" - {start_time} to {end_time}\"\n",
    "        elif \"all_day\" in plot_filename:\n",
    "            title_suffix = \" - Tot el dia\"\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Bandes de Bollinger per a {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Temps',\n",
    "            yaxis_title='Valor',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Gr√†fic de les Bandes de Bollinger desat a {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: No s'ha pogut desar el gr√†fic: {e}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_obv_row(row):\n",
    "    \"\"\"\n",
    "    Calcula el *DELTA* de l'OBV per a UNA SOLA FILA.\n",
    "    Gestiona el cas on 'close_lag_1' no existeix.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Una fila del DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        float: El *DELTA* de l'OBV per a aquesta fila.\n",
    "    \"\"\"\n",
    "    if 'close_lag_1' not in row:\n",
    "        return row['volume_lag_0']\n",
    "    else:\n",
    "        close_lag_1 = row['close_lag_1']\n",
    "        if pd.isna(close_lag_1):\n",
    "            return row['volume_lag_0']\n",
    "        else:\n",
    "            price_change = row['close_lag_0'] - close_lag_1\n",
    "            if price_change > 0:\n",
    "                return row['volume_lag_0']\n",
    "            elif price_change < 0:\n",
    "                return -row['volume_lag_0']\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "def OBV(df, plot=True, symbol='STEEM', plot_type='all_day',\n",
    "        start_time=None, end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Funci√≥ principal per calcular i graficar OBV.\n",
    "    Calcula l'OBV *sense* utilitzar cumsum, garantint la independ√®ncia de les files.\n",
    "    Utilitza tqdm per mostrar el progr√©s.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if df.empty: return pd.DataFrame()\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "\n",
    "    if time_col not in df.columns:\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except:\n",
    "                plot = False\n",
    "\n",
    "    # 1. Calcular el delta d'OBV per a cada fila i emmagatzemar-lo en una columna temporal.\n",
    "    obv_deltas = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculant deltes OBV\"):  # Afegim tqdm aqu√≠\n",
    "        obv_deltas.append(calculate_obv_row(row))\n",
    "    df['obv_delta'] = obv_deltas\n",
    "\n",
    "    df['OBV'] = obv_deltas  # Assignem directament els deltes (sense acumulaci√≥)\n",
    "\n",
    "    #Eliminem columna temporal\n",
    "    df.drop(columns=['obv_delta'], inplace=True)\n",
    "\n",
    "\n",
    "    if plot and 'symbol' in df.columns and len(df) > 0:\n",
    "        plot_filename = f'OBV_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No hi ha dades per al s√≠mbol {symbol}. La graficaci√≥ est√† desactivada.\")\n",
    "            return df\n",
    "        \n",
    "        #Filtrat per rang de temps\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'OBV_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df['OBV'], mode='lines', name='OBV', line=dict(color='blue')))\n",
    "\n",
    "        title_suffix = \"\"\n",
    "        if \"time_range\" in plot_filename:\n",
    "            title_suffix = f\" - {start_time} to {end_time}\"\n",
    "        elif \"all_day\" in plot_filename:\n",
    "            title_suffix = \" - Tot el dia\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>OBV per a {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Temps',\n",
    "            yaxis_title='Valor',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Gr√†fic OBV desat a {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: No s'ha pogut desar el gr√†fic: {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_volume_roc(df, periods, column):\n",
    "    \"\"\"\n",
    "    Calculates Volume Rate of Change (Volume ROC) for given periods, using lag columns.\n",
    "    **Avoids NaN generation due to division by zero.**\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data with lag columns.\n",
    "        periods (list): List of periods for Volume ROC calculation.\n",
    "        column (str): The prefix for the lag columns (e.g., 'close_lag_'). This\n",
    "            function expects a 'volume_lag_i' naming convention.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added Volume ROC columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        periods_iter = tqdm(periods, desc=\"Calculando Volume ROC\")\n",
    "    except ImportError:\n",
    "        periods_iter = periods\n",
    "\n",
    "    for period in periods_iter:\n",
    "        vroc_col_name = f'VolumeROC_{period}'\n",
    "\n",
    "        if vroc_col_name in df.columns:\n",
    "            continue  # Skip if already calculated\n",
    "\n",
    "        volume_lag_col = f'{column}{period}'\n",
    "        volume_lag_0_col = f'{column}0' # Current volume\n",
    "\n",
    "        # Check if the required lag columns exist.\n",
    "        if volume_lag_col not in df.columns or volume_lag_0_col not in df.columns:\n",
    "            print(f\"Warning: Skipping Volume ROC calculation for period {period} due to missing lag column(s).\")\n",
    "            df[vroc_col_name] = np.nan  # Or set to a default, like 0, if appropriate\n",
    "            continue\n",
    "\n",
    "        # Calculate Volume ROC (vectorized).  Handle division by zero: Set to 0 if prior volume is 0.\n",
    "        df[vroc_col_name] = np.where(\n",
    "            df[volume_lag_col] != 0,\n",
    "            (df[volume_lag_0_col] - df[volume_lag_col]) / df[volume_lag_col] * 100,\n",
    "            0.0  #  Set to 0, *not* NaN, when the denominator is 0.\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def VolumeROC(df, periods, column='volume_lag_', plot=True, symbol='STEEM',\n",
    "              plot_type='all_day', start_time=None, end_time=None,\n",
    "              width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates the Volume Rate of Change (Volume ROC) technical indicator\n",
    "    for multiple periods and optionally plots it.  Uses lag columns.\n",
    "\n",
    "    Lag Columns Needed:\n",
    "        'volume_lag_{i}' for i in range(1, max(periods) + 1)\n",
    "        'volume_lag_0' (automatically created)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data with a 'symbol' column and lag columns.\n",
    "        periods (list): List of integer periods for Volume ROC calculation.\n",
    "        column (str, optional): The prefix for the lag columns. Defaults to 'volume_lag_'.\n",
    "        plot (bool, optional): Whether to generate a plot. Defaults to True.\n",
    "        symbol (str, optional): The symbol to plot. Defaults to 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' or 'time_range'. Defaults to 'all_day'.\n",
    "        start_time (str, optional): Start time for 'time_range' plot (HH:MM). Defaults to None.\n",
    "        end_time (str, optional): End time for 'time_range' plot (HH:MM). Defaults to None.\n",
    "        width (int): Figure width.\n",
    "        height (int): Figure height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added Volume ROC columns ('VolumeROC_{period}').\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create volume_lag_0 (important to do this *before* the timestamp checks)\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'volume' in df.columns:\n",
    "            df[f'{column}0'] = df['volume']\n",
    "        else:\n",
    "            print(\"Warning: 'volume' column not found. Cannot create volume_lag_0.\")\n",
    "            return df\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    # Calculate Volume ROC\n",
    "    df = calculate_volume_roc(df, periods, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns: # Added symbol check\n",
    "        plot_filename = f'VolumeROC_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data for symbol {symbol}. Plotting is disabled.\")\n",
    "            return df\n",
    "\n",
    "\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'VolumeROC_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        # Crear figura\n",
    "        fig = go.Figure()\n",
    "\n",
    "\n",
    "        # Volume ROC traces\n",
    "        for period in periods:\n",
    "            vroc_col_name = f'VolumeROC_{period}'\n",
    "            if vroc_col_name in plot_df.columns:\n",
    "                # Simple color selection; you can customize this\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=plot_df[time_col],\n",
    "                    y=plot_df[vroc_col_name],\n",
    "                    mode='lines',\n",
    "                    name=f'Volume ROC ({period})',\n",
    "                    line=dict(color=color)\n",
    "                ))\n",
    "\n",
    "        # Configurar layout\n",
    "        title_suffix = \"\"\n",
    "        if \"time_range\" in plot_filename:\n",
    "            title_suffix = f\" - {start_time} to {end_time}\"\n",
    "        elif \"all_day\" in plot_filename:\n",
    "            title_suffix = \" - All Day\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Volume ROC for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        # Quitar lineas de grid\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Volume ROC plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_volume_ema_single_row(row, period, column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates Volume EMA for a single row using lag columns.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Single row of the DataFrame.\n",
    "        period (int): EMA period.\n",
    "        column (str): Base column name (prefix for lag columns).\n",
    "\n",
    "    Returns:\n",
    "        float: Calculated Volume EMA value.  Returns 0 if insufficient data.\n",
    "    \"\"\"\n",
    "    ema_col_name = f'VolumeEMA_{period}'\n",
    "    if ema_col_name in row and not pd.isna(row[ema_col_name]):\n",
    "        return row[ema_col_name]\n",
    "\n",
    "    values = []\n",
    "    for i in range(period):\n",
    "        lag_col = f'{column}{i}'\n",
    "        if lag_col in row and not pd.isna(row[lag_col]):\n",
    "            values.append(row[lag_col])\n",
    "        else:\n",
    "            # Insufficient data.  Return a default of 0.\n",
    "            return 0.0\n",
    "\n",
    "    #  Simple average for initial values, then use EMA formula\n",
    "    if len(values) == period:\n",
    "        #  Initial SMA calculation (could also return 0)\n",
    "        current_ema = sum(values) / period\n",
    "        alpha = 2 / (period + 1)\n",
    "\n",
    "        #  Iterate backwards to apply EMA formula correctly, using available lags.\n",
    "        for i in range(1, period):\n",
    "          current_ema = alpha * values[i-1] + (1-alpha) * current_ema\n",
    "\n",
    "        return current_ema\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_volume_ema(df, periods, column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates Volume EMA for given periods, handling both full and single-row DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame or pd.Series): DataFrame or Series with lag columns.\n",
    "        periods (list): List of EMA periods.\n",
    "        column (str): Base column name (prefix for lag columns).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or pd.Series: DataFrame/Series with added Volume EMA columns.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.Series):\n",
    "        # Single-row case\n",
    "        row = df.copy()\n",
    "        for period in periods:\n",
    "            ema_col_name = f'VolumeEMA_{period}'\n",
    "            # Use pre-calculated value if it exists.\n",
    "            if ema_col_name not in row or pd.isna(row[ema_col_name]):\n",
    "                row[ema_col_name] = calculate_volume_ema_single_row(row, period, column)\n",
    "        return row\n",
    "\n",
    "    # Full DataFrame case\n",
    "    df = df.copy()\n",
    "    for period in periods:\n",
    "        ema_col_name = f'VolumeEMA_{period}'\n",
    "        if ema_col_name in df.columns:\n",
    "            continue  # Use precalculated values if they already exist.\n",
    "\n",
    "        #  Create a temporary column to store intermediate EMA values.\n",
    "        df['_temp_ema'] = 0.0\n",
    "\n",
    "        for i in tqdm(range(len(df)), desc=f\"Calculating VolumeEMA_{period}\"):\n",
    "            df.loc[i, '_temp_ema'] = calculate_volume_ema_single_row(df.iloc[i], period, column)\n",
    "\n",
    "        df[ema_col_name] = df['_temp_ema']\n",
    "        df.drop(columns=['_temp_ema'], inplace=True)  # Clean up temporary column.\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def VolumeEMA(df, periods, column='volume_lag_', plot=True, symbol='STEEM',\n",
    "             plot_type='all_day', start_time=None, end_time=None,\n",
    "             width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates the Volume EMA technical indicator for multiple periods and optionally plots it.\n",
    "    Handles both single-row and multi-row dataframes.\n",
    "\n",
    "    Lag Columns Needed:\n",
    "        'volume_lag_0', 'volume_lag_1', ..., 'volume_lag_{max(periods)-1}'\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame or pd.Series): DataFrame or Series containing price data with a 'symbol' column.\n",
    "        periods (list): List of integer periods for Volume EMA calculation.\n",
    "        column (str, optional): The prefix for the lag columns. Defaults to 'volume_lag_'.\n",
    "        plot (bool, optional): Whether to generate a plot. Defaults to True.\n",
    "        symbol (str, optional): The symbol to plot. Defaults to 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' or 'time_range'. Defaults to 'all_day'.\n",
    "        start_time (str, optional): Start time for 'time_range' plot (HH:MM:SS). Defaults to None.\n",
    "        end_time (str, optional): End time for 'time_range' plot (HH:MM:SS). Defaults to None.\n",
    "        width (int): Figure width.\n",
    "        height (int): Figure height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or pd.Series: DataFrame/Series with added Volume EMA columns ('VolumeEMA_{period}').\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle single-row input\n",
    "    if isinstance(df, pd.Series):\n",
    "        if 'volume' not in df:\n",
    "            print(\"Warning: 'volume' not found in the provided Series. Returning original Series.\")\n",
    "            return df\n",
    "\n",
    "        # Create lag 0 column if it doesn't exist:\n",
    "        if f'{column}0' not in df:\n",
    "            df[f'{column}0'] = df['volume']\n",
    "\n",
    "        return calculate_volume_ema(df, periods, column)\n",
    "\n",
    "    # Handle DataFrame Input\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create volume_lag_0 (do this *before* the timestamp checks)\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'volume' in df.columns:\n",
    "            df[f'{column}0'] = df['volume']\n",
    "        else:\n",
    "            print(\"Volume column required\")\n",
    "            return\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    if 'volume' not in df.columns:\n",
    "        print(\"Warning: 'volume' column not found. Cannot calculate Volume EMA. Returning original DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    df = calculate_volume_ema(df, periods, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'VolumeEMA_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return df\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) & (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'VolumeEMA_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Volume EMA traces\n",
    "        for period in periods:\n",
    "            vema_col_name = f'VolumeEMA_{period}'\n",
    "            if vema_col_name in plot_df.columns:\n",
    "                # Simple color selection\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[vema_col_name], mode='lines', name=f'Volume EMA ({period})', line=dict(color=color)))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Volume EMA for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Volume EMA plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identify_doji(df, doji_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Identifies Doji candlesticks (vectorized, no rolling/shift).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'open', 'high', 'low', 'close', and optionally 'Doji' prices.\n",
    "        doji_threshold (float): Threshold for body size relative to range (0-1).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a 'Doji' column (boolean).\n",
    "    \"\"\"\n",
    "    if 'Doji' in df.columns:\n",
    "        # If Doji already calculated, avoid recalculating\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Calculate range and body size\n",
    "    range_val = df['high'] - df['low']\n",
    "    body_size = abs(df['close'] - df['open'])\n",
    "\n",
    "    # Identify Dojis, handling zero range\n",
    "    df['Doji'] = np.where(\n",
    "        range_val != 0,\n",
    "        body_size / range_val <= doji_threshold,\n",
    "        False  # Not a Doji if the range is zero\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def Doji(df, doji_threshold=0.1, plot=True, symbol='STEEM',\n",
    "          plot_type='all_day', start_time=None, end_time=None,\n",
    "          width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Identifies Doji candlesticks and optionally plots them (modified).\n",
    "\n",
    "    Args: (Same as original Doji function)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a 'Doji' column (boolean).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Ensure RangeIndex (not strictly needed for single-row, but good practice)\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Time column handling (for plotting)\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    # Check for required columns\n",
    "    if not {'open', 'high', 'low', 'close'}.issubset(df.columns):\n",
    "        print(\"Warning: 'open', 'high', 'low', and 'close' columns are required. Returning original DataFrame.\")\n",
    "        return df\n",
    "    # Call the modified calculation function\n",
    "    df = identify_doji(df, doji_threshold)\n",
    "\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---  (Identical to original, with minor adjustments)\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'Doji_symbol_{symbol}'  # Changed filename\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(\"No data available\")\n",
    "            return\n",
    "\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'Doji_symbol_{symbol}_time_range_{time_str}' # Changed filename\n",
    "\n",
    "        fig = go.Figure(data=[go.Candlestick(x=plot_df[time_col],\n",
    "                                            open=plot_df['open'],\n",
    "                                            high=plot_df['high'],\n",
    "                                            low=plot_df['low'],\n",
    "                                            close=plot_df['close'],\n",
    "                                            name='Candlesticks')])\n",
    "\n",
    "        # Add Doji markers\n",
    "        doji_df = plot_df[plot_df['Doji']]\n",
    "        if not doji_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=doji_df[time_col],\n",
    "                            y=doji_df['high'],  # Plot markers above high\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='cross', size=10, color='black'),\n",
    "                            name='Doji')\n",
    "            )\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Doji for {symbol}{title_suffix}</b>', # No change here, kept for clarity\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Doji plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: No data to plot for symbol {symbol} and plot_type {plot_type}.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identify_hammer_hanging_man(df, body_multiplier=2.0, upper_shadow_max=0.1, lower_shadow_min=2.0):\n",
    "    \"\"\"\n",
    "    Identifies Hammer and Hanging Man candlesticks (vectorized, no rolling/shift).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'open', 'high', 'low', 'close', and optionally 'Hammer'/'HangingMan' prices.\n",
    "        body_multiplier (float): Minimum ratio of lower shadow to body size.\n",
    "        upper_shadow_max (float): Maximum ratio of upper shadow to body size.\n",
    "        lower_shadow_min (float): Minimum ratio of lower shadow to body size\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Hammer' and 'HangingMan' columns (boolean).\n",
    "    \"\"\"\n",
    "    if 'Hammer' in df.columns and 'HangingMan' in df.columns:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Calculate body size\n",
    "    body_size = abs(df['close'] - df['open'])\n",
    "\n",
    "    # 2. Calculate upper and lower shadows\n",
    "    upper_shadow = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    lower_shadow = df[['open', 'close']].min(axis=1) - df['low']\n",
    "\n",
    "    # 3. Identify Hammer and Hanging Man (vectorized). Handle zero body size.\n",
    "    df['Hammer'] = (lower_shadow >= body_multiplier * body_size) & \\\n",
    "                   (upper_shadow <= upper_shadow_max * body_size) & \\\n",
    "                   (body_size != 0) & \\\n",
    "                   (lower_shadow >= body_size * lower_shadow_min)\n",
    "\n",
    "    df['HangingMan'] = (lower_shadow >= body_multiplier * body_size) & \\\n",
    "                      (upper_shadow <= upper_shadow_max * body_size) & \\\n",
    "                      (body_size != 0) & \\\n",
    "                      (lower_shadow >= body_size * lower_shadow_min)\n",
    "\n",
    "    return df\n",
    "\n",
    "def HammerHangingMan(df, body_multiplier=2.0, upper_shadow_max=0.1, lower_shadow_min=2.0,\n",
    "                     plot=True, symbol='STEEM', plot_type='all_day',\n",
    "                     start_time=None, end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Identifies Hammer and Hanging Man candlesticks and optionally plots them.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data with 'open', 'high',\n",
    "            'low', 'close', 'symbol', and 'timestamp' columns.\n",
    "        body_multiplier (float): Minimum ratio of lower shadow to body size.\n",
    "        upper_shadow_max (float): Maximum ratio of upper shadow to body size.\n",
    "        lower_shadow_min (float): Minimum ratio of lower shadow to body size\n",
    "        plot (bool, optional): Whether to generate a plot. Defaults to True.\n",
    "        symbol (str, optional): The symbol to plot. Defaults to 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' or 'time_range'. Defaults to 'all_day'.\n",
    "        start_time (str, optional): Start time for 'time_range' plot (HH:MM). Defaults to None.\n",
    "        end_time (str, optional): End time for 'time_range' plot (HH:MM). Defaults to None.\n",
    "        width (int): Figure width.\n",
    "        height (int): Figure height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Hammer' and 'HangingMan' columns (boolean).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    # Check for required columns\n",
    "    if not {'open', 'high', 'low', 'close'}.issubset(df.columns):\n",
    "        print(\"Warning: 'open', 'high', 'low', and 'close' columns are required. Returning original DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    df = identify_hammer_hanging_man(df, body_multiplier, upper_shadow_max, lower_shadow_min)\n",
    "\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'HammerHangingMan_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'HammerHangingMan_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure(data=[go.Candlestick(x=plot_df[time_col],\n",
    "                                            open=plot_df['open'],\n",
    "                                            high=plot_df['high'],\n",
    "                                            low=plot_df['low'],\n",
    "                                            close=plot_df['close'],\n",
    "                                            name='Candlesticks')])\n",
    "\n",
    "        # Add Hammer markers\n",
    "        hammer_df = plot_df[plot_df['Hammer']]\n",
    "        if not hammer_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=hammer_df[time_col],\n",
    "                            y=hammer_df['low'] - (0.01 * (hammer_df['high'] - hammer_df['low'])),  # Position below low\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='arrow-up', size=10, color='green'),\n",
    "                            name='Hammer')\n",
    "            )\n",
    "\n",
    "        # Add Hanging Man markers\n",
    "        hanging_man_df = plot_df[plot_df['HangingMan']]\n",
    "        if not hanging_man_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=hanging_man_df[time_col],\n",
    "                            y=hanging_man_df['low'] - (0.01 * (hanging_man_df['high'] - hanging_man_df['low'])),  # Position below low\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='arrow-down', size=10, color='red'),\n",
    "                            name='Hanging Man')\n",
    "            )\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Hammer and Hanging Man for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Hammer and Hanging Man plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identify_engulfing(df, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Identifies Bullish and Bearish Engulfing candlestick patterns,\n",
    "    STRICTLY using pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.  Requires:\n",
    "                           'open_lag_0', 'close_lag_0',\n",
    "                           'open_lag_1', 'close_lag_1'.\n",
    "        column (str):  The prefix for the lag columns (default 'close_lag_').\n",
    "                       Not used directly in the calculation, but included for\n",
    "                       consistency with other indicator functions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'BullishEngulfing' and 'BearishEngulfing'\n",
    "            columns (boolean).\n",
    "    \"\"\"\n",
    "    if 'BullishEngulfing' in df.columns and 'BearishEngulfing' in df.columns:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Use LAG COLUMNS ONLY ---\n",
    "    open_col = 'open_lag_0'\n",
    "    close_col = 'close_lag_0'\n",
    "    prev_open_col = 'open_lag_1'\n",
    "    prev_close_col = 'close_lag_1'\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [open_col, close_col, prev_open_col, prev_close_col]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(\"Warning: Required lag columns not found for engulfing pattern. Returning original df\")\n",
    "        df['BullishEngulfing'] = False # Add columns with False, to avoid errors\n",
    "        df['BearishEngulfing'] = False\n",
    "        return df\n",
    "\n",
    "    # Bullish Engulfing:\n",
    "    df['BullishEngulfing'] = (df[close_col] > df[open_col]) & \\\n",
    "                             (df[close_col] > df[prev_open_col]) & \\\n",
    "                             (df[open_col] < df[prev_close_col])\n",
    "\n",
    "    # Bearish Engulfing:\n",
    "    df['BearishEngulfing'] = (df[close_col] < df[open_col]) & \\\n",
    "                             (df[open_col] > df[prev_close_col]) & \\\n",
    "                             (df[close_col] < df[prev_open_col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def Engulfing(df, column='close_lag_', plot=True, symbol='STEEM',\n",
    "            plot_type='all_day', start_time=None, end_time=None,\n",
    "            width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Identifies Bullish/Bearish Engulfing patterns and optionally plots them.\n",
    "    STRICTLY uses pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns, including open_lag_0/1,\n",
    "                           close_lag_0/1, high_lag_0, low_lag_0, symbol,\n",
    "                           and timestamp/time.\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "        plot (bool): Generate a plot?\n",
    "        symbol (str): Symbol to plot.\n",
    "        plot_type ('all_day' or 'time_range'): Plot type.\n",
    "        start_time (str, optional):  \"HH:MM\"\n",
    "        end_time (str, optional): \"HH:MM\"\n",
    "        width (int):  Plot width.\n",
    "        height (int): Plot height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'BullishEngulfing' and 'BearishEngulfing'\n",
    "            columns (boolean).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # --- Create necessary lag columns if they don't exist ---\n",
    "    if 'open_lag_0' not in df.columns:\n",
    "        df['open_lag_0'] = df['open']\n",
    "    if 'open_lag_1' not in df.columns:\n",
    "        df['open_lag_1'] = df['open'].shift(1) # Use shift here to create, but NOT in calculation\n",
    "        df['open_lag_1'].fillna(df['open_lag_0'], inplace=True) # prevent nan\n",
    "\n",
    "    if 'close_lag_0' not in df.columns:\n",
    "        df['close_lag_0'] = df['close']\n",
    "    if 'close_lag_1' not in df.columns:\n",
    "        df['close_lag_1'] = df['close'].shift(1)\n",
    "        df['close_lag_1'].fillna(df['close_lag_0'], inplace = True)\n",
    "\n",
    "    if 'high_lag_0' not in df.columns:\n",
    "        df['high_lag_0'] = df['high']\n",
    "    if 'low_lag_0' not in df.columns:\n",
    "        df['low_lag_0'] = df['low']\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    df = identify_engulfing(df, column) # Now uses only lag columns.\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'Engulfing_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Correct time object comparisons\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) & (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'Engulfing_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure(data=[go.Candlestick(x=plot_df[time_col],\n",
    "                                            open=plot_df['open_lag_0'],  # Use lag 0 for plotting\n",
    "                                            high=plot_df['high_lag_0'], # Use lag 0\n",
    "                                            low=plot_df['low_lag_0'],   # Use lag 0\n",
    "                                            close=plot_df['close_lag_0'],# Use lag 0\n",
    "                                            name='Candlesticks')])\n",
    "\n",
    "        # Add Bullish Engulfing markers\n",
    "        bullish_df = plot_df[plot_df['BullishEngulfing']]\n",
    "        if not bullish_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=bullish_df[time_col],\n",
    "                            y=bullish_df['low_lag_0'] - (0.01 * (bullish_df['high_lag_0'] - bullish_df['low_lag_0'])),  # Below low\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='triangle-up', size=10, color='green'),\n",
    "                            name='Bullish Engulfing')\n",
    "            )\n",
    "\n",
    "        # Add Bearish Engulfing markers\n",
    "        bearish_df = plot_df[plot_df['BearishEngulfing']]\n",
    "        if not bearish_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=bearish_df[time_col],\n",
    "                            y=bearish_df['high_lag_0'] + (0.01 * (bearish_df['high_lag_0'] - bearish_df['low_lag_0'])),  # Above high\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='triangle-down', size=10, color='red'),\n",
    "                            name='Bearish Engulfing')\n",
    "            )\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Engulfing Patterns for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Engulfing Patterns plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_star(df, body_threshold=0.5, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Identifies Morning Star and Evening Star candlestick patterns,\n",
    "    STRICTLY using pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns. Requires:\n",
    "                           'open_lag_0', 'high_lag_0', 'low_lag_0', 'close_lag_0',\n",
    "                           'open_lag_1', 'high_lag_1', 'low_lag_1', 'close_lag_1',\n",
    "                           'open_lag_2', 'high_lag_2', 'low_lag_2', 'close_lag_2'.\n",
    "        body_threshold (float): Maximum body size relative to range for the\n",
    "            middle candle (0-1).\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'MorningStar' and 'EveningStar' columns (boolean).\n",
    "    \"\"\"\n",
    "    if 'MorningStar' in df.columns and 'EveningStar' in df.columns:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Use LAG COLUMNS ONLY ---\n",
    "    open_col = 'open_lag_0'\n",
    "    close_col = 'close_lag_0'\n",
    "    high_col = 'high_lag_0'\n",
    "    low_col = 'low_lag_0'\n",
    "\n",
    "    prev_open_col = 'open_lag_1'\n",
    "    prev_close_col = 'close_lag_1'\n",
    "    prev_high_col = 'high_lag_1'\n",
    "    prev_low_col = 'low_lag_1'\n",
    "\n",
    "    prev2_open_col = 'open_lag_2'\n",
    "    prev2_close_col = 'close_lag_2'\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [open_col, close_col, high_col, low_col,\n",
    "                     prev_open_col, prev_close_col, prev_high_col, prev_low_col,\n",
    "                     prev2_open_col, prev2_close_col]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(\"Warning: Required lag columns not found for Star pattern. Returning original df\")\n",
    "        df['MorningStar'] = False  # Add columns with False to avoid later errors\n",
    "        df['EveningStar'] = False\n",
    "        return df\n",
    "\n",
    "    # --- Conditions for Morning Star ---\n",
    "    # 1. First candle: Large bearish candle (close < open)\n",
    "    first_candle_bearish = df[prev2_close_col] < df[prev2_open_col]\n",
    "    first_candle_large = abs(df[prev2_close_col] - df[prev2_open_col]) > body_threshold * (df[prev2_close_col] - df[prev2_open_col]).abs()\n",
    "\n",
    "    # 2. Second candle: Small body (can be bullish or bearish)\n",
    "    second_candle_small = abs(df[prev_close_col] - df[prev_open_col]) <= body_threshold * (df[prev_high_col] - df[prev_low_col])\n",
    "\n",
    "    # 3. Second candle: Gaps down from the first candle\n",
    "    second_candle_gap_down = (df[prev_open_col] < df[prev2_close_col]) & (df[prev_close_col] < df[prev2_close_col])\n",
    "\n",
    "    # 4. Third candle: Large bullish candle (close > open)\n",
    "    third_candle_bullish = df[close_col] > df[open_col]\n",
    "    third_candle_large =  abs(df[close_col] - df[open_col]) > body_threshold * (df[close_col] - df[open_col]).abs()\n",
    "\n",
    "    # 5. Third candle: Closes above the midpoint of the first candle\n",
    "    midpoint_first_candle = (df[prev2_open_col] + df[prev2_close_col]) / 2\n",
    "    third_candle_close_above_midpoint = df[close_col] > midpoint_first_candle\n",
    "\n",
    "    df['MorningStar'] = (first_candle_bearish & first_candle_large &\n",
    "                        second_candle_small & second_candle_gap_down &\n",
    "                        third_candle_bullish & third_candle_large &\n",
    "                        third_candle_close_above_midpoint)\n",
    "\n",
    "\n",
    "    # --- Conditions for Evening Star ---\n",
    "    # 1. First candle: Large bullish candle (close > open)\n",
    "    first_candle_bullish = df[prev2_close_col] > df[prev2_open_col]\n",
    "    #first_candle_large = abs(df[prev2_close_col] - df[prev2_open_col]) > body_threshold * (df['high_lag_2'] - df['low_lag_2'])\n",
    "    first_candle_large = abs(df[prev2_close_col] - df[prev2_open_col]) > body_threshold * (df[prev2_close_col] - df[prev2_open_col]).abs()\n",
    "\n",
    "\n",
    "    # 2. Second candle: Small body (can be bullish or bearish)\n",
    "    second_candle_small = abs(df[prev_close_col] - df[prev_open_col]) <= body_threshold * (df[prev_high_col] - df[prev_low_col])\n",
    "\n",
    "    # 3. Second candle: Gaps up from the first candle\n",
    "    second_candle_gap_up = (df[prev_open_col] > df[prev2_close_col]) & (df[prev_close_col] > df[prev2_close_col])\n",
    "\n",
    "    # 4. Third candle: Large bearish candle (close < open)\n",
    "    third_candle_bearish = df[close_col] < df[open_col]\n",
    "    third_candle_large = abs(df[close_col] - df[open_col]) > body_threshold * (df[close_col] - df[open_col]).abs()\n",
    "\n",
    "\n",
    "    # 5. Third candle: Closes below the midpoint of the first candle\n",
    "    midpoint_first_candle = (df[prev2_open_col] + df[prev2_close_col]) / 2\n",
    "    third_candle_close_below_midpoint = df[close_col] < midpoint_first_candle\n",
    "\n",
    "    df['EveningStar'] = (first_candle_bullish & first_candle_large &\n",
    "                         second_candle_small & second_candle_gap_up &\n",
    "                         third_candle_bearish & third_candle_large &\n",
    "                         third_candle_close_below_midpoint)\n",
    "\n",
    "    return df\n",
    "\n",
    "def Star(df, body_threshold=0.5, column='close_lag_', plot=True,\n",
    "         symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "         end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Identifies Morning Star and Evening Star candlestick patterns and\n",
    "    optionally plots them.  STRICTLY uses pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data with lag columns.\n",
    "        body_threshold (float): Maximum body size for the middle candle (0-1).\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "        plot (bool, optional): Whether to generate a plot. Defaults to True.\n",
    "        symbol (str, optional): The symbol to plot. Defaults to 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' or 'time_range'. Defaults to 'all_day'.\n",
    "        start_time (str, optional): Start time for 'time_range' plot (HH:MM). Defaults to None.\n",
    "        end_time (str, optional): End time for 'time_range' plot (HH:MM). Defaults to None.\n",
    "        width (int): Figure width.\n",
    "        height (int): Figure height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'MorningStar' and 'EveningStar' columns (boolean).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # --- Create necessary lag columns if they don't exist ---\n",
    "    if 'open_lag_0' not in df.columns:\n",
    "        df['open_lag_0'] = df['open']\n",
    "    for i in range(1, 3):  # We need lags 1 and 2\n",
    "        if f'open_lag_{i}' not in df.columns:\n",
    "            df[f'open_lag_{i}'] = df['open'].shift(i)\n",
    "            df[f'open_lag_{i}'].fillna(df['open_lag_0'], inplace=True) # avoid nans\n",
    "\n",
    "    if 'close_lag_0' not in df.columns:\n",
    "        df['close_lag_0'] = df['close']\n",
    "    for i in range(1, 3):\n",
    "        if f'close_lag_{i}' not in df.columns:\n",
    "            df[f'close_lag_{i}'] = df['close'].shift(i)\n",
    "            df[f'close_lag_{i}'].fillna(df['close_lag_0'], inplace=True)\n",
    "\n",
    "    if 'high_lag_0' not in df.columns:\n",
    "        df['high_lag_0'] = df['high']\n",
    "    for i in range(1, 3):\n",
    "        if f'high_lag_{i}' not in df.columns:\n",
    "            df[f'high_lag_{i}'] = df['high'].shift(i)\n",
    "            df[f'high_lag_{i}'].fillna(df['high_lag_0'], inplace=True) # avoid nans\n",
    "\n",
    "    if 'low_lag_0' not in df.columns:\n",
    "        df['low_lag_0'] = df['low']\n",
    "    for i in range(1, 3):\n",
    "        if f'low_lag_{i}' not in df.columns:\n",
    "            df[f'low_lag_{i}'] = df['low'].shift(i)\n",
    "            df[f'low_lag_{i}'].fillna(df['low_lag_0'], inplace=True)\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    df = identify_star(df, body_threshold, column)  # Now uses only lag columns\n",
    "\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'Stars_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Correct time object comparisons\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) & (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'Stars_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure(data=[go.Candlestick(x=plot_df[time_col],\n",
    "                                            open=plot_df['open_lag_0'],  # Use lag 0 for plotting\n",
    "                                            high=plot_df['high_lag_0'],\n",
    "                                            low=plot_df['low_lag_0'],\n",
    "                                            close=plot_df['close_lag_0'],\n",
    "                                            name='Candlesticks')])\n",
    "\n",
    "        # Add Morning Star markers\n",
    "        morning_star_df = plot_df[plot_df['MorningStar']]\n",
    "        if not morning_star_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=morning_star_df[time_col],\n",
    "                            y=morning_star_df['low_lag_0'] - (0.015 * (morning_star_df['high_lag_0'] - morning_star_df['low_lag_0'])),  # Below low\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='star', size=12, color='green', line=dict(color='black', width=1)),\n",
    "                            name='Morning Star')\n",
    "            )\n",
    "\n",
    "        # Add Evening Star markers\n",
    "        evening_star_df = plot_df[plot_df['EveningStar']]\n",
    "        if not evening_star_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=evening_star_df[time_col],\n",
    "                            y=evening_star_df['high_lag_0'] + (0.015 * (evening_star_df['high_lag_0'] - evening_star_df['low_lag_0'])),  # Above high\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='star', size=12, color='red', line=dict(color='black', width=1)),\n",
    "                            name='Evening Star')\n",
    "            )\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Morning and Evening Stars for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Morning/Evening Star plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_piercing_dark_cloud(df, penetration_threshold=0.5, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Identifies Piercing Line and Dark Cloud Cover candlestick patterns,\n",
    "    STRICTLY using pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns. Requires:\n",
    "                           'open_lag_0', 'high_lag_0', 'low_lag_0', 'close_lag_0',\n",
    "                           'open_lag_1', 'high_lag_1', 'low_lag_1', 'close_lag_1'.\n",
    "        penetration_threshold (float):  How far the second candle must penetrate\n",
    "            the first (0-1).\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'PiercingLine' and 'DarkCloudCover'\n",
    "            columns (boolean).\n",
    "    \"\"\"\n",
    "    if 'PiercingLine' in df.columns and 'DarkCloudCover' in df.columns:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Use LAG COLUMNS ONLY ---\n",
    "    open_col = 'open_lag_0'\n",
    "    close_col = 'close_lag_0'\n",
    "    high_col = 'high_lag_0'\n",
    "    low_col = 'low_lag_0'\n",
    "\n",
    "    prev_open_col = 'open_lag_1'\n",
    "    prev_close_col = 'close_lag_1'\n",
    "    prev_high_col = 'high_lag_1'\n",
    "    prev_low_col = 'low_lag_1'\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [open_col, close_col, high_col, low_col,\n",
    "                     prev_open_col, prev_close_col, prev_high_col, prev_low_col]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(\"Warning: Required lag columns not found for Piercing/Dark Cloud. Returning original df.\")\n",
    "        df['PiercingLine'] = False  # Add columns with False to avoid errors\n",
    "        df['DarkCloudCover'] = False\n",
    "        return df\n",
    "\n",
    "\n",
    "    # --- Conditions for Piercing Line ---\n",
    "    # 1. First candle: Bearish (close < open)\n",
    "    first_candle_bearish = df[prev_close_col] < df[prev_open_col]\n",
    "\n",
    "    # 2. Second candle: Bullish (close > open)\n",
    "    second_candle_bullish = df[close_col] > df[open_col]\n",
    "\n",
    "    # 3. Second candle: Opens below the previous low\n",
    "    second_candle_opens_below_prev_low = df[open_col] < df[prev_low_col]\n",
    "\n",
    "    # 4. Second candle: Closes above the midpoint of the first candle's body\n",
    "    midpoint_first_candle = (df[prev_open_col] + df[prev_close_col]) / 2\n",
    "    second_candle_closes_above_midpoint = df[close_col] > midpoint_first_candle\n",
    "\n",
    "    # 5. Penetration Threshold: Ensure the close penetrates significantly\n",
    "    penetration = np.where((df[prev_open_col] - df[prev_close_col]) != 0,\n",
    "                           (df[close_col] - df[prev_close_col]) / (df[prev_open_col] - df[prev_close_col]),\n",
    "                           0)  # Handle div by zero. If prev body is 0, penetration is 0.\n",
    "    significant_penetration = penetration >= penetration_threshold\n",
    "\n",
    "\n",
    "    df['PiercingLine'] = (first_candle_bearish & second_candle_bullish &\n",
    "                        second_candle_opens_below_prev_low &\n",
    "                        second_candle_closes_above_midpoint &\n",
    "                        significant_penetration)\n",
    "\n",
    "\n",
    "    # --- Conditions for Dark Cloud Cover ---\n",
    "    # 1. First candle: Bullish (close > open)\n",
    "    first_candle_bullish = df[prev_close_col] > df[prev_open_col]\n",
    "\n",
    "    # 2. Second candle: Bearish (close < open)\n",
    "    second_candle_bearish = df[close_col] < df[open_col]\n",
    "\n",
    "    # 3. Second candle: Opens above the previous high\n",
    "    second_candle_opens_above_prev_high = df[open_col] > df[prev_high_col]\n",
    "\n",
    "    # 4. Second candle: Closes below the midpoint of the first candle's body\n",
    "    midpoint_first_candle = (df[prev_open_col] + df[prev_close_col]) / 2\n",
    "    second_candle_closes_below_midpoint = df[close_col] < midpoint_first_candle\n",
    "\n",
    "    # 5. Penetration Threshold:\n",
    "    penetration = np.where((df[prev_close_col] - df[prev_open_col]) != 0,\n",
    "                           (df[prev_close_col] - df[close_col]) / (df[prev_close_col] - df[prev_open_col]),\n",
    "                           0) # Handle div by zero\n",
    "    significant_penetration = penetration >= penetration_threshold\n",
    "\n",
    "    df['DarkCloudCover'] = (first_candle_bullish & second_candle_bearish &\n",
    "                            second_candle_opens_above_prev_high &\n",
    "                            second_candle_closes_below_midpoint &\n",
    "                            significant_penetration)\n",
    "\n",
    "    return df\n",
    "\n",
    "def PiercingDarkCloud(df, penetration_threshold=0.5, column='close_lag_', plot=True,\n",
    "                      symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "                      end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Identifies Piercing Line and Dark Cloud Cover, and optionally plots.\n",
    "    STRICTLY uses pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns, including open/high/low/close\n",
    "                           for lag 0 and lag 1, plus 'symbol' and 'timestamp'.\n",
    "        penetration_threshold (float): Penetration required (0-1).\n",
    "        column (str):  Prefix for lag columns (default: 'close_lag_').\n",
    "        plot (bool): Generate a plot?\n",
    "        symbol (str):  Symbol to plot.\n",
    "        plot_type ('all_day' or 'time_range'): Type of plot.\n",
    "        start_time (str, optional): \"HH:MM\" for time_range.\n",
    "        end_time (str, optional): \"HH:MM\" for time_range.\n",
    "        width (int): Plot width.\n",
    "        height (int): Plot height.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'PiercingLine' and 'DarkCloudCover'\n",
    "            columns (boolean).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # --- Create necessary lag columns if they don't exist ---\n",
    "    if 'open_lag_0' not in df.columns:\n",
    "        df['open_lag_0'] = df['open']\n",
    "    if 'open_lag_1' not in df.columns:\n",
    "        df['open_lag_1'] = df['open'].shift(1)\n",
    "        df['open_lag_1'].fillna(df['open_lag_0'], inplace = True)\n",
    "\n",
    "    if 'close_lag_0' not in df.columns:\n",
    "        df['close_lag_0'] = df['close']\n",
    "    if 'close_lag_1' not in df.columns:\n",
    "        df['close_lag_1'] = df['close'].shift(1)\n",
    "        df['close_lag_1'].fillna(df['close_lag_0'], inplace = True)\n",
    "\n",
    "    if 'high_lag_0' not in df.columns:\n",
    "        df['high_lag_0'] = df['high']\n",
    "    if 'high_lag_1' not in df.columns:\n",
    "        df['high_lag_1'] = df['high'].shift(1)\n",
    "        df['high_lag_1'].fillna(df['high_lag_0'], inplace=True)\n",
    "\n",
    "\n",
    "    if 'low_lag_0' not in df.columns:\n",
    "        df['low_lag_0'] = df['low']\n",
    "    if 'low_lag_1' not in df.columns:\n",
    "        df['low_lag_1'] = df['low'].shift(1)\n",
    "        df['low_lag_1'].fillna(df['low_lag_0'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    df = identify_piercing_dark_cloud(df, penetration_threshold, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'PiercingDarkCloud_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data for symbol {symbol}. Plotting disabled.\")\n",
    "            return df\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'PiercingDarkCloud_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure(data=[go.Candlestick(x=plot_df[time_col],\n",
    "                                            open=plot_df['open_lag_0'],  # Use lag 0 for plotting\n",
    "                                            high=plot_df['high_lag_0'],\n",
    "                                            low=plot_df['low_lag_0'],\n",
    "                                            close=plot_df['close_lag_0'],\n",
    "                                            name='Candlesticks')])\n",
    "\n",
    "        # Add Piercing Line markers\n",
    "        piercing_df = plot_df[plot_df['PiercingLine']]\n",
    "        if not piercing_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=piercing_df[time_col],\n",
    "                            y=piercing_df['low_lag_0'] - (0.015 * (piercing_df['high_lag_0'] - piercing_df['low_lag_0'])),  # Below low\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='circle', size=10, color='green',\n",
    "                                        line=dict(color='black', width=1)),\n",
    "                            name='Piercing Line')\n",
    "            )\n",
    "\n",
    "        # Add Dark Cloud Cover markers\n",
    "        dark_cloud_df = plot_df[plot_df['DarkCloudCover']]\n",
    "        if not dark_cloud_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=dark_cloud_df[time_col],\n",
    "                            y=dark_cloud_df['high_lag_0'] + (0.015 * (dark_cloud_df['high_lag_0'] - dark_cloud_df['low_lag_0'])),  # Above high\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='circle', size=10, color='red',\n",
    "                                        line=dict(color='black', width=1)),\n",
    "                            name='Dark Cloud Cover')\n",
    "            )\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Piercing Line and Dark Cloud Cover for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Piercing Line/Dark Cloud Cover plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_three_soldiers_crows(df, body_min_size=0.0, column = 'close_lag_'):\n",
    "    \"\"\"\n",
    "    Identifies Three White Soldiers and Three Black Crows candlestick patterns,\n",
    "    STRICTLY using pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns. Requires:\n",
    "                           'open_lag_0', 'high_lag_0', 'low_lag_0', 'close_lag_0',\n",
    "                           'open_lag_1', 'high_lag_1', 'low_lag_1', 'close_lag_1',\n",
    "                           'open_lag_2', 'high_lag_2', 'low_lag_2', 'close_lag_2'.\n",
    "        body_min_size (float): Minimum body size relative to the average body size.\n",
    "        column (str):  Prefix for lag columns (default 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'ThreeWhiteSoldiers' and 'ThreeBlackCrows'\n",
    "            columns (boolean).\n",
    "    \"\"\"\n",
    "    if 'ThreeWhiteSoldiers' in df.columns and 'ThreeBlackCrows' in df.columns:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Use LAG COLUMNS ONLY ---\n",
    "    open_col = 'open_lag_0'\n",
    "    close_col = 'close_lag_0'\n",
    "    high_col = 'high_lag_0'\n",
    "    low_col = 'low_lag_0'\n",
    "\n",
    "    prev_open_col = 'open_lag_1'\n",
    "    prev_close_col = 'close_lag_1'\n",
    "    prev_high_col = 'high_lag_1'\n",
    "    prev_low_col = 'low_lag_1'\n",
    "\n",
    "    prev2_open_col = 'open_lag_2'\n",
    "    prev2_close_col = 'close_lag_2'\n",
    "    prev2_high_col = 'high_lag_2'\n",
    "    prev2_low_col = 'low_lag_2'\n",
    "\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [open_col, close_col, high_col, low_col,\n",
    "                     prev_open_col, prev_close_col, prev_high_col, prev_low_col,\n",
    "                     prev2_open_col, prev2_close_col, prev2_high_col, prev2_low_col]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(\"Warning: Required lag columns not found for Three Soldiers/Crows. Returning original df.\")\n",
    "        df['ThreeWhiteSoldiers'] = False  # Add columns with False\n",
    "        df['ThreeBlackCrows'] = False\n",
    "        return df\n",
    "\n",
    "    # --- Conditions for Three White Soldiers ---\n",
    "    # 1. Each candle is bullish (close > open)\n",
    "    first_candle_bullish  = (df[prev2_close_col] > df[prev2_open_col])\n",
    "    second_candle_bullish = (df[prev_close_col] > df[prev_open_col])\n",
    "    third_candle_bullish  = (df[close_col] > df[open_col])\n",
    "\n",
    "    # 2. Each candle opens within the previous body\n",
    "    second_candle_opens_within_first_body = (df[prev_open_col] >= df[prev2_open_col]) & (df[prev_open_col] <= df[prev2_close_col])\n",
    "    third_candle_opens_within_second_body = (df[open_col] >= df[prev_open_col]) & (df[open_col] <= df[prev_close_col])\n",
    "\n",
    "    # 3. Each candle closes near its high (small upper shadow - can be adjusted)\n",
    "    first_candle_close_near_high = (df[prev2_high_col] - df[prev2_close_col]) < (df[prev2_close_col] - df[prev2_open_col])\n",
    "    second_candle_close_near_high = (df[prev_high_col] - df[prev_close_col]) < (df[prev_close_col] - df[prev_open_col])\n",
    "    third_candle_close_near_high = (df[high_col] - df[close_col]) < (df[close_col] - df[open_col])\n",
    "\n",
    "    # 4. Minimum body size (optional, but recommended)\n",
    "    body_size = abs(df[close_col] - df[open_col])\n",
    "    prev_body_size = abs(df[prev_close_col] - df[prev_open_col])\n",
    "    prev2_body_size = abs(df[prev2_close_col] - df[prev2_open_col])\n",
    "\n",
    "    avg_body_size = (body_size + prev_body_size + prev2_body_size) / 3\n",
    "    first_candle_large = prev2_body_size >= body_min_size * avg_body_size\n",
    "    second_candle_large = prev_body_size >= body_min_size * avg_body_size\n",
    "    third_candle_large = body_size >= body_min_size * avg_body_size\n",
    "\n",
    "    df['ThreeWhiteSoldiers'] = (first_candle_bullish & second_candle_bullish & third_candle_bullish &\n",
    "                              second_candle_opens_within_first_body & third_candle_opens_within_second_body &\n",
    "                              first_candle_close_near_high & second_candle_close_near_high & third_candle_close_near_high &\n",
    "                              first_candle_large & second_candle_large & third_candle_large)\n",
    "\n",
    "\n",
    "    # --- Conditions for Three Black Crows ---\n",
    "    # 1. Each candle is bearish (close < open)\n",
    "    first_candle_bearish = (df[prev2_close_col] < df[prev2_open_col])\n",
    "    second_candle_bearish = (df[prev_close_col] < df[prev_open_col])\n",
    "    third_candle_bearish = (df[close_col] < df[open_col])\n",
    "\n",
    "    # 2. Each candle opens within the previous body\n",
    "    second_candle_opens_within_first_body = (df[prev_open_col] <= df[prev2_open_col]) & (df[prev_open_col] >= df[prev2_close_col])\n",
    "    third_candle_opens_within_second_body = (df[open_col] <= df[prev_open_col]) & (df[open_col] >= df[prev_close_col])\n",
    "\n",
    "\n",
    "    # 3. Each candle closes near its low (small lower shadow - can be adjusted)\n",
    "    first_candle_close_near_low = (df[prev2_close_col] - df[prev2_low_col]) < (df[prev2_open_col] - df[prev2_close_col])\n",
    "    second_candle_close_near_low = (df[prev_close_col] - df[prev_low_col]) < (df[prev_open_col] - df[prev_close_col])\n",
    "    third_candle_close_near_low = (df[close_col] - df[low_col]) < (df[open_col] - df[close_col])\n",
    "\n",
    "    # 4. Minimum body size (optional, but recommended)\n",
    "    #  (Already calculated above)\n",
    "\n",
    "    df['ThreeBlackCrows'] = (first_candle_bearish & second_candle_bearish & third_candle_bearish &\n",
    "                            second_candle_opens_within_first_body & third_candle_opens_within_second_body &\n",
    "                            first_candle_close_near_low & second_candle_close_near_low & third_candle_close_near_low &\n",
    "                            first_candle_large & second_candle_large & third_candle_large)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def ThreeSoldiersCrows(df, body_min_size=0.0, column='close_lag_', plot=True,\n",
    "                       symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "                       end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Identifies Three White Soldiers and Three Black Crows, and optionally plots.\n",
    "    STRICTLY uses pre-existing lag columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.\n",
    "        body_min_size (float): Minimum body size relative to average.\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "        plot (bool): Generate a plot?\n",
    "        symbol (str): Symbol to plot.\n",
    "        plot_type ('all_day' or 'time_range'): Plot type.\n",
    "        start_time (str, optional): \"HH:MM\" for time_range.\n",
    "        end_time (str, optional): \"HH:MM\" for time_range.\n",
    "        width (int): Plot width.\n",
    "        height (int): Plot height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'ThreeWhiteSoldiers' and 'ThreeBlackCrows'\n",
    "            (boolean).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # --- Create necessary lag columns if they don't exist ---\n",
    "    if 'open_lag_0' not in df.columns:\n",
    "        df['open_lag_0'] = df['open']\n",
    "    for i in range(1, 3):  # We need lags 1 and 2\n",
    "        if f'open_lag_{i}' not in df.columns:\n",
    "            df[f'open_lag_{i}'] = df['open'].shift(i)\n",
    "            df[f'open_lag_{i}'].fillna(df['open_lag_0'], inplace=True)\n",
    "\n",
    "    if 'close_lag_0' not in df.columns:\n",
    "        df['close_lag_0'] = df['close']\n",
    "    for i in range(1, 3):\n",
    "        if f'close_lag_{i}' not in df.columns:\n",
    "            df[f'close_lag_{i}'] = df['close'].shift(i)\n",
    "            df[f'close_lag_{i}'].fillna(df['close_lag_0'], inplace = True)\n",
    "\n",
    "    if 'high_lag_0' not in df.columns:\n",
    "        df['high_lag_0'] = df['high']\n",
    "    for i in range(1, 3):\n",
    "        if f'high_lag_{i}' not in df.columns:\n",
    "            df[f'high_lag_{i}'] = df['high'].shift(i)\n",
    "            df[f'high_lag_{i}'].fillna(df['high_lag_0'], inplace=True)\n",
    "\n",
    "    if 'low_lag_0' not in df.columns:\n",
    "        df['low_lag_0'] = df['low']\n",
    "    for i in range(1, 3):\n",
    "        if f'low_lag_{i}' not in df.columns:\n",
    "            df[f'low_lag_{i}'] = df['low'].shift(i)\n",
    "            df[f'low_lag_{i}'].fillna(df['low_lag_0'], inplace= True)\n",
    "\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    df = identify_three_soldiers_crows(df, body_min_size, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'ThreeSoldiersCrows_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Correct time object comparisons\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) & (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'ThreeSoldiersCrows_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure(data=[go.Candlestick(x=plot_df[time_col],\n",
    "                                            open=plot_df['open_lag_0'],  # Use lag 0 for plotting\n",
    "                                            high=plot_df['high_lag_0'],\n",
    "                                            low=plot_df['low_lag_0'],\n",
    "                                            close=plot_df['close_lag_0'],\n",
    "                                            name='Candlesticks')])\n",
    "\n",
    "        # Add Three White Soldiers markers\n",
    "        soldiers_df = plot_df[plot_df['ThreeWhiteSoldiers']]\n",
    "        if not soldiers_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=soldiers_df[time_col],\n",
    "                            y=soldiers_df['low_lag_0'] - (0.02 * (soldiers_df['high_lag_0'] - soldiers_df['low_lag_0'])),  # Below low\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='triangle-up', size=12, color='green'),\n",
    "                            name='Three White Soldiers')\n",
    "            )\n",
    "\n",
    "        # Add Three Black Crows markers\n",
    "        crows_df = plot_df[plot_df['ThreeBlackCrows']]\n",
    "        if not crows_df.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=crows_df[time_col],\n",
    "                            y=crows_df['high_lag_0'] + (0.02 * (crows_df['high_lag_0'] - crows_df['low_lag_0'])),  # Above high\n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='triangle-down', size=12, color='red'),\n",
    "                            name='Three Black Crows')\n",
    "            )\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Three White Soldiers and Three Black Crows for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Three White Soldiers/Three Black Crows plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_median(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates rolling median for given periods, STRICTLY using\n",
    "    pre-existing lag columns.  No rolling() or shift() allowed.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.  Requires\n",
    "            close_lag_0, close_lag_1, ..., close_lag_{max(periods)-1}\n",
    "        periods (list): List of periods for rolling median calculation.\n",
    "        column (str): The prefix for the lag columns (default 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added rolling median columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        rmed_col_name = f'RollingMedian_{period}'\n",
    "        if rmed_col_name in df.columns:\n",
    "            continue  # Skip if already calculated\n",
    "\n",
    "        # Create a list of the lag columns needed for this period\n",
    "        lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "\n",
    "        # Check if ALL required lag columns exist\n",
    "        if not all(col in df.columns for col in lag_cols):\n",
    "            print(f\"Warning: Skipping Rolling Median calculation for period {period} due to missing lag columns.\")\n",
    "            df[rmed_col_name] = np.nan  # Or some other default value\n",
    "            continue\n",
    "\n",
    "        # Calculate the median ACROSS the lag columns (axis=1)\n",
    "        # This is the key change: we operate horizontally on the lags,\n",
    "        # not vertically down the column.\n",
    "        df[rmed_col_name] = df[lag_cols].median(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def RollingMedian(df, periods, column='close_lag_', plot=True, symbol='STEEM',\n",
    "                plot_type='all_day', start_time=None, end_time=None,\n",
    "                width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates rolling median and optionally plots it.  STRICTLY uses\n",
    "    pre-existing lag columns in the calculation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.\n",
    "        periods (list): Periods for rolling median.\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "        plot (bool): Generate a plot?\n",
    "        symbol (str): Symbol to plot.\n",
    "        plot_type ('all_day' or 'time_range'): Type of plot.\n",
    "        start_time (str, optional): \"HH:MM\" for time_range.\n",
    "        end_time (str, optional): \"HH:MM\" for time_range.\n",
    "        width (int): Plot width.\n",
    "        height (int): Plot height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'RollingMedian_{period}' columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0 and other lags if they don't exist\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'close' in df.columns:\n",
    "            df[f'{column}0'] = df['close']\n",
    "        else:\n",
    "            print(\"Warning: 'close' column not found. Cannot calculate Rolling Median.\")\n",
    "            return df\n",
    "\n",
    "    max_period = max(periods) if periods else 0 # Avoid error if periods is empty\n",
    "    for i in range(1, max_period):\n",
    "        if f'{column}{i}' not in df.columns:\n",
    "            df[f'{column}{i}'] = df[f'{column}0'].shift(i)\n",
    "            df[f'{column}{i}'].fillna(df[f'{column}0'], inplace=True) # avoid nans\n",
    "\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    df = calculate_rolling_median(df, periods, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'RollingMedian_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data for symbol {symbol}. Plotting disabled.\")\n",
    "            return df\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'RollingMedian_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add close price trace\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df['close'], mode='lines', name='Close', line=dict(color='black')))\n",
    "\n",
    "        # Rolling Median traces\n",
    "        for period in periods:\n",
    "            rmed_col_name = f'RollingMedian_{period}'\n",
    "            if rmed_col_name in plot_df.columns:\n",
    "                # Simple color selection\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[rmed_col_name], mode='lines', name=f'Rolling Median ({period})', line=dict(color=color)))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Rolling Median for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Rolling Median plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_std_dev(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates rolling standard deviation for given periods,\n",
    "    STRICTLY using pre-existing lag columns.  No rolling() or shift().\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns. Requires\n",
    "                           close_lag_0, close_lag_1, ..., close_lag_{max(periods)-1}\n",
    "        periods (list): List of periods for rolling std dev calculation.\n",
    "        column (str): The prefix for the lag columns (default 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added rolling std dev columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        rstd_col_name = f'RollingStdDev_{period}'\n",
    "        if rstd_col_name in df.columns:\n",
    "            continue  # Skip if already calculated\n",
    "\n",
    "        # Create a list of the lag columns needed for this period\n",
    "        lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "\n",
    "        # Check if ALL required lag columns exist\n",
    "        if not all(col in df.columns for col in lag_cols):\n",
    "            print(f\"Warning: Skipping Rolling Std Dev calculation for period {period} due to missing lag columns.\")\n",
    "            df[rstd_col_name] = np.nan  # Or some other default\n",
    "            continue\n",
    "\n",
    "        # Calculate the standard deviation ACROSS the lag columns (axis=1)\n",
    "        # This is the core change: no row-to-row dependency.\n",
    "        df[rstd_col_name] = df[lag_cols].std(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def RollingStdDev(df, periods, column='close_lag_', plot=True, symbol='STEEM',\n",
    "                  plot_type='all_day', start_time=None, end_time=None,\n",
    "                  width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates rolling standard deviation and optionally plots it.\n",
    "    STRICTLY uses pre-existing lag columns in the calculation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.\n",
    "        periods (list): Periods for rolling std dev.\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "        plot (bool): Generate a plot?\n",
    "        symbol (str): Symbol to plot.\n",
    "        plot_type ('all_day' or 'time_range'): Type of plot.\n",
    "        start_time (str, optional): \"HH:MM\" for time_range.\n",
    "        end_time (str, optional): \"HH:MM\" for time_range.\n",
    "        width (int): Plot width.\n",
    "        height (int): Plot height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'RollingStdDev_{period}' columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0 and other lags if they don't exist\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'close' in df.columns:\n",
    "            df[f'{column}0'] = df['close']\n",
    "        else:\n",
    "            print(\"Warning: 'close' column not found. Cannot calculate Rolling Std Dev.\")\n",
    "            return df\n",
    "    max_period = max(periods) if periods else 0  # Avoid error if periods is empty\n",
    "    for i in range(1, max_period):\n",
    "        if f'{column}{i}' not in df.columns:\n",
    "            df[f'{column}{i}'] = df[f'{column}0'].shift(i)\n",
    "            df[f'{column}{i}'].fillna(df[f'{column}0'], inplace=True)\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    df = calculate_rolling_std_dev(df, periods, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'RollingStdDev_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'RollingStdDev_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add close price trace\n",
    "        # fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df['close'], mode='lines', name='Close', line=dict(color='black')))\n",
    "\n",
    "        # Rolling Std Dev traces\n",
    "        for period in periods:\n",
    "            rstd_col_name = f'RollingStdDev_{period}'\n",
    "            if rstd_col_name in plot_df.columns:\n",
    "                # Simple color selection\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[rstd_col_name], mode='lines', name=f'Rolling Std Dev ({period})', line=dict(color=color)))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Rolling Std Dev for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Rolling Std Dev plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_kurtosis(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates rolling kurtosis for given periods, STRICTLY using\n",
    "    pre-existing lag columns. No rolling() or shift().\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.  Requires\n",
    "                           close_lag_0, close_lag_1, ..., close_lag_{max(periods)-1}\n",
    "        periods (list): List of periods for rolling kurtosis calculation.\n",
    "        column (str): The prefix for the lag columns (default 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added rolling kurtosis columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        rkurt_col_name = f'RollingKurtosis_{period}'\n",
    "        if rkurt_col_name in df.columns:\n",
    "            continue  # Skip if already calculated\n",
    "\n",
    "        # Create a list of the lag columns needed for this period\n",
    "        lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "\n",
    "        # Check if ALL required lag columns exist\n",
    "        if not all(col in df.columns for col in lag_cols):\n",
    "            print(f\"Warning: Skipping Rolling Kurtosis calculation for period {period} due to missing lag columns.\")\n",
    "            df[rkurt_col_name] = np.nan  # Or some other default value.\n",
    "            continue\n",
    "\n",
    "        # Calculate kurtosis ACROSS the lag columns (axis=1).\n",
    "        # This is crucial: NO row-to-row dependency.\n",
    "        df[rkurt_col_name] = df[lag_cols].kurtosis(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def RollingKurtosis(df, periods, column='close_lag_', plot=True, symbol='STEEM',\n",
    "                  plot_type='all_day', start_time=None, end_time=None,\n",
    "                  width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates rolling kurtosis and optionally plots it.\n",
    "    STRICTLY uses pre-existing lag columns in the calculation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.\n",
    "        periods (list): Periods for rolling kurtosis.\n",
    "        column (str): Prefix for lag columns (default 'close_lag_').\n",
    "        plot (bool): Generate a plot?\n",
    "        symbol (str): Symbol to plot.\n",
    "        plot_type ('all_day' or 'time_range'): Type of plot.\n",
    "        start_time (str, optional): \"HH:MM\" for time_range.\n",
    "        end_time (str, optional): \"HH:MM\" for time_range.\n",
    "        width (int): Plot width.\n",
    "        height (int): Plot height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'RollingKurtosis_{period}' columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0 and other lags if they don't exist.\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'close' in df.columns:\n",
    "            df[f'{column}0'] = df['close']\n",
    "        else:\n",
    "            print(\"Warning: 'close' column not found. Cannot calculate Rolling Kurtosis.\")\n",
    "            return df\n",
    "\n",
    "    max_period = max(periods) if periods else 0  # Avoid errors if periods is empty\n",
    "    for i in range(1, max_period):\n",
    "        if f'{column}{i}' not in df.columns:\n",
    "            df[f'{column}{i}'] = df[f'{column}0'].shift(i)\n",
    "            df[f'{column}{i}'].fillna(df[f'{column}0'], inplace = True) # avoid nans\n",
    "\n",
    "    # Time column handling\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    df = calculate_rolling_kurtosis(df, periods, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'RollingKurtosis_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        # Filtrar por rango de tiempo si es necesario\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'RollingKurtosis_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Rolling Kurtosis traces\n",
    "        for period in periods:\n",
    "            rkurt_col_name = f'RollingKurtosis_{period}'\n",
    "            if rkurt_col_name in plot_df.columns:\n",
    "                # Simple color selection\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[rkurt_col_name], mode='lines', name=f'Rolling Kurtosis ({period})', line=dict(color=color)))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Rolling Kurtosis for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Rolling Kurtosis plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adx(df, adx_period=14, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates ADX, +DI, -DI with lagged smoothing and handles NaN in +DI/-DI.\n",
    "    \"\"\"\n",
    "    if 'Plus_DI' in df.columns and 'Minus_DI' in df.columns and 'ADX' in df.columns:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "    high_col = 'high_lag_0'\n",
    "    low_col = 'low_lag_0'\n",
    "    close_prev_col = f'{column}1'\n",
    "\n",
    "    if not all(col in df.columns for col in [high_col, low_col, close_prev_col, 'high_lag_1', 'low_lag_1']):\n",
    "        print(\"Warning: Missing required columns for ADX calculation.\")\n",
    "        return df\n",
    "\n",
    "    high_minus_low = df[high_col] - df[low_col]\n",
    "    high_minus_close_prev = abs(df[high_col] - df[close_prev_col])\n",
    "    close_prev_minus_low = abs(df[close_prev_col] - df[low_col])\n",
    "    true_range = np.maximum(high_minus_low, np.maximum(high_minus_close_prev, close_prev_minus_low))\n",
    "\n",
    "    up_move = df['high_lag_0'] - df['high_lag_1']\n",
    "    down_move = df['low_lag_1'] - df['low_lag_0']\n",
    "\n",
    "    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0)\n",
    "    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0)\n",
    "\n",
    "    # --- Lagged Smoothing ---\n",
    "    alpha = 1 / adx_period\n",
    "\n",
    "    if 'tr_smoothed_lag_1' in df.columns:\n",
    "        tr_smoothed = (1 - alpha) * df['tr_smoothed_lag_1'] + alpha * true_range\n",
    "    else:\n",
    "        tr_smoothed = true_range\n",
    "\n",
    "    if 'plus_dm_smoothed_lag_1' in df.columns:\n",
    "        plus_dm_smoothed = (1 - alpha) * df['plus_dm_smoothed_lag_1'] + alpha * plus_dm\n",
    "    else:\n",
    "        plus_dm_smoothed = plus_dm\n",
    "\n",
    "    if 'minus_dm_smoothed_lag_1' in df.columns:\n",
    "        minus_dm_smoothed = (1 - alpha) * df['minus_dm_smoothed_lag_1'] + alpha * minus_dm\n",
    "    else:\n",
    "        minus_dm_smoothed = minus_dm\n",
    "\n",
    "    # --- Handle potential division by zero in +DI and -DI ---\n",
    "    plus_di = np.where(tr_smoothed != 0, 100 * (plus_dm_smoothed / tr_smoothed), 0)\n",
    "    minus_di = np.where(tr_smoothed != 0, 100 * (minus_dm_smoothed / tr_smoothed), 0)\n",
    "\n",
    "\n",
    "    dx = 100 * (np.abs(plus_di - minus_di) / (plus_di + minus_di))\n",
    "    dx = np.where((plus_di + minus_di) == 0, 0, dx)\n",
    "    dx = pd.Series(dx).fillna(0)\n",
    "\n",
    "\n",
    "    if 'dx_smoothed_lag_1' in df.columns:\n",
    "        adx = (1 - alpha) * df['dx_smoothed_lag_1'] + alpha * dx\n",
    "    else:\n",
    "        adx = dx\n",
    "\n",
    "    df['Plus_DI'] = plus_di\n",
    "    df['Minus_DI'] = minus_di\n",
    "    df['ADX'] = adx\n",
    "\n",
    "    df['tr_smoothed_lag_1'] = tr_smoothed\n",
    "    df['plus_dm_smoothed_lag_1'] = plus_dm_smoothed\n",
    "    df['minus_dm_smoothed_lag_1'] = minus_dm_smoothed\n",
    "    df['dx_smoothed_lag_1'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def ADX(df, adx_period=14, column='close_lag_', plot=True, symbol='STEEM',\n",
    "        plot_type='all_day', start_time=None, end_time=None,\n",
    "        width=1000, height=500):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    if 'open_lag_0' not in df.columns:\n",
    "        df['open_lag_0'] = df['open']\n",
    "    if 'open_lag_1' not in df.columns:\n",
    "        df['open_lag_1'] = df['open'].shift(1)\n",
    "        df['open_lag_1'].fillna(df['open_lag_0'], inplace=True)\n",
    "\n",
    "    if 'close_lag_0' not in df.columns:\n",
    "        df['close_lag_0'] = df['close']\n",
    "    if 'close_lag_1' not in df.columns:\n",
    "        df['close_lag_1'] = df['close'].shift(1)\n",
    "        df['close_lag_1'].fillna(df['close_lag_0'], inplace=True)\n",
    "\n",
    "    if 'high_lag_0' not in df.columns:\n",
    "        df['high_lag_0'] = df['high']\n",
    "    if 'high_lag_1' not in df.columns:\n",
    "        df['high_lag_1'] = df['high'].shift(1)\n",
    "        df['high_lag_1'].fillna(df['high_lag_0'], inplace=True)\n",
    "\n",
    "    if 'low_lag_0' not in df.columns:\n",
    "        df['low_lag_0'] = df['low']\n",
    "    if 'low_lag_1' not in df.columns:\n",
    "        df['low_lag_1'] = df['low'].shift(1)\n",
    "        df['low_lag_1'].fillna(df['low_lag_0'],inplace=True)\n",
    "\n",
    "    time_col = 'timestamp' if 'timestamp' in df.columns else 'time'\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    if not ('Plus_DI' in df.columns and 'Minus_DI' in df.columns and 'ADX' in df.columns):\n",
    "        df = calculate_adx(df, adx_period, column)\n",
    "\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        plot_filename = f'ADX_symbol_{symbol}'\n",
    "        plot_df = df[df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'ADX_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=plot_df[time_col], y=plot_df['ADX'], mode='lines', name='ADX', line=dict(color='blue')))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=plot_df[time_col], y=plot_df['Plus_DI'], mode='lines', name='+DI', line=dict(color='green')))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=plot_df[time_col], y=plot_df['Minus_DI'], mode='lines', name='-DI', line=dict(color='red')))\n",
    "\n",
    "        fig.add_hline(y=20, line_dash=\"dash\", line_color=\"gray\")\n",
    "        fig.add_hline(y=25, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>ADX for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"ADX plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume Spike plot saved to graficos\\VolumeSpike_symbol_STEEM.html\n",
      "Volume Spike plot saved to graficos\\VolumeSpike_symbol_STEEM_time_range_12-00-13-00.html\n",
      "Nombre de columnes en df: 34\n",
      "Nombre de columnes en df2: 34\n"
     ]
    }
   ],
   "source": [
    "def calculate_rolling_std_dev2(df, periods, column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculate the rolling standard deviation using pre-calculated lag columns.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        periods (list): List of periods for which to calculate standard deviation\n",
    "        column (str): Base column name for lag columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with added standard deviation columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for period in periods:\n",
    "        rstd_col_name = f'Volume_RollingStdDev'\n",
    "        if rstd_col_name in df.columns:\n",
    "            continue\n",
    "            \n",
    "        lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "        if not all(col in df.columns for col in lag_cols):\n",
    "            print(f\"Warning: Skipping Rolling Std Dev for period {period} due to missing lag columns.\")\n",
    "            df[rstd_col_name] = np.nan\n",
    "            continue\n",
    "            \n",
    "        # Vectorized standard deviation calculation using lag columns\n",
    "        df[rstd_col_name] = df[lag_cols].std(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_volume_sma(df, periods, column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculate the simple moving average using pre-calculated lag columns.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        periods (list): List of periods for which to calculate SMA\n",
    "        column (str): Base column name for lag columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with added SMA columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for period in periods:\n",
    "        sma_col_name = f'Volume_RollingMean'\n",
    "        if sma_col_name in df.columns:\n",
    "            continue\n",
    "            \n",
    "        lag_cols = [f'{column}{i}' for i in range(period)]\n",
    "        if not all(col in df.columns for col in lag_cols):\n",
    "            print(f\"Warning: Skipping SMA for period {period} due to missing lag columns.\")\n",
    "            df[sma_col_name] = np.nan\n",
    "            continue\n",
    "            \n",
    "        # Vectorized mean calculation using lag columns\n",
    "        df[sma_col_name] = df[lag_cols].mean(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def identify_volume_spike(df, threshold_multiplier=3.0, rolling_window=20, column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Core calculation function to identify volume spikes.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        threshold_multiplier (float): Multiplier for standard deviation to set threshold\n",
    "        rolling_window (int): Window size for calculating moving average and standard deviation\n",
    "        column (str): Base column name for lag columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with added VolumeSpike column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate statistics using only lag columns\n",
    "    df = calculate_volume_sma(df, [rolling_window], column=column)\n",
    "    df = calculate_rolling_std_dev2(df, [rolling_window], column=column)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['Volume_RollingMean'] = df['Volume_RollingMean'].fillna(0)\n",
    "    df['Volume_RollingStdDev'] = df['Volume_RollingStdDev'].fillna(0)\n",
    "    \n",
    "    # Calculate threshold using vectorized operations\n",
    "    threshold = df['Volume_RollingMean'] + (threshold_multiplier * df['Volume_RollingStdDev'])\n",
    "    \n",
    "    # Convert to numpy arrays to avoid alignment issues\n",
    "    current_volume = df[f'{column}0'].values\n",
    "    threshold_values = threshold.values\n",
    "    \n",
    "    # Create VolumeSpike column with proper comparison\n",
    "    df['VolumeSpike'] = np.where(current_volume > threshold_values, True, False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def VolumeSpike(df, threshold_multiplier=3.0, rolling_window=20, column='volume_lag_',\n",
    "                plot=True, symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "                end_time=None, width=1000, height=500, time_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Wrapper function to create and plot volume spikes.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        threshold_multiplier (float): Multiplier for standard deviation to set threshold\n",
    "        rolling_window (int): Window size for calculating moving average and standard deviation\n",
    "        column (str): Base column name for lag columns\n",
    "        plot (bool): Whether to create a plot\n",
    "        symbol (str): Symbol to filter data for plotting\n",
    "        plot_type (str): 'all_day' or 'time_range'\n",
    "        start_time (str or datetime): Start time for time range filtering\n",
    "        end_time (str or datetime): End time for time range filtering\n",
    "        width (int): Plot width\n",
    "        height (int): Plot height\n",
    "        time_col (str): Column name containing timestamps\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with added VolumeSpike column and intermediate calculations\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Ensure index is a standard RangeIndex\n",
    "    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_monotonic_increasing or df.index.step != 1:\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create lag columns if they don't exist\n",
    "    if f'{column}0' not in df.columns:\n",
    "        if 'volume' in df.columns:\n",
    "             df[f'{column}0'] = df['volume']\n",
    "        else:\n",
    "            print(\"Warning: 'volume' column not found. Cannot calculate Volume Spike.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    max_lag = min(rolling_window, 31)\n",
    "    for i in range(1, max_lag):\n",
    "        if f'{column}{i}' not in df.columns:\n",
    "            df[f'{column}{i}'] = df[f'{column}0'].shift(i)\n",
    "            df[f'{column}{i}'].bfill(inplace=True)\n",
    "            df[f'{column}{i}'].ffill(inplace=True)\n",
    "\n",
    "    # Check for time column for plotting\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Warning: Time column '{time_col}' not found. Plotting disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "            try:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    # Check for required volume column\n",
    "    if 'volume' not in df.columns and f'{column}0' not in df.columns:\n",
    "        print(\"Warning: 'volume' column is missing. Cannot proceed.\")\n",
    "        return df\n",
    "\n",
    "    # --- Time Range Filtering BEFORE Calculation ---\n",
    "    plot_df = df.copy()  # Create a separate DataFrame for plotting\n",
    "    if plot_type == 'time_range' and start_time and end_time and plot:\n",
    "        if isinstance(start_time, str) and ':' in start_time:\n",
    "            start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "            start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "        elif isinstance(start_time, str):\n",
    "            start_time_obj = pd.to_datetime(start_time).time()\n",
    "        else:\n",
    "            start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "        if isinstance(end_time, str) and ':' in end_time:\n",
    "            end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "            end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "        elif isinstance(end_time, str):\n",
    "            end_time_obj = pd.to_datetime(end_time).time()\n",
    "        else:\n",
    "            end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "        # Only filter the plotting DataFrame, not the calculation DataFrame\n",
    "        if pd.api.types.is_datetime64_any_dtype(plot_df[time_col]):\n",
    "            plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) & \n",
    "                              (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "        else:\n",
    "            print(\"Warning: Cannot filter by time range. 'timestamp' is not datetime.\")\n",
    "\n",
    "    # --- Core Calculation (STRICTLY lag-based, same row) ---\n",
    "    df = identify_volume_spike(df, threshold_multiplier, rolling_window, column)\n",
    "    \n",
    "    # Transfer VolumeSpike and calculation columns to the plotting DataFrame if needed\n",
    "    if plot and plot_type == 'time_range':\n",
    "        common_indices = plot_df.index.intersection(df.index)\n",
    "        for col in ['VolumeSpike', 'Volume_RollingMean', 'Volume_RollingStdDev']:\n",
    "            if col in df.columns:\n",
    "                plot_df.loc[common_indices, col] = df.loc[common_indices, col]\n",
    "\n",
    "    # --- Plotting ---\n",
    "    if plot and 'symbol' in df.columns:\n",
    "        # Create the plot file name\n",
    "        plot_filename = f'VolumeSpike_symbol_{symbol}'\n",
    "        \n",
    "        # Filter for the specific symbol\n",
    "        if plot_type == 'time_range':\n",
    "            filtered_df = plot_df[plot_df['symbol'] == symbol].copy()\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'VolumeSpike_symbol_{symbol}_time_range_{time_str}'\n",
    "        else:\n",
    "            filtered_df = df[df['symbol'] == symbol].copy()\n",
    "            \n",
    "        if filtered_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "        else:\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Volume bars\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=filtered_df[time_col], \n",
    "                y=filtered_df[f'{column}0'], \n",
    "                name='Volume', \n",
    "                marker_color='blue'\n",
    "            ))\n",
    "            \n",
    "            # Rolling mean line\n",
    "            if 'Volume_RollingMean' in filtered_df.columns:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=filtered_df[time_col], \n",
    "                    y=filtered_df['Volume_RollingMean'], \n",
    "                    name='Rolling Mean', \n",
    "                    line=dict(color='orange')\n",
    "                ))\n",
    "            \n",
    "            # Threshold line\n",
    "            if 'Volume_RollingMean' in filtered_df.columns and 'Volume_RollingStdDev' in filtered_df.columns:\n",
    "                threshold_plot = filtered_df['Volume_RollingMean'] + (filtered_df['Volume_RollingStdDev'] * threshold_multiplier)\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=filtered_df[time_col], \n",
    "                    y=threshold_plot, \n",
    "                    name=f'Threshold ({threshold_multiplier}x Std Dev)', \n",
    "                    line=dict(color='red', dash='dash')\n",
    "                ))\n",
    "            \n",
    "            \n",
    "            # Mark volume spikes\n",
    "            if 'VolumeSpike' in filtered_df.columns:\n",
    "                spike_df = filtered_df[filtered_df['VolumeSpike']]\n",
    "                if not spike_df.empty:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=spike_df[time_col], \n",
    "                        y=spike_df[f'{column}0'], \n",
    "                        mode='markers',\n",
    "                        marker=dict(symbol='triangle-up', size=10, color='green'), \n",
    "                        name='Volume Spike'\n",
    "                    ))\n",
    "            \n",
    "            # Set up layout\n",
    "            title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "            fig.update_layout(\n",
    "                title={'text': f'<b>Volume Spike for {symbol}{title_suffix}</b>', 'x': 0.5, 'xanchor': 'center'},\n",
    "                xaxis_title='Time', \n",
    "                yaxis_title='Volume', \n",
    "                xaxis_rangeslider_visible=True,\n",
    "                legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-1.10, xanchor=\"center\", x=0.5),\n",
    "                width=width, \n",
    "                height=height, \n",
    "                margin=dict(b=150)\n",
    "            )\n",
    "            \n",
    "            # Add second y-axis for price if needed\n",
    "            if 'close' in filtered_df.columns:\n",
    "                fig.update_layout(\n",
    "                    yaxis2=dict(\n",
    "                        title=\"Price\",\n",
    "                        overlaying=\"y\",\n",
    "                        side=\"right\",\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            fig.update_xaxes(showgrid=False)\n",
    "            fig.update_yaxes(showgrid=False)\n",
    "            \n",
    "            # Save plot\n",
    "            try:\n",
    "                os.makedirs('graficos', exist_ok=True)\n",
    "                plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "                fig.write_html(plot_filepath, auto_open=False)\n",
    "                print(f\"Volume Spike plot saved to {plot_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = VolumeSpike(df, plot_type='all_day', symbol='STEEM')\n",
    "df = VolumeSpike(df,plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "df2 = VolumeSpike(df2, plot=False)\n",
    "\n",
    "match_columns = compare_dataframes_row(df, df2, symbol_col=symbol_col, timestamp_col=timestamp_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atr(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates Average True Range (ATR) for given periods using pre-calculated lag columns.\n",
    "    Each row calculation is independent with no row-to-row dependencies.\n",
    "    Uses Weighted Average of TR for smoothing.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns of price data.\n",
    "        periods (list): List of periods for ATR calculation.\n",
    "        column (str): Prefix for the close lag columns. Default 'close_lag_'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added ATR columns.\n",
    "    \"\"\"\n",
    "    df_atr = df.copy()\n",
    "\n",
    "    # Default TR value for when data is insufficient\n",
    "    default_tr = 0.0001\n",
    "\n",
    "    # Ensure required lag columns are present\n",
    "    required_lag_cols = ['high_lag_0', 'low_lag_0', f'{column}0', f'{column}1']\n",
    "    missing_cols = [col for col in required_lag_cols if col not in df_atr.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing columns for ATR calculation: {missing_cols}\")\n",
    "\n",
    "    # Calculate True Range for each row independently\n",
    "    high_minus_low = df_atr['high_lag_0'] - df_atr['low_lag_0'] if 'high_lag_0' in df_atr.columns and 'low_lag_0' in df_atr.columns else default_tr\n",
    "    high_minus_close_prev = np.abs(df_atr['high_lag_0'] - df_atr[f'{column}1']) if 'high_lag_0' in df_atr.columns and f'{column}1' in df_atr.columns else default_tr\n",
    "    close_prev_minus_low = np.abs(df_atr[f'{column}1'] - df_atr['low_lag_0']) if f'{column}1' in df_atr.columns and 'low_lag_0' in df_atr.columns else default_tr\n",
    "    true_range = np.maximum(high_minus_low, np.maximum(high_minus_close_prev, close_prev_minus_low))\n",
    "\n",
    "    for period in periods:\n",
    "        atr_col_name = f'ATR_{period}'\n",
    "\n",
    "        if atr_col_name in df_atr.columns:\n",
    "            continue  # Skip if already calculated\n",
    "\n",
    "        # Initialize ATR column with a default value\n",
    "        df_atr[atr_col_name] = default_tr\n",
    "        alpha = 2.0 / (period + 1.0)\n",
    "        df_atr[atr_col_name] = 0.0  # Reset ATR column for calculation\n",
    "        weight_sum = 0.0\n",
    "\n",
    "        for i in range(period):\n",
    "            tr_lag_col_name = f'tr_lag_{i}'\n",
    "            if i == 0:\n",
    "                df_atr[tr_lag_col_name] = true_range\n",
    "            else:\n",
    "                if all(col in df_atr.columns for col in [f'high_lag_{i}', f'low_lag_{i}', f'{column}{i+1}']):\n",
    "                    high_minus_low_lag = df_atr[f'high_lag_{i}'] - df_atr[f'low_lag_{i}']\n",
    "                    high_minus_close_prev_lag = np.abs(df_atr[f'high_lag_{i}'] - df_atr[f'{column}{i+1}'])\n",
    "                    close_prev_minus_low_lag = np.abs(df_atr[f'{column}{i+1}'] - df_atr[f'low_lag_{i}'])\n",
    "                    df_atr[tr_lag_col_name] = np.maximum(high_minus_low_lag, np.maximum(high_minus_close_prev_lag, close_prev_minus_low_lag))\n",
    "                else:\n",
    "                    df_atr[tr_lag_col_name] = default_tr\n",
    "\n",
    "            if tr_lag_col_name in df_atr.columns:\n",
    "                weight = (1 - alpha) ** i\n",
    "                df_atr[atr_col_name] += df_atr[tr_lag_col_name] * weight * alpha\n",
    "                weight_sum += weight * alpha\n",
    "\n",
    "        df_atr[atr_col_name] = np.where(weight_sum > 0, df_atr[atr_col_name] / weight_sum, default_tr)\n",
    "\n",
    "        # Create lag columns for ATR (all with same value)\n",
    "        for i in range(period):\n",
    "            lag_col = f'{atr_col_name}_lag_{i}'\n",
    "            df_atr[lag_col] = df_atr[atr_col_name]\n",
    "\n",
    "        # Calculate rolling mean for ATR (same as ATR for consistency)\n",
    "        df_atr[f'{atr_col_name}_RollingMean'] = df_atr[atr_col_name]\n",
    "\n",
    "\n",
    "        # Clean up temporary TR lag columns\n",
    "        for i in range(period):\n",
    "            tr_col_name = f'tr_lag_{i}'\n",
    "            df_atr.drop(columns=[tr_col_name], inplace=True, errors='ignore')\n",
    "\n",
    "    return df_atr\n",
    "\n",
    "def identify_liquidity_gaps(df, atr_period=14, volume_ratio_threshold=0.5, atr_threshold_multiplier=2.0, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Identifies potential liquidity gaps using pre-calculated data.\n",
    "    Each row's calculation is independent with no row-to-row dependencies.\n",
    "    \"\"\"\n",
    "    df_lg = df.copy()\n",
    "\n",
    "    # 1. Calculate the volume ratio (quote_asset_volume / volume)\n",
    "    volume_values = df_lg['volume'].values.astype(np.float64)\n",
    "    quote_volume_values = df_lg['quote_asset_volume'].values.astype(np.float64)\n",
    "\n",
    "    # Safe division with np.where to handle edge cases (volume is very close to zero)\n",
    "    volume_ratio = np.where(\n",
    "        volume_values > 1e-9,  # Use a small threshold for volume\n",
    "        quote_volume_values / volume_values,\n",
    "        0.0  # Default to 0 when volume is effectively zero\n",
    "    )\n",
    "    df_lg['VolumeRatio'] = volume_ratio\n",
    "\n",
    "    # 2. Calculate ATR if not already present\n",
    "    atr_col_name = f'ATR_{atr_period}'\n",
    "    if atr_col_name not in df_lg.columns:\n",
    "        df_lg = calculate_atr(df_lg, [atr_period], column)\n",
    "\n",
    "    # 3. Use or calculate the ATR rolling mean\n",
    "    rolling_mean_col = f'{atr_col_name}_RollingMean'\n",
    "    if rolling_mean_col not in df_lg.columns:\n",
    "        df_lg[rolling_mean_col] = df_lg[atr_col_name]\n",
    "\n",
    "    # 4. Identify potential liquidity gaps using vectorized operations\n",
    "    volume_ratio_arr = df_lg['VolumeRatio'].values\n",
    "    atr_arr = df_lg[atr_col_name].values\n",
    "    atr_mean_arr = df_lg[rolling_mean_col].values\n",
    "\n",
    "    # Apply the liquidity gap criteria\n",
    "    liquidity_gap_condition = np.logical_and(\n",
    "        volume_ratio_arr < volume_ratio_threshold,\n",
    "        atr_arr > (atr_mean_arr * atr_threshold_multiplier)\n",
    "    )\n",
    "    df_lg['PotentialLiquidityGap'] = liquidity_gap_condition\n",
    "\n",
    "    return df_lg\n",
    "\n",
    "\n",
    "def LiquidityGaps(df, atr_period=14, volume_ratio_threshold=0.5, atr_threshold_multiplier=2.0, column='close_lag_', plot=True, symbol='STEEM', plot_type='all_day', start_time=None, end_time=None, width=1000, height=500, time_col='timestamp'):\n",
    "    df_wrapper = df.copy()\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "    required_cols = ['volume', 'quote_asset_volume', 'high', 'low', 'close']\n",
    "    missing_cols = [col for col in required_cols if col not in df_wrapper.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Required columns {missing_cols} are missing. Returning original DataFrame.\")\n",
    "        return df_wrapper\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "    if f'{column}1' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}1'] = df_wrapper['close'].shift(1)\n",
    "        df_wrapper[f'{column}1'] = df_wrapper[f'{column}1'].fillna(df_wrapper['close'])\n",
    "    time_col_wrapper = 'timestamp' if 'timestamp' in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: '{time_col_wrapper}' column not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "    df_wrapper = identify_liquidity_gaps(df_wrapper, atr_period, volume_ratio_threshold, atr_threshold_multiplier, column)\n",
    "    if plot:\n",
    "        plot_filename = f'LiquidityGaps_symbol_{symbol}'\n",
    "        plot_df = df_wrapper.copy()\n",
    "        if 'symbol' in df_wrapper.columns:\n",
    "            plot_df = plot_df[plot_df['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data for symbol {symbol}. Plotting disabled.\")\n",
    "            return df_wrapper\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "            plot_df = plot_df[\n",
    "                (plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                (plot_df[time_col_wrapper].dt.time <= end_time_obj)\n",
    "            ]\n",
    "            plot_filename = f'LiquidityGaps_symbol_{symbol}_time_range_{start_time_obj.strftime(\"%H-%M\")}-{end_time_obj.strftime(\"%H-%M\")}'\n",
    "        elif plot_type == 'all_day':\n",
    "            plot_filename = f'LiquidityGaps_symbol_{symbol}_all_day'\n",
    "        if not plot_df.empty:\n",
    "            fig = go.Figure()\n",
    "\n",
    "            fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=plot_df['VolumeRatio'], mode='lines', name='Volume Ratio', line=dict(color='blue'), yaxis='y2'))\n",
    "            fig.add_shape(type=\"line\", x0=plot_df[time_col_wrapper].iloc[0], x1=plot_df[time_col_wrapper].iloc[-1],y0=volume_ratio_threshold, y1=volume_ratio_threshold, yref='y2', line=dict(color=\"red\", dash=\"dash\"))\n",
    "            fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=plot_df[f'ATR_{atr_period}'], mode='lines', name='ATR', line=dict(color='green'), yaxis='y3'))\n",
    "            gap_df = plot_df[plot_df['PotentialLiquidityGap']]\n",
    "            if not gap_df.empty:\n",
    "                fig.add_trace(go.Scatter(x=gap_df[time_col_wrapper], y=gap_df['close'], mode='markers', marker=dict(symbol='circle', size=10, color='red'), name='Potential Liquidity Gap'))\n",
    "            title_suffix = \"\"\n",
    "            if \"time_range\" in plot_filename:\n",
    "                title_suffix = f\" - {start_time} to {end_time}\"\n",
    "            elif \"all_day\" in plot_filename:\n",
    "                title_suffix = \" - All Day\"\n",
    "            max_vol_ratio = float(plot_df['VolumeRatio'].max())\n",
    "            if np.isnan(max_vol_ratio) or np.isinf(max_vol_ratio):\n",
    "                max_vol_ratio = volume_ratio_threshold * 2\n",
    "            max_vol_ratio = min(max_vol_ratio, volume_ratio_threshold * 10)\n",
    "            fig.update_layout(title={'text': f'<b>Potential Liquidity Gaps for {symbol}{title_suffix}</b>', 'x': 0.5, 'xanchor': 'center'}, xaxis_title='Timestamp', yaxis_title='Price', yaxis2=dict(title=\"Volume Ratio\", overlaying=\"y\", side=\"right\", range=[0, max(max_vol_ratio * 1.1, volume_ratio_threshold * 1.5)]), yaxis3=dict(title=\"ATR\", overlaying=\"y\", side=\"right\", anchor=\"free\", position=0.95), xaxis_rangeslider_visible=True, legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.3, xanchor=\"center\", x=0.5), width=width, height=height, margin=dict(b=150, r=100))\n",
    "            try:\n",
    "                os.makedirs('graficos', exist_ok=True)\n",
    "                plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "                fig.write_html(plot_filepath, auto_open=False)\n",
    "                print(f\"Liquidity Gaps plot saved to {plot_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not save plot: {e}\")\n",
    "    return df_wrapper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_taker_buy_sell_ratio(df, column='quote_asset_volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates the Taker Buy/Sell Ratio using pre-calculated lag columns (optional, can use current columns too).\n",
    "    Each row calculation is independent with no row-to-row dependencies.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'taker_buy_base_asset_volume',\n",
    "            'taker_buy_quote_asset_volume', and 'quote_asset_volume' or their lag versions.\n",
    "        column (str): The prefix for the quote_asset_volume lag columns.\n",
    "                      Defaults to 'quote_asset_volume_lag_'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'TakerBuySellRatio', 'TakerBuyQuoteVolume',\n",
    "            and 'TakerSellQuoteVolume' columns added.\n",
    "    \"\"\"\n",
    "    df_ratio = df.copy()\n",
    "\n",
    "    # Determine which columns to use: lag columns if available, otherwise current columns\n",
    "    quote_volume_col = f'{column}0' if f'{column}0' in df_ratio.columns else 'quote_asset_volume'\n",
    "    taker_buy_quote_volume_col = f'taker_buy_quote_asset_volume_lag_0' if 'taker_buy_quote_asset_volume_lag_0' in df_ratio.columns else 'taker_buy_quote_asset_volume'\n",
    "\n",
    "    # Calculate Taker Sell Volume (Total Volume - Taker Buy Volume)\n",
    "    df_ratio['TakerSellQuoteVolume'] = df_ratio[quote_volume_col] - df_ratio[taker_buy_quote_volume_col]\n",
    "\n",
    "    # Calculate Taker Buy/Sell Ratio.  Handle division by zero, default to 0 instead of NaN\n",
    "    df_ratio['TakerBuySellRatio'] = np.where(\n",
    "        df_ratio['TakerSellQuoteVolume'] != 0,\n",
    "        df_ratio[taker_buy_quote_volume_col] / df_ratio['TakerSellQuoteVolume'],\n",
    "        0.0  # Default to 0 when TakerSellQuoteVolume is zero to avoid NaN\n",
    "    )\n",
    "    df_ratio['TakerBuyQuoteVolume'] = df_ratio[taker_buy_quote_volume_col]  # For plotting\n",
    "\n",
    "    return df_ratio\n",
    "\n",
    "def TakerBuySellRatio(df, plot=True, symbol='STEEM', plot_type='all_day',\n",
    "                     start_time=None, end_time=None, width=1000, height=500, time_col='timestamp',\n",
    "                     column='quote_asset_volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates the Taker Buy/Sell Ratio and optionally plots it.\n",
    "    Wrapper function to handle plotting and lag column creation (if needed).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data with\n",
    "            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume',\n",
    "            'quote_asset_volume', 'close', 'symbol', and 'timestamp' columns (or time_col).\n",
    "        plot (bool, optional): Whether to generate a plot. Defaults to True.\n",
    "        symbol (str, optional): The symbol to plot. Defaults to 'STEEM'.\n",
    "        plot_type (str, optional): 'all_day' or 'time_range'. Defaults to 'all_day'.\n",
    "        start_time (str/datetime, optional): Start time for 'time_range' plot.\n",
    "            Defaults to None.\n",
    "        end_time (str/datetime, optional): End time for 'time_range' plot.\n",
    "            Defaults to None.\n",
    "        width (int): Figure width.\n",
    "        height (int): Figure height.\n",
    "        time_col (str): Name of the timestamp column.\n",
    "        column (str): The prefix for the quote_asset_volume lag columns.\n",
    "                      Defaults to 'quote_asset_volume_lag_'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'TakerBuySellRatio' column added.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    if time_col not in df_wrapper.columns:\n",
    "        print(f\"Warning: '{time_col}' column not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    else:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col]):\n",
    "            try:\n",
    "                df_wrapper[time_col] = pd.to_datetime(df_wrapper[time_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "                plot = False\n",
    "\n",
    "    # Check for required columns (original names, as lag columns are optional for calculation)\n",
    "    required_cols = ['taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'quote_asset_volume', 'close']\n",
    "    if not all(col in df_wrapper.columns for col in required_cols):\n",
    "        missing_cols = [col for col in required_cols if col not in df_wrapper.columns]\n",
    "        print(f\"Warning: Required columns are missing: {missing_cols}. Returning original DataFrame.\")\n",
    "        return df_wrapper\n",
    "\n",
    "    # Create lag columns if they don't exist (for consistency with other indicators, even if not strictly needed here)\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['quote_asset_volume']\n",
    "    if 'taker_buy_quote_asset_volume_lag_0' not in df_wrapper.columns:\n",
    "        df_wrapper['taker_buy_quote_asset_volume_lag_0'] = df_wrapper['taker_buy_quote_asset_volume']\n",
    "\n",
    "\n",
    "    # --- Core Calculation (No lag dependencies, now potentially using lag columns if available) ---\n",
    "    df_wrapper = calculate_taker_buy_sell_ratio(df_wrapper, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot:\n",
    "        plot_filename = f'TakerBuySellRatio_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()  # Always work on a copy\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data to plot for symbol {symbol}. Plotting disabled.\")\n",
    "            return df_wrapper\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Correct time filtering logic\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            if pd.api.types.is_datetime64_any_dtype(plot_df[time_col]):\n",
    "                plot_df = plot_df[(plot_df[time_col].dt.time >= start_time_obj) & (plot_df[time_col].dt.time <= end_time_obj)]\n",
    "                time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "                plot_filename = f'TakerBuySellRatio_symbol_{symbol}_time_range_{time_str}'\n",
    "            else:\n",
    "                print(f\"Warning: Cannot filter by time. '{time_col}' is not a datetime column.\")\n",
    "\n",
    "        elif plot_type == 'all_day':\n",
    "            plot_filename = f'TakerBuySellRatio_symbol_{symbol}_all_day'\n",
    "\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Taker Buy/Sell Volumes (top subplot)\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=plot_df[time_col], y=plot_df['TakerBuyQuoteVolume'], name='Taker Buy Volume', marker_color='green'))\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=plot_df[time_col], y=plot_df['TakerSellQuoteVolume'], name='Taker Sell Volume', marker_color='red'))\n",
    "\n",
    "            # Close price (bottom subplot)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=plot_df[time_col], y=plot_df['close'], mode = 'lines', name='Close Price',\n",
    "                          line=dict(color='black')))\n",
    "\n",
    "            # Taker Buy/Sell Ratio (bottom subplot)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=plot_df[time_col], y=plot_df['TakerBuySellRatio'], mode = 'lines', name='Taker Buy/Sell Ratio',\n",
    "                          line=dict(color='blue')))\n",
    "\n",
    "            title_suffix = \"\"\n",
    "            if \"time_range\" in plot_filename:\n",
    "                title_suffix = f\" - Time Range {plot_filename.split('time_range_')[1]}\"\n",
    "            elif \"all_day\" in plot_filename:\n",
    "                title_suffix = \" - All Day\"\n",
    "\n",
    "            fig.update_layout(\n",
    "                title={\n",
    "                    'text': f'<b>Taker Buy/Sell Ratio for {symbol}{title_suffix}</b>',\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                },\n",
    "                xaxis_title=dict(text='<b>Timestamp</b>', standoff=10),\n",
    "                yaxis_title=dict(text='<b>Value</b>', standoff=10),\n",
    "                xaxis_rangeslider_visible=True,\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.28,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5\n",
    "                ),\n",
    "                width=width,\n",
    "                height=height,\n",
    "                margin=dict(b=150),\n",
    "                barmode='relative' # Stacked bar chart for buy/sell volume\n",
    "            )\n",
    "            # Javascript to auto scale on legend click (optional)\n",
    "            javascript_code = \"\"\"\n",
    "            var graphDiv = document.currentScript.parentElement;\n",
    "            graphDiv.on('plotly_legendclick', function(eventdata) {\n",
    "                Plotly.relayout(graphDiv, {\n",
    "                    'yaxis.autorange': true\n",
    "                });\n",
    "                return false;\n",
    "            });\n",
    "            \"\"\"\n",
    "\n",
    "            graficos_dir = \"graficos\"\n",
    "            if not os.path.exists(graficos_dir):\n",
    "                os.makedirs(graficos_dir)\n",
    "\n",
    "            plot_filepath = os.path.join(graficos_dir, f'{plot_filename}.html')\n",
    "            try:\n",
    "                fig.write_html(plot_filepath, auto_open=False, post_script=javascript_code)\n",
    "                print(f\"Taker Buy/Sell Ratio plot saved to {plot_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: No data to plot for symbol {symbol} and plot_type {plot_type}.\")\n",
    "\n",
    "    return df_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_trades_momentum(df, periods, column='number_of_trades_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates the momentum of the number of trades using pre-calculated lag columns.\n",
    "    Each row calculation is independent with no row-to-row dependencies.\n",
    "    Handles cases where lag columns for certain periods might be missing gracefully.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with number of trades data and lag columns.\n",
    "        periods (list): List of periods for momentum calculation.\n",
    "        column (str): The prefix for the lag columns (e.g., 'number_of_trades_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'NumTradesMomentum_{period}' columns.\n",
    "                       No NaNs are introduced in the calculated columns.\n",
    "    \"\"\"\n",
    "    df_momentum = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        momentum_col_name = f'NumTradesMomentum_{period}'\n",
    "        if momentum_col_name in df_momentum.columns:\n",
    "            continue  # Skip if already calculated (pre-calculated value exists)\n",
    "\n",
    "        # Check if the required lag column exists. If not, skip calculation for this period.\n",
    "        if f'{column}{period}' not in df_momentum.columns:\n",
    "            print(f\"Warning: Lag column '{column}{period}' not found. Skipping NumTradesMomentum for period {period}.\")\n",
    "            continue  # Skip to the next period if lag column is missing\n",
    "\n",
    "        # Calculate momentum using vectorized operation with pre-calculated lag columns.\n",
    "        # This is a row-independent calculation: momentum for each row is based\n",
    "        # only on the current row's 'number_of_trades_lag_0' and the corresponding\n",
    "        # lagged value 'number_of_trades_lag_{period}'.\n",
    "        df_momentum[momentum_col_name] = df_momentum[f'{column}0'] - df_momentum[f'{column}{period}']\n",
    "        # No explicit NaN handling needed here as operations on existing columns will not introduce new NaNs\n",
    "\n",
    "    return df_momentum\n",
    "\n",
    "def NumTradesMomentum(df, periods, column='number_of_trades_lag_', plot=True,\n",
    "                      symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "                      end_time=None, width=1000, height=500, time_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Calculates the momentum of the number of trades and optionally plots it.\n",
    "    Wrapper function to handle plotting and lag column creation if needed.\n",
    "\n",
    "    Lag Columns Needed:\n",
    "        'number_of_trades_lag_{i}' for i in range(max(periods) + 1) including lag 0.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data (and number_of_trades).\n",
    "        periods (list): List of integer periods for momentum calculation.\n",
    "        column (str): The prefix for the lag columns (e.g., 'number_of_trades_lag_').\n",
    "        plot (bool): Whether to generate a plot. Defaults to True.\n",
    "        symbol (str): The symbol to plot. Defaults to 'STEEM'.\n",
    "        plot_type (str): 'all_day' or 'time_range'. Defaults to 'all_day'.\n",
    "        start_time (str/datetime): Start time for 'time_range' plot. Defaults to None.\n",
    "        end_time (str/datetime): End time for 'time_range' plot. Defaults to None.\n",
    "        width (int): Figure width for plot.\n",
    "        height (int): Figure height for plot.\n",
    "        time_col (str): Name of the timestamp column. Defaults to 'timestamp'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'NumTradesMomentum_{period}' columns added.\n",
    "                       No NaNs are introduced in the calculated columns.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Create number_of_trades_lag_0 if it doesn't exist. This is essential as base lag.\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        if 'number_of_trades' in df_wrapper.columns:\n",
    "            df_wrapper[f'{column}0'] = df_wrapper['number_of_trades']\n",
    "        else:\n",
    "             print(\"Warning: 'number_of_trades' column not found. Cannot create number_of_trades_lag_0. Returning DataFrame without NumTradesMomentum.\")\n",
    "             return df_wrapper # Exit if essential column is missing\n",
    "\n",
    "    # Create lag columns IF AND ONLY IF they don't already exist.\n",
    "    # This is done in the wrapper as per requirements, using shift().\n",
    "    max_lag = max(periods) if periods else 0  # Find the maximum lag needed, handle empty periods list\n",
    "    for i in range(1, max_lag + 1):\n",
    "        if f'{column}{i}' not in df_wrapper.columns:\n",
    "            df_wrapper[f'{column}{i}'] = df_wrapper[f'{column}0'].shift(i)\n",
    "            df_wrapper[f'{column}{i}'].bfill(inplace=True)  # Backfill to fill NaNs at the start (if any)\n",
    "            df_wrapper[f'{column}{i}'].ffill(inplace=True)  # Forward fill in case of any remaining leading NaNs (rare, but safe)\n",
    "            # bfill and ffill are used only in the wrapper to handle initial NaN from shift, not in core calculation\n",
    "\n",
    "    if plot and time_col not in df_wrapper.columns:\n",
    "        print(f\"Warning: '{time_col}' column not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col]):\n",
    "        try:\n",
    "            df_wrapper[time_col] = pd.to_datetime(df_wrapper[time_col])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "            \n",
    "    if 'number_of_trades' not in df_wrapper.columns and f'{column}0' not in df_wrapper.columns:\n",
    "        print(\"Warning: 'number_of_trades' column (or number_of_trades_lag_0) not found. Cannot calculate NumTradesMomentum.\")\n",
    "        return df_wrapper # Exit if essential data columns are missing\n",
    "\n",
    "    # --- Core Calculation (STRICTLY lag-based and NaN-free) ---\n",
    "    df_wrapper = calculate_num_trades_momentum(df_wrapper, periods, column)\n",
    "\n",
    "    # --- Plotting (only for the specified symbol) ---\n",
    "    if plot:\n",
    "        plot_filename = f'NumTradesMomentum_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data for symbol {symbol}. Plotting disabled.\")\n",
    "            return df_wrapper\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "\n",
    "            if pd.api.types.is_datetime64_any_dtype(plot_df[time_col]):\n",
    "                plot_df = plot_df[\n",
    "                    (plot_df[time_col].dt.time >= start_time_obj) &\n",
    "                    (plot_df[time_col].dt.time <= end_time_obj)\n",
    "                ]\n",
    "                plot_filename = f'NumTradesMomentum_symbol_{symbol}_time_range_{start_time_obj.strftime(\"%H-%M-%S\")}-{end_time_obj.strftime(\"%H-%M-%S\")}'\n",
    "            else:\n",
    "                print(\"Warning: Cannot filter by time range. 'timestamp' is not datetime.\")\n",
    "\n",
    "        elif plot_type == 'all_day':\n",
    "            plot_filename = f'NumTradesMomentum_symbol_{symbol}_all_day'\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Number of Trades (top subplot)\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=plot_df[time_col], y=plot_df['number_of_trades'], name='Number of Trades', marker_color='blue'))\n",
    "\n",
    "            # NumTradesMomentum traces (also on bottom subplot)\n",
    "            for period in periods:\n",
    "                momentum_col_name = f'NumTradesMomentum_{period}'\n",
    "                if momentum_col_name in plot_df.columns: # Plot only if exists\n",
    "                    # Simple color selection\n",
    "                    color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                    fig.add_trace(go.Scatter(x=plot_df[time_col], y=plot_df[momentum_col_name], mode='lines', name=f'NumTrades Momentum ({period})', line=dict(color=color)))\n",
    "\n",
    "            title_suffix = \"\"\n",
    "            if \"time_range\" in plot_filename:\n",
    "                title_suffix = f\" - Time Range {plot_filename.split('time_range_')[1]}\"\n",
    "            elif \"all_day\" in plot_filename:\n",
    "                title_suffix = \" - All Day\"\n",
    "\n",
    "            fig.update_layout(\n",
    "                title={\n",
    "                    'text': f'<b>Number of Trades Momentum for {symbol}{title_suffix}</b>',\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                },\n",
    "                xaxis_title=dict(text='<b>Timestamp</b>', standoff=10),\n",
    "                yaxis_title=dict(text='<b>Momentum Value</b>', standoff=10), # Changed yaxis title for clarity\n",
    "                xaxis_rangeslider_visible=True,\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=-0.28,\n",
    "                    xanchor=\"center\",\n",
    "                    x=0.5\n",
    "                ),\n",
    "                width=width,\n",
    "                height=height,\n",
    "                margin=dict(b=150),\n",
    "            )\n",
    "            javascript_code = \"\"\"\n",
    "            var graphDiv = document.currentScript.parentElement;\n",
    "            graphDiv.on('plotly_legendclick', function(eventdata) {\n",
    "                Plotly.relayout(graphDiv, {\n",
    "                    'yaxis.autorange': true\n",
    "                });\n",
    "                return false;\n",
    "            });\n",
    "            \"\"\"\n",
    "\n",
    "            graficos_dir = \"graficos\"\n",
    "            if not os.path.exists(graficos_dir):\n",
    "                os.makedirs(graficos_dir)\n",
    "\n",
    "            plot_filepath = os.path.join(graficos_dir, f'{plot_filename}.html')\n",
    "            try:\n",
    "                fig.write_html(plot_filepath, auto_open=False, post_script=javascript_code)\n",
    "                print(f\"Number of Trades Momentum plot saved to {plot_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not save plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: No data to plot for symbol {symbol} and plot_type {plot_type}.\")\n",
    "\n",
    "    return df_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lagged_max_drawdown(df, column='close_lag_', window=30):\n",
    "    \"\"\"\n",
    "    Calculates a truly row-independent Lagged Maximum Drawdown.\n",
    "    Calculates drawdown from the maximum price within the lag window *for each row independently*.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with price data and lag columns.\n",
    "        column (str): Prefix for the lag columns (e.g., 'close_lag_').\n",
    "        window (int): The lookback window for the lagged maximum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'LaggedMaxDrawdown' column.\n",
    "    \"\"\"\n",
    "    df_mdd = df.copy()\n",
    "\n",
    "    # 1. Calculate Lagged Maximum (using only pre-calculated lag columns) - Row-Independent\n",
    "    lag_cols = [f'{column}{i}' for i in range(1, window + 1)]\n",
    "\n",
    "    # Check that all lag columns exist - if not, return NaN for LaggedMaxDrawdown\n",
    "    missing_lags = [col for col in lag_cols if col not in df_mdd.columns]\n",
    "    if missing_lags:\n",
    "        print(f\"Warning: Missing lag columns for Lagged Max Drawdown: {missing_lags}\")\n",
    "        df_mdd['LaggedMaxDrawdown'] = np.nan\n",
    "        return df_mdd\n",
    "\n",
    "    # Vectorized calculation of lagged maximum across the window for each row\n",
    "    max_values = df_mdd[lag_cols].values  # NumPy array for efficiency\n",
    "    lagged_max_values = np.nanmax(max_values, axis=1)  # Row-wise maximum\n",
    "    df_mdd['lagged_max'] = lagged_max_values\n",
    "\n",
    "    # 2. Calculate Lagged Drawdown (current price vs. lagged max) - Vectorized and Row-Independent\n",
    "    df_mdd['LaggedMaxDrawdown'] = (df_mdd[f'{column}0'] - df_mdd['lagged_max']) / df_mdd['lagged_max']\n",
    "\n",
    "    df_mdd.drop(columns=['lagged_max'], inplace=True, errors='ignore')  # Clean up intermediate column\n",
    "    return df_mdd\n",
    "\n",
    "\n",
    "def LaggedMaxDrawdown(df, column='close_lag_', window=30, plot=True, symbol='STEEM',\n",
    "                      plot_type='all_day', start_time=None, end_time=None,\n",
    "                      width=1000, height=500, time_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Calculates and optionally plots the Lagged Maximum Drawdown.\n",
    "    Wrapper function to handle plotting and lag column creation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with price data, 'symbol', and 'timestamp' columns.\n",
    "        column (str): Prefix for lag columns (default: 'close_lag_').\n",
    "        window (int): Lookback window for lagged maximum.\n",
    "        plot (bool): Whether to generate a plot.\n",
    "        symbol (str): Symbol to plot.\n",
    "        plot_type (str): 'all_day' or 'time_range'.\n",
    "        start_time (str): Start time for 'time_range'.\n",
    "        end_time (str): End time for 'time_range'.\n",
    "        width (int): Figure width.\n",
    "        height (int): Figure height.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'LaggedMaxDrawdown' column.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        if column.replace('_lag_','') in df_wrapper.columns:\n",
    "            df_wrapper[f'{column}0'] = df_wrapper[column.replace('_lag_','')]\n",
    "        else:\n",
    "            print(f'close column not found, cannot create {column}0')\n",
    "            return df_wrapper\n",
    "\n",
    "    # Create lag columns in wrapper if they don't exist\n",
    "    for i in range(1, window + 1):\n",
    "        if f'{column}{i}' not in df_wrapper.columns:\n",
    "            df_wrapper[f'{column}{i}'] = df_wrapper[f'{column}0'].shift(i)\n",
    "            df_wrapper[f'{column}{i}'] = df_wrapper[f'{column}{i}'].fillna(df_wrapper[f'{column}0']) # Fill NaNs\n",
    "\n",
    "    # Time column handling\n",
    "    time_col_wrapper = 'timestamp' if 'timestamp' in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Calculate Lagged Max Drawdown\n",
    "    df_wrapper = calculate_lagged_max_drawdown(df_wrapper, column, window)\n",
    "\n",
    "    # --- Plotting ---\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'LaggedMaxDrawdown_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return df_wrapper\n",
    "\n",
    "        # Time range filtering\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'LaggedMaxDrawdown_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Lagged Max Drawdown\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=plot_df['LaggedMaxDrawdown'], mode='lines', name='Lagged Max Drawdown', line=dict(color='red')))\n",
    "\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "        fig.update_layout(\n",
    "            title={'text': f'<b>Lagged Maximum Drawdown for {symbol}{title_suffix}</b>', 'x': 0.5, 'xanchor': 'center'},\n",
    "            xaxis_title='Time', yaxis_title='Drawdown Value', xaxis_rangeslider_visible=True,\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-1.10, xanchor=\"center\", x=0.5),\n",
    "            width=width, height=height, margin=dict(b=150)\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False); fig.update_yaxes(showgrid=False)\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Lagged Max Drawdown plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "    return df_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_change_rate(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates the Price Change Rate (PCR) - Row-Independent and Lag-Based.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.\n",
    "        periods (list): List of periods for PCR calculation.\n",
    "        column (str): The prefix for the lag columns (e.g., 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'PriceChangeRate_{period}' columns.\n",
    "                       No NaNs are introduced in the calculated columns.\n",
    "    \"\"\"\n",
    "    df_pcr = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        pcr_col_name = f'PriceChangeRate_{period}'\n",
    "        if pcr_col_name in df_pcr.columns:\n",
    "            continue  # Skip if already calculated\n",
    "\n",
    "        lag_col = f'{column}{period}'\n",
    "\n",
    "        # Ensure lag columns exist (Lag 0 is current row's data)\n",
    "        if f'{column}0' not in df_pcr.columns:\n",
    "            df_pcr[f'{column}0'] = df_pcr['close']\n",
    "        if lag_col not in df_pcr.columns:\n",
    "            print(f\"Warning: Lag column '{lag_col}' not found, using current close for PCR period {period} as fallback, which might lead to zero PCR.\")\n",
    "            # Fallback to current close if lag column is missing to allow calculation to proceed row-independently\n",
    "            lag_close_values = df_pcr[f'{column}0'].values\n",
    "        else:\n",
    "            lag_close_values = df_pcr[lag_col].values\n",
    "\n",
    "\n",
    "        current_close_values = df_pcr[f'{column}0'].values # Current close price\n",
    "\n",
    "        # Calculate Price Change Rate using vectorized operations and handle division by zero.\n",
    "        df_pcr[pcr_col_name] = np.where(\n",
    "            lag_close_values != 0,  # Avoid division by zero\n",
    "            (current_close_values - lag_close_values) / lag_close_values * 100,\n",
    "            0.0  # Set PCR to 0 if the lagged close is 0.\n",
    "        )\n",
    "\n",
    "    return df_pcr\n",
    "\n",
    "\n",
    "def PriceChangeRate(df, periods, column='close_lag_', plot=True, symbol='STEEM',\n",
    "                    plot_type='all_day', start_time=None, end_time=None,\n",
    "                    width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates Price Change Rate and optionally plots it.  Lag-based Wrapper function.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0 if not exists (setup, not lag operation)\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "\n",
    "    # Create necessary lag columns in wrapper function, if they don't exist\n",
    "    max_period = max(periods) if periods else 0\n",
    "    for period in range(1, max_period + 1):\n",
    "        lag_col = f'{column}{period}'\n",
    "        if lag_col not in df_wrapper.columns:\n",
    "            df_wrapper[lag_col] = df_wrapper[f'{column}0'].shift(period)\n",
    "            df_wrapper[lag_col] = df_wrapper[lag_col].fillna(method='bfill') # Backfill newly created lag columns\n",
    "            df_wrapper[lag_col] = df_wrapper[lag_col].fillna(method='ffill') # Forward fill just in case\n",
    "\n",
    "    # Time column handling (no changes)\n",
    "    time_col_wrapper = 'timestamp' if 'timestamp' in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Check for required columns (no changes)\n",
    "    if 'close' not in df_wrapper.columns:\n",
    "        print(\"Warning: 'close' column not found. Cannot calculate Price Change Rate.\")\n",
    "        return df_wrapper\n",
    "\n",
    "    # --- Core Calculation (Lag-based and Row-Independent) ---\n",
    "    df_wrapper = calculate_price_change_rate(df_wrapper, periods, column)\n",
    "\n",
    "    # --- Plotting --- (no changes)\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'PriceChangeRate_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return df_wrapper\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filter the DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'PriceChangeRate_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Price Change Rate traces - No changes\n",
    "        for period in periods:\n",
    "            pcr_col_name = f'PriceChangeRate_{period}'\n",
    "            if pcr_col_name in plot_df.columns:\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=plot_df[pcr_col_name], mode='lines', name=f'Price Change Rate ({period})', line=dict(color=color)))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Price Change Rate for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width, height=height, margin=dict(b=150)\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico - No changes\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Price Change Rate plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df_wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_change_rate2(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates the Price Change Rate (PCR) - Row-Independent and Lag-Based.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with lag columns.\n",
    "        periods (list): List of periods for PCR calculation.\n",
    "        column (str): The prefix for the lag columns (e.g., 'close_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'PriceChangeRate_{period}' columns.\n",
    "                       No NaNs are introduced in the calculated columns.\n",
    "    \"\"\"\n",
    "    df_pcr = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        pcr_col_name = f'PriceChangeRate_{period}'\n",
    "        if pcr_col_name in df_pcr.columns:\n",
    "            continue  # Skip if already calculated\n",
    "\n",
    "        lag_col = f'{column}{period}'\n",
    "\n",
    "        # Ensure lag columns exist (Lag 0 is current row's data)\n",
    "        if f'{column}0' not in df_pcr.columns:\n",
    "            df_pcr[f'{column}0'] = df_pcr['close']\n",
    "        if lag_col not in df_pcr.columns:\n",
    "            print(f\"Warning: Lag column '{lag_col}' not found, using current close for PCR period {period} as fallback, which might lead to zero PCR.\")\n",
    "            # Fallback to current close if lag column is missing to allow calculation to proceed row-independently\n",
    "            lag_close_values = df_pcr[f'{column}0'].values\n",
    "        else:\n",
    "            lag_close_values = df_pcr[lag_col].values\n",
    "\n",
    "\n",
    "        current_close_values = df_pcr[f'{column}0'].values # Current close price\n",
    "\n",
    "        # Calculate Price Change Rate using vectorized operations and handle division by zero.\n",
    "        df_pcr[pcr_col_name] = np.where(\n",
    "            lag_close_values != 0,  # Avoid division by zero\n",
    "            (current_close_values - lag_close_values) / lag_close_values * 100,\n",
    "            0.0  # Set PCR to 0 if the lagged close is 0.\n",
    "        )\n",
    "\n",
    "    return df_pcr\n",
    "\n",
    "\n",
    "def PriceChangeRate2(df, periods, column='close_lag_', plot=True, symbol='STEEM',\n",
    "                    plot_type='all_day', start_time=None, end_time=None,\n",
    "                    width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates Price Change Rate and optionally plots it.  Lag-based Wrapper function.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0 if not exists (setup, not lag operation)\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "\n",
    "    # Create necessary lag columns in wrapper function, if they don't exist\n",
    "    max_period = max(periods) if periods else 0\n",
    "    for period in range(1, max_period + 1):\n",
    "        lag_col = f'{column}{period}'\n",
    "        if lag_col not in df_wrapper.columns:\n",
    "            df_wrapper[lag_col] = df_wrapper[f'{column}0'].shift(period)\n",
    "            df_wrapper[lag_col] = df_wrapper[lag_col].fillna(method='bfill') # Backfill newly created lag columns\n",
    "            df_wrapper[lag_col] = df_wrapper[lag_col].fillna(method='ffill') # Forward fill just in case\n",
    "\n",
    "    # Time column handling (no changes)\n",
    "    time_col_wrapper = 'timestamp' if 'timestamp' in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Check for required columns (no changes)\n",
    "    if 'close' not in df_wrapper.columns:\n",
    "        print(\"Warning: 'close' column not found. Cannot calculate Price Change Rate.\")\n",
    "        return df_wrapper\n",
    "\n",
    "    # --- Core Calculation (Lag-based and Row-Independent) ---\n",
    "    df_wrapper = calculate_price_change_rate2(df_wrapper, periods, column)\n",
    "\n",
    "    # --- Plotting --- (no changes)\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'PriceChangeRate_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return df_wrapper\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filter the DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'PriceChangeRate_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Price Change Rate traces - No changes\n",
    "        for period in periods:\n",
    "            pcr_col_name = f'PriceChangeRate_{period}'\n",
    "            if pcr_col_name in plot_df.columns:\n",
    "                color = 'blue' if period == periods[0] else 'red' if period == periods[-1] else 'green'\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=plot_df[pcr_col_name], mode='lines', name=f'Price Change Rate ({period})', line=dict(color=color)))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Price Change Rate for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width, height=height, margin=dict(b=150)\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico - No changes\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Price Change Rate plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_change_rate3(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates the Price Change Rate (similar to ROC, but for price).\n",
    "    Uses lag columns and handles division by zero. Row-independent.\n",
    "    \"\"\"\n",
    "    df_pcr = df.copy()\n",
    "\n",
    "    for period in periods:\n",
    "        pcr_col_name = f'PriceChangeRate_{period}'\n",
    "        if pcr_col_name in df_pcr.columns:\n",
    "            continue\n",
    "\n",
    "        lag_col = f'{column}{period}'\n",
    "        # Ensure lag columns exist (Lag 0 is current row's data)\n",
    "        if f'{column}0' not in df_pcr.columns:\n",
    "            df_pcr[f'{column}0'] = df_pcr['close']\n",
    "        if lag_col not in df_pcr.columns:\n",
    "            print(f\"Warning: Lag column {lag_col} not found, using current close for PCR period {period} as fallback.\")\n",
    "            # Fallback to current close if lag column is missing for row-independence\n",
    "            lag_close_values = df_pcr[f'{column}0'].values\n",
    "        else:\n",
    "            lag_close_values = df_pcr[lag_col].values\n",
    "\n",
    "        current_close_values = df_pcr[f'{column}0'].values  # Current close price\n",
    "\n",
    "        # Calculate Price Change Rate using vectorized operations and handle division by zero.\n",
    "        df_pcr[pcr_col_name] = np.where(\n",
    "            lag_close_values != 0,\n",
    "            (current_close_values - lag_close_values) / lag_close_values * 100,\n",
    "            0.0  # Set PCR to 0 if lagged close is 0.\n",
    "        )\n",
    "\n",
    "    return df_pcr\n",
    "\n",
    "def calculate_price_acceleration(df, periods, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates Price Acceleration using two methods: ROC Difference and Momentum Difference. Row-independent.\n",
    "    Corrected to ensure ALL PriceAccel_ROC_Diff, PriceAccel_Momentum_Diff, and PriceMomentum columns are calculated and saved.\n",
    "    \"\"\"\n",
    "    df_accel = df.copy()\n",
    "    # --- Method 1: Difference of ROCs (Rate of Change) ---\n",
    "    df_accel = calculate_price_change_rate3(df_accel, periods, column)  # Use existing row-independent PCR function\n",
    "\n",
    "    # Corrected loops to ensure ALL combinations are processed\n",
    "    for i in range(len(periods)):  # Iterate through ALL periods for the first period of the pair\n",
    "        for j in range(len(periods)):  # Iterate through ALL periods for the second period of the pair\n",
    "            if i >= j:  # To avoid redundant calculations and cases where period1 >= period2\n",
    "                continue\n",
    "\n",
    "            period1 = periods[i]\n",
    "            period2 = periods[j]\n",
    "            roc1_col = f'PriceChangeRate_{period1}'\n",
    "            roc2_col = f'PriceChangeRate_{period2}'\n",
    "\n",
    "            if roc1_col not in df_accel.columns or roc2_col not in df_accel.columns:\n",
    "                print(f\"Warning: Could not calculate ROCs for periods {period1} and {period2}. Skipping PriceAccel_ROC_Diff.\")\n",
    "                df_accel[f'PriceAccel_ROC_Diff_{period1}_{period2}'] = np.nan  # Or some other default\n",
    "                continue\n",
    "\n",
    "            # Calculate the difference between ROCs (vectorized and row-independent)\n",
    "            df_accel[f'PriceAccel_ROC_Diff_{period1}_{period2}'] = df_accel[roc2_col] - df_accel[roc1_col]\n",
    "\n",
    "    # --- Method 2: Difference of Momentum (Price Momentum) ---\n",
    "    # --- Calculate and SAVE PriceMomentum columns in the FIRST loop ---\n",
    "    for i in range(len(periods)):  # Iterate through ALL periods to calculate PriceMomentum for each\n",
    "        period1 = periods[i]\n",
    "        lag_col1 = f'{column}{period1}'\n",
    "\n",
    "        # Ensure lag columns exist - fallback to current close if missing (for row-independence)\n",
    "        if lag_col1 not in df_accel.columns:\n",
    "            df_accel[lag_col1] = df_accel[f'{column}0']  # Fallback to current close\n",
    "        if f'{column}0' not in df_accel.columns:  # Ensure current close exists\n",
    "            df_accel[f'{column}0'] = df_accel['close']\n",
    "\n",
    "        momentum_col_name1 = f'PriceMomentum_{period1}'\n",
    "\n",
    "        # Calculate momentum using lag columns (vectorized and row-independent)\n",
    "        df_accel[momentum_col_name1] = df_accel[f'{column}0'] - df_accel[lag_col1]  # Calculate PriceMomentum and SAVE it\n",
    "        print(f\"Debug: PriceMomentum_{period1} calculated and added to df_accel\") # DEBUG PRINT\n",
    "\n",
    "\n",
    "    print(\"Debug: Columns after PriceMomentum calculation loop:\", df_accel.columns.tolist()) # DEBUG PRINT\n",
    "\n",
    "    # Calculate Momentum Difference AFTER calculating ALL PriceMomentum columns\n",
    "    for i in range(len(periods)):\n",
    "        for j in range(i + 1, len(periods)):\n",
    "            period1 = periods[i]\n",
    "            period2 = periods[j]\n",
    "            momentum_col_name1 = f'PriceMomentum_{period1}'\n",
    "            momentum_col_name2 = f'PriceMomentum_{period2}'\n",
    "\n",
    "            # Now, PriceMomentum columns SHOULD exist as they were calculated in the previous loop\n",
    "            if momentum_col_name1 not in df_accel.columns or momentum_col_name2 not in df_accel.columns:\n",
    "                print(f\"Warning: Could not calculate PriceMomentum for periods {period1} and {period2}. Skipping PriceAccel_Momentum_Diff.\")\n",
    "                df_accel[f'PriceAccel_Momentum_Diff_{period1}_{period2}'] = np.nan\n",
    "                continue\n",
    "\n",
    "            # Calculate difference of momentums (vectorized and row-independent)\n",
    "            df_accel[f'PriceAccel_Momentum_Diff_{period1}_{period2}'] = df_accel[momentum_col_name2] - df_accel[momentum_col_name1]\n",
    "\n",
    "    print(\"Debug: Columns at end of calculate_price_acceleration:\", df_accel.columns.tolist()) # DEBUG PRINT\n",
    "    return df_accel\n",
    "\n",
    "\n",
    "def PriceAcceleration(df, periods, column='close_lag_', plot=True,\n",
    "                     symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "                     end_time=None, width=1000, height=500):\n",
    "    \"\"\"\n",
    "    Calculates Price Acceleration and optionally plots it.  Lag-based Wrapper.\n",
    "    [DEBUGGING PRINT STATEMENTS ADDED]\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0 if not exists (setup, not lag operation)\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "\n",
    "    # Create lag columns in wrapper\n",
    "    max_period = max(periods) if periods else 0\n",
    "    for period in range(1, max_period + 1):\n",
    "        lag_col = f'{column}{period}'\n",
    "        if lag_col not in df_wrapper.columns:\n",
    "            df_wrapper[lag_col] = df_wrapper[f'{column}0'].shift(period)\n",
    "            df_wrapper[lag_col] = df_wrapper[lag_col].fillna(method='bfill').fillna(method='ffill') #Fill NaNs\n",
    "\n",
    "    # Time column handling (no changes)\n",
    "    time_col_wrapper = 'timestamp' if 'timestamp' in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' column not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Check for required columns (no changes)\n",
    "    if 'close' not in df_wrapper.columns:\n",
    "        print(\"Warning: 'close' column not found. Cannot calculate Price Acceleration.\")\n",
    "        return df_wrapper\n",
    "    if len(periods) < 2:\n",
    "        print(\"Warning: Price Acceleration requires at least two periods.\")\n",
    "        return df_wrapper\n",
    "\n",
    "    print(\"Debug Wrapper: Columns BEFORE calculate_price_acceleration:\", df_wrapper.columns.tolist()) # DEBUG PRINT - BEFORE CALL\n",
    "\n",
    "    # --- Core Calculation (Lag-based and Row-Independent) ---\n",
    "    df_wrapper = calculate_price_acceleration(df_wrapper, periods, column)\n",
    "\n",
    "    print(\"Debug Wrapper: Columns AFTER calculate_price_acceleration:\", df_wrapper.columns.tolist()) # DEBUG PRINT - AFTER CALL\n",
    "\n",
    "    # --- Plotting --- (no changes)\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'PriceAcceleration_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return df_wrapper\n",
    "\n",
    "        # Time range filtering (no changes)\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'PriceAcceleration_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Plot acceleration values (iterate through columns) - No changes\n",
    "        for col in plot_df.columns:\n",
    "            if col.startswith('PriceAccel_'):\n",
    "                fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=plot_df[col], mode='lines', name=col))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Price Acceleration for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width, height=height, margin=dict(b=150)\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico - No changes\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Price Acceleration plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vpt(df, column='close_lag_', volume_column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates an APPROXIMATION of Volume-Price Trend (VPT) indicator - Row-Independent.\n",
    "    This version calculates a ROW-INDEPENDENT approximation of VPT. It calculates the\n",
    "    'VPT' value for each row based only on data within that row (lagged data).\n",
    "    Note: This is NOT a true cumulative VPT, but a row-independent approximation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'close' prices, 'volume', and lag columns.\n",
    "        column (str): Prefix for the close price lag columns (e.g., 'close_lag_').\n",
    "        volume_column (str): Prefix for the volume lag columns (e.g., 'volume_lag_').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'VPT' column added (Row-Independent Approximation).\n",
    "    \"\"\"\n",
    "    df_vpt = df.copy()\n",
    "\n",
    "    # Ensure required lag columns exist\n",
    "    if f'{column}0' not in df_vpt.columns:\n",
    "        df_vpt[f'{column}0'] = df_vpt['close']  # Fallback to current close if no lag column\n",
    "    if f'{column}1' not in df_vpt.columns:\n",
    "        df_vpt[f'{column}1'] = df_vpt['close'].shift(1).fillna(df_vpt['close']) # Fallback, but shift(1) is still needed in wrapper\n",
    "    if f'{volume_column}0' not in df_vpt.columns:\n",
    "        df_vpt[f'{volume_column}0'] = df_vpt['volume'] # Fallback to current volume\n",
    "\n",
    "\n",
    "    # Calculate row-independent price change: Current Close - Previous Close (using lag columns)\n",
    "    price_change = df_vpt[f'{column}0'] - df_vpt[f'{column}1']\n",
    "\n",
    "    # Calculate VPT Approximation: Volume * Price Change (No cumulative sum anymore)\n",
    "    # This is now row-independent. Each row's VPT is based only on current and previous row's *lagged* data.\n",
    "    df_vpt['VPT'] = df_vpt[f'{volume_column}0'] * price_change\n",
    "\n",
    "    return df_vpt\n",
    "\n",
    "\n",
    "def VPT(df, plot=True, symbol='STEEM', plot_type='all_day',\n",
    "        start_time=None, end_time=None, width=1000, height=500,\n",
    "        column='close_lag_', volume_column='volume_lag_', time_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Calculates the Volume-Price Trend (VPT) indicator (Row-Independent Approximation) and optionally plots it.\n",
    "    Wrapper function.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Create close_lag_0, close_lag_1 and volume_lag_0 if not exists (setup in wrapper)\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "    if f'{column}1' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}1'] = df_wrapper[f'{column}0'].shift(1) # Shift is allowed in wrapper\n",
    "        df_wrapper[f'{column}1'] = df_wrapper[f'{column}1'].fillna(df_wrapper[f'{column}0']) # Fill NaN from shift\n",
    "    if f'{volume_column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{volume_column}0'] = df_wrapper['volume']\n",
    "\n",
    "    # Time column handling (no changes)\n",
    "    time_col_wrapper = time_col if time_col in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Check for required columns (no changes)\n",
    "    if not ('close' in df_wrapper.columns and 'volume' in df_wrapper.columns):\n",
    "        print(\"Warning: 'close' and 'volume' columns are required. Returning original DataFrame.\")\n",
    "        return df_wrapper\n",
    "\n",
    "    df_wrapper = calculate_vpt(df_wrapper, column, volume_column) # Call core calculation\n",
    "\n",
    "    # --- Plotting --- (mostly no changes, just title update)\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'VPT_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return df_wrapper\n",
    "\n",
    "        # Time range filtering (no changes)\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'VPT_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # VPT - No changes\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=plot_df[time_col_wrapper], y=plot_df['VPT'], mode='lines', name='VPT (Approx)', # Updated name in plot\n",
    "                      line=dict(color='blue')))\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Volume-Price Trend (VPT - Approx) for {symbol}{title_suffix}</b>', # Updated title\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width, height=height, margin=dict(b=150)\n",
    "        )\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico - No changes\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"VPT plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df_wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFI plot saved to graficos\\MFI_symbol_STEEM.html\n",
      "MFI plot saved to graficos\\MFI_symbol_STEEM_time_range_12-00-13-00.html\n",
      "Nombre de columnes en df: 38\n",
      "Nombre de columnes en df2: 37\n"
     ]
    }
   ],
   "source": [
    "def calculate_mfi(df, mfi_period=14, column='close_lag_', volume_column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates an APPROXIMATION of Money Flow Index (MFI) - Row-Independent.\n",
    "    This version calculates a ROW-INDEPENDENT approximation of MFI using a lagged window for money flow sums.\n",
    "    Note: This is NOT a true Rolling MFI with cumulative sums, but a row-independent approximation.\n",
    "    \"\"\"\n",
    "    df_mfi = df.copy()\n",
    "\n",
    "    # 1. Calculate Typical Price (no changes - row-independent)\n",
    "    df_mfi['typical_price'] = (df_mfi['high'] + df_mfi['low'] + df_mfi['close']) / 3\n",
    "\n",
    "    # 2. Calculate Raw Money Flow (no changes - row-independent)\n",
    "    df_mfi['raw_money_flow'] = df_mfi['typical_price'] * df_mfi['volume']\n",
    "\n",
    "    # 3. Approximate Positive and Negative Money Flow using lagged price differences\n",
    "    # Initialize lists to store money flow approximations for the lag window\n",
    "    positive_mf_values_list = []\n",
    "    negative_mf_values_list = []\n",
    "\n",
    "    # Iterate through the lag window (from 0 to mfi_period - 1)\n",
    "    for lag in range(mfi_period):\n",
    "        lag_close_col = f'{column}{lag}'\n",
    "        lag_close_prev_col = f'{column}{lag + 1}' # Compare to the *previous* bar in the lag window\n",
    "        lag_volume_col = f'{volume_column}{lag}'\n",
    "\n",
    "        # Ensure lag columns exist - fallback to current close and volume if missing (for row-independence)\n",
    "        current_close_col = lag_close_col if lag_close_col in df_mfi.columns else f'{column}0'\n",
    "        previous_close_col = lag_close_prev_col if lag_close_prev_col in df_mfi.columns else f'{column}1' #lag 1 as \"previous\"\n",
    "        current_volume_col = lag_volume_col if lag_volume_col in df_mfi.columns else 'volume_lag_0'\n",
    "\n",
    "\n",
    "        # Get close, previous close and volume values for the CURRENT LAG - Row Independent access\n",
    "        close_values = df_mfi[current_close_col].values\n",
    "        prev_close_values = df_mfi[previous_close_col].values\n",
    "        volume_values = df_mfi[current_volume_col].values\n",
    "\n",
    "\n",
    "        # Calculate price difference for the current lag (row-independent)\n",
    "        price_diff = close_values - prev_close_values\n",
    "\n",
    "        # Calculate positive and negative money flow for the current lag (row-independent)\n",
    "        positive_money_flow = np.where(price_diff > 0, volume_values * (df_mfi['typical_price']).values, 0.0) # Use .values for array-like *\n",
    "        negative_money_flow = np.where(price_diff < 0, volume_values* (df_mfi['typical_price']).values, 0.0) # Use .values for array-like *\n",
    "\n",
    "        positive_mf_values_list.append(positive_money_flow)\n",
    "        negative_mf_values_list.append(negative_money_flow)\n",
    "\n",
    "    # 4. Approximate Money Flow Sums - Row-Independent Averaging over Lag Window\n",
    "    # Calculate average positive and negative money flow over the lag window\n",
    "    positive_mf_sum_approx = np.nanmean(np.stack(positive_mf_values_list), axis=0)\n",
    "    negative_mf_sum_approx = np.nanmean(np.stack(negative_mf_values_list), axis=0)\n",
    "\n",
    "    df_mfi['positive_mf_sum'] = positive_mf_sum_approx # Assign approximated sums to DataFrame\n",
    "    df_mfi['negative_mf_sum'] = negative_mf_sum_approx\n",
    "\n",
    "\n",
    "    # 5. Calculate Money Flow Ratio and MFI (using approximated lagged sums) - Vectorized and Row-Independent\n",
    "    money_flow_ratio = np.where(df_mfi['negative_mf_sum'] != 0, df_mfi['positive_mf_sum'] / df_mfi['negative_mf_sum'], 1e9)\n",
    "    df_mfi[f'MFI_{mfi_period}'] = 100 - (100 / (1 + money_flow_ratio))\n",
    "\n",
    "    # 6. Drop intermediate columns (no changes)\n",
    "    df_mfi.drop(columns=['typical_price', 'raw_money_flow', 'price_diff',\n",
    "                     'positive_mf_sum', 'negative_mf_sum'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "    return df_mfi\n",
    "\n",
    "\n",
    "def MFI(df, mfi_period=14, plot=True, symbol='STEEM',\n",
    "        plot_type='all_day', start_time=None, end_time=None,\n",
    "        width=1000, height=500, column='close_lag_', volume_column='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates the Money Flow Index (MFI) (Row-Independent Approximation) and optionally plots it.\n",
    "    Wrapper function.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Ensure required lag columns are created in wrapper\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "    if f'{column}1' not in df_wrapper.columns: # Need lag 1 for price difference calculation\n",
    "        df_wrapper[f'{column}1'] = df_wrapper[f'{column}0'].shift(1)\n",
    "        df_wrapper[f'{column}1'] = df_wrapper[f'{column}1'].fillna(df_wrapper[f'{column}0']) # Fill NaN from shift\n",
    "    if f'{volume_column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{volume_column}0'] = df_wrapper['volume']\n",
    "\n",
    "    # Time column handling (no changes)\n",
    "    time_col_wrapper = 'timestamp' if 'timestamp' in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Check for required columns (no changes)\n",
    "    if not {'high', 'low', 'close', 'volume'}.issubset(df_wrapper.columns):\n",
    "        print(\"Warning: 'high', 'low', 'close', and 'volume' columns are required. Returning original DataFrame.\")\n",
    "        return df_wrapper\n",
    "\n",
    "\n",
    "    df_wrapper = calculate_mfi(df_wrapper, mfi_period, column, volume_column) # Call core calculation\n",
    "\n",
    "    # --- Plotting --- (mostly no changes, just title update)\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'MFI_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        # Filtrar por rango de tiempo si es necesario (no changes)\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'MFI_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # MFI - No changes\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=plot_df[time_col_wrapper], y=plot_df[f'MFI_{mfi_period}'], mode = 'lines', name=f'MFI ({mfi_period} - Approx)', # Updated name\n",
    "                      line=dict(color='blue')))\n",
    "\n",
    "        # Add overbought/oversold lines (typically at 80 and 20) - No changes\n",
    "        fig.add_hline(y=80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Overbought\", annotation_position=\"bottom right\")\n",
    "        fig.add_hline(y=20, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Oversold\", annotation_position=\"bottom right\")\n",
    "\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Money Flow Index (MFI - Approx) for {symbol}{title_suffix}</b>', # Updated title\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='MFI Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width, height=height, margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico - No changes\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"MFI plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df_wrapper\n",
    "\n",
    "df = MFI(df, plot_type='all_day', symbol='STEEM')\n",
    "df = MFI(df, mfi_period=20, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "df2 = MFI(df2, plot=False)\n",
    "\n",
    "match_columns = compare_dataframes_row(df, df2, symbol_col=symbol_col, timestamp_col=timestamp_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var(df, var_period=1, confidence_level=0.95, column='close_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates Historical and Parametric Value at Risk (VaR) using LAGS - Row-Independent.\n",
    "    Approximates Historical VaR using lagged returns available in each row.\n",
    "    Parametric VaR remains a static, dataset-level calculation (as per original code).\n",
    "    \"\"\"\n",
    "    df_var = df.copy()\n",
    "\n",
    "    # Calculate returns (percentage change) using lag columns. Row-independent.\n",
    "    if f'{column}1' not in df.columns or f'{column}0' not in df.columns:\n",
    "        print(\"Warning: Required lag columns close_lag_0 and close_lag_1 not found for VaR calculation.\")\n",
    "        df_var['Historical_VaR'] = np.nan\n",
    "        df_var['Parametric_VaR'] = np.nan\n",
    "        return df_var\n",
    "\n",
    "\n",
    "    df_var['returns'] = (df_var[f'{column}0'] - df_var[f'{column}1']) / df_var[f'{column}1'] * 100\n",
    "    df_var['returns'] = df_var['returns'].fillna(0)  # Replace initial NaN with 0 - Row-Independent\n",
    "\n",
    "    # --- Historical VaR (Lagged and Row-Independent Approximation) ---\n",
    "    # Use pre-calculated lagged returns columns directly.\n",
    "    return_lags_cols = [f'returns_lag_{i}' for i in range(1, var_period + 1)]\n",
    "\n",
    "    # Check if lag columns exist; if not, return NaN for Historical VaR\n",
    "    missing_lag_returns = [col for col in return_lags_cols if col not in df_var.columns]\n",
    "    if missing_lag_returns:\n",
    "        print(f\"Warning: Missing lag columns for Historical VaR: {missing_lag_returns}. Returning NaN for Historical_VaR.\")\n",
    "        df_var['Historical_VaR'] = np.nan\n",
    "    else:\n",
    "        # Calculate Historical VaR using pre-calculated lagged returns (row-independent)\n",
    "        df_var['Historical_VaR'] = -df_var[return_lags_cols].quantile(1 - confidence_level, axis=1)\n",
    "\n",
    "\n",
    "    # --- Parametric VaR (assuming normal distribution) - Dataset-Level Calculation (Not Row-Independent) ---\n",
    "    # Parametric VaR, in its standard form, is a *single, constant value*\n",
    "    # for the entire dataset. It does *not* change row to row and thus is \"row-independent\" in that sense of being the same for every row.\n",
    "    mean_return = df_var['returns'].mean()  # Mean of the *entire* returns series (ALLOWED - dataset-level statistic)\n",
    "    std_dev_return = df_var['returns'].std() # Standard deviation of the *entire* series (ALLOWED - dataset-level statistic)\n",
    "\n",
    "    # Calculate the critical value (z-score) - No changes\n",
    "    alpha = 1 - confidence_level\n",
    "    z_critical = norm.ppf(alpha)\n",
    "\n",
    "    # Calculate Parametric VaR and store it (constant value) - No changes\n",
    "    df_var['Parametric_VaR'] = -(mean_return + z_critical * std_dev_return)\n",
    "\n",
    "    # --- Clean Up --- (no changes)\n",
    "    df_var.drop(columns=['returns'], inplace=True)\n",
    "\n",
    "    return df_var\n",
    "\n",
    "\n",
    "\n",
    "def VaR(df, var_period=1, confidence_level=0.95, column='close_lag_',\n",
    "         plot=True, symbol='STEEM', plot_type='all_day', start_time=None,\n",
    "         end_time=None, width=1000, height=500, time_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Calculates Historical and Parametric Value at Risk (VaR) and\n",
    "    optionally plots them.  Uses a lag-based approach for Historical VaR.\n",
    "    Wrapper function.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Time column handling (no changes)\n",
    "    time_col_wrapper = 'timestamp' if time_col in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Check for required columns (no changes)\n",
    "    if 'close' not in df_wrapper.columns:\n",
    "        print(\"Warning: 'close' column not found.  Cannot calculate VaR.\")\n",
    "        return df_wrapper\n",
    "\n",
    "     # Create close_lag_0 and close_lag_1 if not exists (setup, not lag operation)\n",
    "    if f'{column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "    if f'{column}1' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{column}1'] = df_wrapper[f'{column}0'].shift(1)\n",
    "        df_wrapper[f'{column}1'] = df_wrapper[f'{column}1'].fillna(df_wrapper[f'{column}0']) # Fill NaN from shift\n",
    "\n",
    "    # Create lag columns for returns (needed for Historical VaR approximation)\n",
    "    if 'returns' not in df_wrapper.columns: # Calculate returns if not pre-calculated\n",
    "        df_wrapper['returns'] = (df_wrapper[f'{column}0'] - df_wrapper[f'{column}1']) / df_wrapper[f'{column}1'] * 100\n",
    "        df_wrapper['returns'] = df_wrapper['returns'].fillna(0)\n",
    "\n",
    "    for i in range(1, var_period + 1): # Create returns lag columns in wrapper\n",
    "        if f'returns_lag_{i}' not in df_wrapper.columns:\n",
    "            df_wrapper[f'returns_lag_{i}'] = df_wrapper['returns'].shift(i)\n",
    "            df_wrapper[f'returns_lag_{i}'] = df_wrapper[f'returns_lag_{i}'].fillna(0) # Fill NaNs in lag columns\n",
    "\n",
    "    df_wrapper = calculate_var(df_wrapper, var_period, confidence_level, column) # Call core calculation\n",
    "\n",
    "    # --- Plotting --- (no changes)\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'VaR_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "        if plot_df.empty:\n",
    "            print(f\"No data for symbol {symbol}\")\n",
    "            return\n",
    "\n",
    "        # Time range filtering (no changes)\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convertir strings de tiempo a objetos time\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) &\n",
    "                              (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'VaR_symbol_{symbol}_time_range_{time_str}'\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Historical VaR (multiply by close price for plotting)\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=-plot_df['Historical_VaR'] * plot_df['close'], mode='lines', name='Historical VaR', line=dict(color='red')))\n",
    "\n",
    "        # Parametric VaR (constant value, horizontal line * close price)\n",
    "        # We use close price of the first row to have a value to multiply\n",
    "        fig.add_trace(go.Scatter(x=plot_df[time_col_wrapper], y=[-plot_df['Parametric_VaR'].iloc[0] * plot_df['close'].iloc[0]] * len(plot_df), mode='lines',  name='Parametric VaR',\n",
    "                                  line=dict(color='blue', dash='dash')))  # Use a constant value\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Value at Risk (VaR) for {symbol}{title_suffix}</b>',\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width, height=height, margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"VaR plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_herding_behavior(df, momentum_period=10,\n",
    "                               price_col='close_lag_', volume_col='volume_lag_'):\n",
    "    \"\"\"\n",
    "    Calculates a ROW-INDEPENDENT proxy for herding behavior using only pre-calculated lag columns.\n",
    "    Simplified to use ONLY momentum direction alignment as the proxy (rolling correlation removed for row-independence).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with pre-calculated lag columns for 'close' and 'volume'.\n",
    "        momentum_period (int): Period for momentum calculation (using lags).\n",
    "        price_col (str): Prefix for price lag columns.\n",
    "        volume_col (str): Prefix for volume lag columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'HerdingBehaviorProxy' column added (Row-Independent, Momentum-Based Proxy).\n",
    "    \"\"\"\n",
    "    df_herding = df.copy()\n",
    "\n",
    "    # --- 1. Calculate Price and Volume Momentum (using pre-calculated lags) ---\n",
    "    # Ensure lag columns exist\n",
    "    if f'{price_col}0' not in df.columns or f'{price_col}{momentum_period}' not in df.columns:\n",
    "        print(f\"Warning: Price lag columns (0 and {momentum_period}) not found. Skipping herding calculation.\")\n",
    "        df_herding['HerdingBehaviorProxy'] = np.nan # Return NaN if essential data is missing\n",
    "        return df_herding\n",
    "    if f'{volume_col}0' not in df.columns or f'{volume_col}{momentum_period}' not in df.columns:\n",
    "        print(f\"Warning: Volume lag columns (0 and {momentum_period}) not found. Skipping herding calculation.\")\n",
    "        df_herding['HerdingBehaviorProxy'] = np.nan # Return NaN if essential data is missing\n",
    "        return df_herding\n",
    "\n",
    "\n",
    "    df_herding['price_momentum'] = df_herding[f'{price_col}0'] - df_herding[f'{price_col}{momentum_period}']\n",
    "    df_herding['volume_momentum'] = df_herding[f'{volume_col}0'] - df_herding[f'{volume_col}{momentum_period}']\n",
    "\n",
    "    # --- 2. Check if Momentum is in the Same Direction (Row-Independent) ---\n",
    "    df_herding['momentum_same_direction'] = (np.sign(df_herding['price_momentum']) == np.sign(df_herding['volume_momentum'])).astype(int)\n",
    "\n",
    "    # --- 3. Herding Behavior Proxy (Simplified - Momentum Direction Only) ---\n",
    "    # Herding Behavior Proxy is now ONLY based on momentum direction alignment (rolling correlation removed)\n",
    "    df_herding['HerdingBehaviorProxy'] = df_herding['momentum_same_direction'] # Simplified proxy - Momentum Direction Only\n",
    "\n",
    "    # Clean up intermediate columns\n",
    "    df_herding.drop(columns=['price_momentum', 'volume_momentum', 'momentum_same_direction'], inplace=True, errors='ignore')\n",
    "\n",
    "    return df_herding\n",
    "\n",
    "\n",
    "def HerdingBehavior(df, correlation_window=20, momentum_period=10,\n",
    "                    plot=True, symbol='STEEM', plot_type='all_day',\n",
    "                    start_time=None, end_time=None, width=1000, height=500,\n",
    "                    column='close_lag_', volume_column='volume_lag_', time_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Calculates a proxy for herding behavior (Row-Independent, Momentum-Based Proxy) and optionally plots it.\n",
    "    Wrapper function.\n",
    "    \"\"\"\n",
    "    df_wrapper = df.copy()\n",
    "\n",
    "    if df_wrapper.empty:\n",
    "        print(\"Warning: Empty DataFrame provided.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure RangeIndex\n",
    "    if not isinstance(df_wrapper.index, pd.RangeIndex) or not df_wrapper.index.is_monotonic_increasing or df_wrapper.index.step != 1:\n",
    "        df_wrapper = df_wrapper.reset_index(drop=True)\n",
    "\n",
    "    # Time column handling (no changes)\n",
    "    time_col_wrapper = time_col if time_col in df_wrapper.columns else 'time'\n",
    "    if plot and time_col_wrapper not in df_wrapper.columns:\n",
    "        print(f\"Warning: Time column '{time_col_wrapper}' not found. Plotting will be disabled.\")\n",
    "        plot = False\n",
    "    elif plot and not pd.api.types.is_datetime64_any_dtype(df_wrapper[time_col_wrapper]):\n",
    "        try:\n",
    "            df_wrapper[time_col_wrapper] = pd.to_datetime(df_wrapper[time_col_wrapper])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col_wrapper}' to datetime: {e}. Plotting will be disabled.\")\n",
    "            plot = False\n",
    "\n",
    "    # Check for required columns (no changes)\n",
    "    if not all(col in df_wrapper.columns for col in ['close', 'volume']):\n",
    "        print(\"Warning: 'close' and 'volume' columns are required. Returning original DataFrame.\")\n",
    "        return df_wrapper\n",
    "\n",
    "    # --- Create lag columns for price and volume if they don't exist ---\n",
    "    max_lag_needed =  momentum_period # Only momentum period lags are needed now\n",
    "\n",
    "    if f'{column}0' not in df_wrapper.columns:  # Create close_lag_0 if it doesn't exist\n",
    "        df_wrapper[f'{column}0'] = df_wrapper['close']\n",
    "    if f'{volume_column}0' not in df_wrapper.columns:\n",
    "        df_wrapper[f'{volume_column}0'] = df_wrapper['volume']\n",
    "\n",
    "    for i in range(max_lag_needed + 1): # Only create momentum period lags now\n",
    "        if f'{column}{i}' not in df_wrapper.columns:\n",
    "            df_wrapper[f'{column}{i}'] = df_wrapper[f'{column}0'].shift(i) # Create close lags\n",
    "        if f'{volume_column}{i}' not in df_wrapper.columns:\n",
    "            df_wrapper[f'{volume_column}{i}'] = df_wrapper[f'{volume_column}0'].shift(i) # Create volume lags\n",
    "\n",
    "\n",
    "    df_wrapper.fillna(0, inplace=True)  # Replace any remaining NaNs after lag creation\n",
    "\n",
    "    df_wrapper = calculate_herding_behavior(df_wrapper, momentum_period, column, volume_column) # Call core calculation\n",
    "\n",
    "    # --- Plotting --- (Plotting code remains mostly the same, just adjusted for simplified proxy)\n",
    "    if plot and 'symbol' in df_wrapper.columns:\n",
    "        plot_filename = f'HerdingBehavior_symbol_{symbol}'\n",
    "        plot_df = df_wrapper[df_wrapper['symbol'] == symbol].copy()\n",
    "\n",
    "        if plot_df.empty:\n",
    "            print(f\"Warning: No data for symbol {symbol}. Plotting disabled.\")\n",
    "            return df_wrapper\n",
    "\n",
    "        if plot_type == 'time_range' and start_time and end_time:\n",
    "            # Convert strings to time objects (no changes)\n",
    "            if isinstance(start_time, str) and ':' in start_time:\n",
    "                start_hour, start_minute = map(int, start_time.split(':')[:2])\n",
    "                start_time_obj = pd.Timestamp('2000-01-01').replace(hour=start_hour, minute=start_minute).time()\n",
    "            elif isinstance(start_time, str):\n",
    "                start_time_obj = pd.to_datetime(start_time).time()\n",
    "            else:\n",
    "                start_time_obj = start_time.time() if hasattr(start_time, 'time') else start_time\n",
    "\n",
    "            if isinstance(end_time, str) and ':' in end_time:\n",
    "                end_hour, end_minute = map(int, end_time.split(':')[:2])\n",
    "                end_time_obj = pd.Timestamp('2000-01-01').replace(hour=end_hour, minute=end_minute).time()\n",
    "            elif isinstance(end_time, str):\n",
    "                end_time_obj = pd.to_datetime(end_time).time()\n",
    "            else:\n",
    "                end_time_obj = end_time.time() if hasattr(end_time, 'time') else end_time\n",
    "\n",
    "            plot_df = plot_df[(plot_df[time_col_wrapper].dt.time >= start_time_obj) & (plot_df[time_col_wrapper].dt.time <= end_time_obj)]\n",
    "            time_str = f\"{start_time_obj.strftime('%H-%M')}-{end_time_obj.strftime('%H-%M')}\"\n",
    "            plot_filename = f'HerdingBehavior_symbol_{symbol}_time_range_{time_str}'\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Herding Behavior Proxy - Updated name in plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=plot_df[time_col_wrapper], y=plot_df['HerdingBehaviorProxy'], mode='lines', name='Herding Behavior Proxy (Momentum-Based)',\n",
    "                        line=dict(color='red')))\n",
    "\n",
    "\n",
    "        title_suffix = \" - All Day\" if plot_type == 'all_day' else f\" - {start_time} to {end_time}\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'<b>Herding Behavior Proxy (Momentum-Based) for {symbol}{title_suffix}</b>', # Updated title\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "            },\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Value',\n",
    "            xaxis_rangeslider_visible=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-1.10,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            margin=dict(b=150),\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showgrid=False)\n",
    "        fig.update_yaxes(showgrid=False)\n",
    "\n",
    "        # Guardar gr√°fico - No changes\n",
    "        try:\n",
    "            os.makedirs('graficos', exist_ok=True)\n",
    "            plot_filepath = os.path.join('graficos', f'{plot_filename}.html')\n",
    "            fig.write_html(plot_filepath, auto_open=False)\n",
    "            print(f\"Herding Behavior plot saved to {plot_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot: {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indicadors_df = True  \n",
    "\n",
    "\n",
    "\n",
    "if indicadors_df:\n",
    "\n",
    "    df = generate_lags_and_leads(df, 30, 15)\n",
    "\n",
    "\n",
    "    print(\"Aplicando indicadores a df\")\n",
    "\n",
    "    graficos_dir = \"graficos\"\n",
    "    if not os.path.exists(graficos_dir):\n",
    "        os.makedirs(graficos_dir)\n",
    "\n",
    "    df = SMA(df, indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = SMA(df, indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = EMA(df, indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = EMA(df, indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = WMA(df, indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = WMA(df, indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = RSI(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = RSI(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = StochasticOscillator(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = StochasticOscillator(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00:00', end_time='13:00:00')\n",
    "\n",
    "    df = calculate_macd(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = calculate_macd(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = WilliamsR(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = WilliamsR(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = ATR_row_independent(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = ATR_row_independent(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = BollingerBands(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = BollingerBands(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = OBV(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = OBV(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = VolumeROC(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = VolumeROC(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = VolumeEMA(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = VolumeEMA(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='10:00', end_time='13:00')\n",
    "\n",
    "    df = Doji(df, doji_threshold=0.1, plot_type='all_day', symbol='STEEM')\n",
    "    df = Doji(df, doji_threshold=0.05, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = HammerHangingMan(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = HammerHangingMan(df, body_multiplier=2.5, upper_shadow_max=0.2, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = Engulfing(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = Engulfing(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = PiercingDarkCloud(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = PiercingDarkCloud(df, penetration_threshold=0.6, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = ThreeSoldiersCrows(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = ThreeSoldiersCrows(df, body_min_size=0.5, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = RollingMedian(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = RollingMedian(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = RollingStdDev(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = RollingStdDev(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = LiquidityGaps(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = LiquidityGaps(df, atr_period=20, volume_ratio_threshold=0.4, atr_threshold_multiplier=2.5,plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = TakerBuySellRatio(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = TakerBuySellRatio(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = NumTradesMomentum(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = NumTradesMomentum(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = LaggedMaxDrawdown(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = LaggedMaxDrawdown(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = PriceChangeRate(df, periods=indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = PriceChangeRate(df, periods=indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = RollingStdDev(df, indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = RollingStdDev(df, indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "    df = RollingKurtosis(df, indicator_periods, plot_type='all_day', symbol='STEEM')\n",
    "    df = RollingKurtosis(df, indicator_periods, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "    df = ADX(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = ADX(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "    df = VolumeSpike(df, column='volume_lag_', plot_type='all_day', symbol='STEEM')\n",
    "    df = VolumeSpike(df, column='volume_lag_', plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "    df = VPT(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = VPT(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "\n",
    "    df = MFI(df, plot_type='all_day', symbol='STEEM')\n",
    "    df = MFI(df, plot_type='time_range', symbol='STEEM', start_time='12:00', end_time='13:00')\n",
    "    df.to_csv(os.path.join(output_dir, 'df_con_indicadores.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando Lags y Leads:  19%|‚ñà‚ñä        | 52/280 [00:00<00:00, 517.34it/s]C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "Generando Lags y Leads:  38%|‚ñà‚ñà‚ñà‚ñä      | 107/280 [00:00<00:00, 532.35it/s]C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "Generando Lags y Leads:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 161/280 [00:00<00:00, 501.85it/s]C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "Generando Lags y Leads:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 212/280 [00:00<00:00, 479.51it/s]C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "Generando Lags y Leads:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 261/280 [00:00<00:00, 480.78it/s]C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:12: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\forad\\AppData\\Local\\Temp\\ipykernel_15732\\3329080057.py:18: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "Generando Lags y Leads: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 280/280 [00:00<00:00, 489.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando indicadores a df2\n",
      "Debug Wrapper: Columns BEFORE calculate_price_acceleration: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'symbol', 'volume_lag_0', 'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_4', 'volume_lag_5', 'volume_lag_6', 'volume_lag_7', 'volume_lag_8', 'volume_lag_9', 'volume_lag_10', 'volume_lag_11', 'volume_lag_12', 'volume_lag_13', 'volume_lag_14', 'volume_lag_15', 'volume_lag_16', 'volume_lag_17', 'volume_lag_18', 'volume_lag_19', 'Volume_RollingMean', 'Volume_RollingStdDev', 'VolumeSpike', 'close_lag_0', 'close_lag_1', 'MFI_14', 'MFI_20', 'open_lag_0', 'high_lag_0', 'low_lag_0', 'quote_asset_volume_lag_0', 'number_of_trades_lag_0', 'taker_buy_base_asset_volume_lag_0', 'taker_buy_quote_asset_volume_lag_0', 'open_lag_1', 'high_lag_1', 'low_lag_1', 'quote_asset_volume_lag_1', 'number_of_trades_lag_1', 'taker_buy_base_asset_volume_lag_1', 'taker_buy_quote_asset_volume_lag_1', 'close_lag_2', 'open_lag_2', 'high_lag_2', 'low_lag_2', 'quote_asset_volume_lag_2', 'number_of_trades_lag_2', 'taker_buy_base_asset_volume_lag_2', 'taker_buy_quote_asset_volume_lag_2', 'close_lag_3', 'open_lag_3', 'high_lag_3', 'low_lag_3', 'quote_asset_volume_lag_3', 'number_of_trades_lag_3', 'taker_buy_base_asset_volume_lag_3', 'taker_buy_quote_asset_volume_lag_3', 'close_lag_4', 'open_lag_4', 'high_lag_4', 'low_lag_4', 'quote_asset_volume_lag_4', 'number_of_trades_lag_4', 'taker_buy_base_asset_volume_lag_4', 'taker_buy_quote_asset_volume_lag_4', 'close_lag_5', 'open_lag_5', 'high_lag_5', 'low_lag_5', 'quote_asset_volume_lag_5', 'number_of_trades_lag_5', 'taker_buy_base_asset_volume_lag_5', 'taker_buy_quote_asset_volume_lag_5', 'close_lag_6', 'open_lag_6', 'high_lag_6', 'low_lag_6', 'quote_asset_volume_lag_6', 'number_of_trades_lag_6', 'taker_buy_base_asset_volume_lag_6', 'taker_buy_quote_asset_volume_lag_6', 'close_lag_7', 'open_lag_7', 'high_lag_7', 'low_lag_7', 'quote_asset_volume_lag_7', 'number_of_trades_lag_7', 'taker_buy_base_asset_volume_lag_7', 'taker_buy_quote_asset_volume_lag_7', 'close_lag_8', 'open_lag_8', 'high_lag_8', 'low_lag_8', 'quote_asset_volume_lag_8', 'number_of_trades_lag_8', 'taker_buy_base_asset_volume_lag_8', 'taker_buy_quote_asset_volume_lag_8', 'close_lag_9', 'open_lag_9', 'high_lag_9', 'low_lag_9', 'quote_asset_volume_lag_9', 'number_of_trades_lag_9', 'taker_buy_base_asset_volume_lag_9', 'taker_buy_quote_asset_volume_lag_9', 'close_lag_10', 'open_lag_10', 'high_lag_10', 'low_lag_10', 'quote_asset_volume_lag_10', 'number_of_trades_lag_10', 'taker_buy_base_asset_volume_lag_10', 'taker_buy_quote_asset_volume_lag_10', 'close_lag_11', 'open_lag_11', 'high_lag_11', 'low_lag_11', 'quote_asset_volume_lag_11', 'number_of_trades_lag_11', 'taker_buy_base_asset_volume_lag_11', 'taker_buy_quote_asset_volume_lag_11', 'close_lag_12', 'open_lag_12', 'high_lag_12', 'low_lag_12', 'quote_asset_volume_lag_12', 'number_of_trades_lag_12', 'taker_buy_base_asset_volume_lag_12', 'taker_buy_quote_asset_volume_lag_12', 'close_lag_13', 'open_lag_13', 'high_lag_13', 'low_lag_13', 'quote_asset_volume_lag_13', 'number_of_trades_lag_13', 'taker_buy_base_asset_volume_lag_13', 'taker_buy_quote_asset_volume_lag_13', 'close_lag_14', 'open_lag_14', 'high_lag_14', 'low_lag_14', 'quote_asset_volume_lag_14', 'number_of_trades_lag_14', 'taker_buy_base_asset_volume_lag_14', 'taker_buy_quote_asset_volume_lag_14', 'close_lag_15', 'open_lag_15', 'high_lag_15', 'low_lag_15', 'quote_asset_volume_lag_15', 'number_of_trades_lag_15', 'taker_buy_base_asset_volume_lag_15', 'taker_buy_quote_asset_volume_lag_15', 'close_lag_16', 'open_lag_16', 'high_lag_16', 'low_lag_16', 'quote_asset_volume_lag_16', 'number_of_trades_lag_16', 'taker_buy_base_asset_volume_lag_16', 'taker_buy_quote_asset_volume_lag_16', 'close_lag_17', 'open_lag_17', 'high_lag_17', 'low_lag_17', 'quote_asset_volume_lag_17', 'number_of_trades_lag_17', 'taker_buy_base_asset_volume_lag_17', 'taker_buy_quote_asset_volume_lag_17', 'close_lag_18', 'open_lag_18', 'high_lag_18', 'low_lag_18', 'quote_asset_volume_lag_18', 'number_of_trades_lag_18', 'taker_buy_base_asset_volume_lag_18', 'taker_buy_quote_asset_volume_lag_18', 'close_lag_19', 'open_lag_19', 'high_lag_19', 'low_lag_19', 'quote_asset_volume_lag_19', 'number_of_trades_lag_19', 'taker_buy_base_asset_volume_lag_19', 'taker_buy_quote_asset_volume_lag_19', 'close_lag_20', 'volume_lag_20', 'open_lag_20', 'high_lag_20', 'low_lag_20', 'quote_asset_volume_lag_20', 'number_of_trades_lag_20', 'taker_buy_base_asset_volume_lag_20', 'taker_buy_quote_asset_volume_lag_20', 'close_lag_21', 'volume_lag_21', 'open_lag_21', 'high_lag_21', 'low_lag_21', 'quote_asset_volume_lag_21', 'number_of_trades_lag_21', 'taker_buy_base_asset_volume_lag_21', 'taker_buy_quote_asset_volume_lag_21', 'close_lag_22', 'volume_lag_22', 'open_lag_22', 'high_lag_22', 'low_lag_22', 'quote_asset_volume_lag_22', 'number_of_trades_lag_22', 'taker_buy_base_asset_volume_lag_22', 'taker_buy_quote_asset_volume_lag_22', 'close_lag_23', 'volume_lag_23', 'open_lag_23', 'high_lag_23', 'low_lag_23', 'quote_asset_volume_lag_23', 'number_of_trades_lag_23', 'taker_buy_base_asset_volume_lag_23', 'taker_buy_quote_asset_volume_lag_23', 'close_lag_24', 'volume_lag_24', 'open_lag_24', 'high_lag_24', 'low_lag_24', 'quote_asset_volume_lag_24', 'number_of_trades_lag_24', 'taker_buy_base_asset_volume_lag_24', 'taker_buy_quote_asset_volume_lag_24', 'close_lag_25', 'volume_lag_25', 'open_lag_25', 'high_lag_25', 'low_lag_25', 'quote_asset_volume_lag_25', 'number_of_trades_lag_25', 'taker_buy_base_asset_volume_lag_25', 'taker_buy_quote_asset_volume_lag_25', 'close_lag_26', 'volume_lag_26', 'open_lag_26', 'high_lag_26', 'low_lag_26', 'quote_asset_volume_lag_26', 'number_of_trades_lag_26', 'taker_buy_base_asset_volume_lag_26', 'taker_buy_quote_asset_volume_lag_26', 'close_lag_27', 'volume_lag_27', 'open_lag_27', 'high_lag_27', 'low_lag_27', 'quote_asset_volume_lag_27', 'number_of_trades_lag_27', 'taker_buy_base_asset_volume_lag_27', 'taker_buy_quote_asset_volume_lag_27', 'close_lag_28', 'volume_lag_28', 'open_lag_28', 'high_lag_28', 'low_lag_28', 'quote_asset_volume_lag_28', 'number_of_trades_lag_28', 'taker_buy_base_asset_volume_lag_28', 'taker_buy_quote_asset_volume_lag_28', 'close_lag_29', 'volume_lag_29', 'open_lag_29', 'high_lag_29', 'low_lag_29', 'quote_asset_volume_lag_29', 'number_of_trades_lag_29', 'taker_buy_base_asset_volume_lag_29', 'taker_buy_quote_asset_volume_lag_29', 'close_lag_30', 'volume_lag_30', 'open_lag_30', 'high_lag_30', 'low_lag_30', 'quote_asset_volume_lag_30', 'number_of_trades_lag_30', 'taker_buy_base_asset_volume_lag_30', 'taker_buy_quote_asset_volume_lag_30', 'close_lead_0', 'RollingStdDev_5', 'RollingStdDev_10', 'RollingStdDev_15', 'RollingStdDev_20', 'RollingStdDev_25', 'RollingStdDev_30', 'RollingKurtosis_5', 'RollingKurtosis_10', 'RollingKurtosis_15', 'RollingKurtosis_20', 'RollingKurtosis_25', 'RollingKurtosis_30', 'Plus_DI', 'Minus_DI', 'ADX', 'tr_smoothed_lag_1', 'plus_dm_smoothed_lag_1', 'minus_dm_smoothed_lag_1', 'dx_smoothed_lag_1', 'PriceChangeRate_5', 'PriceChangeRate_10', 'PriceChangeRate_15', 'PriceChangeRate_20', 'PriceChangeRate_25', 'PriceChangeRate_30']\n",
      "Debug: PriceMomentum_5 calculated and added to df_accel\n",
      "Debug: PriceMomentum_10 calculated and added to df_accel\n",
      "Debug: PriceMomentum_15 calculated and added to df_accel\n",
      "Debug: PriceMomentum_20 calculated and added to df_accel\n",
      "Debug: PriceMomentum_25 calculated and added to df_accel\n",
      "Debug: PriceMomentum_30 calculated and added to df_accel\n",
      "Debug: Columns after PriceMomentum calculation loop: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'symbol', 'volume_lag_0', 'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_4', 'volume_lag_5', 'volume_lag_6', 'volume_lag_7', 'volume_lag_8', 'volume_lag_9', 'volume_lag_10', 'volume_lag_11', 'volume_lag_12', 'volume_lag_13', 'volume_lag_14', 'volume_lag_15', 'volume_lag_16', 'volume_lag_17', 'volume_lag_18', 'volume_lag_19', 'Volume_RollingMean', 'Volume_RollingStdDev', 'VolumeSpike', 'close_lag_0', 'close_lag_1', 'MFI_14', 'MFI_20', 'open_lag_0', 'high_lag_0', 'low_lag_0', 'quote_asset_volume_lag_0', 'number_of_trades_lag_0', 'taker_buy_base_asset_volume_lag_0', 'taker_buy_quote_asset_volume_lag_0', 'open_lag_1', 'high_lag_1', 'low_lag_1', 'quote_asset_volume_lag_1', 'number_of_trades_lag_1', 'taker_buy_base_asset_volume_lag_1', 'taker_buy_quote_asset_volume_lag_1', 'close_lag_2', 'open_lag_2', 'high_lag_2', 'low_lag_2', 'quote_asset_volume_lag_2', 'number_of_trades_lag_2', 'taker_buy_base_asset_volume_lag_2', 'taker_buy_quote_asset_volume_lag_2', 'close_lag_3', 'open_lag_3', 'high_lag_3', 'low_lag_3', 'quote_asset_volume_lag_3', 'number_of_trades_lag_3', 'taker_buy_base_asset_volume_lag_3', 'taker_buy_quote_asset_volume_lag_3', 'close_lag_4', 'open_lag_4', 'high_lag_4', 'low_lag_4', 'quote_asset_volume_lag_4', 'number_of_trades_lag_4', 'taker_buy_base_asset_volume_lag_4', 'taker_buy_quote_asset_volume_lag_4', 'close_lag_5', 'open_lag_5', 'high_lag_5', 'low_lag_5', 'quote_asset_volume_lag_5', 'number_of_trades_lag_5', 'taker_buy_base_asset_volume_lag_5', 'taker_buy_quote_asset_volume_lag_5', 'close_lag_6', 'open_lag_6', 'high_lag_6', 'low_lag_6', 'quote_asset_volume_lag_6', 'number_of_trades_lag_6', 'taker_buy_base_asset_volume_lag_6', 'taker_buy_quote_asset_volume_lag_6', 'close_lag_7', 'open_lag_7', 'high_lag_7', 'low_lag_7', 'quote_asset_volume_lag_7', 'number_of_trades_lag_7', 'taker_buy_base_asset_volume_lag_7', 'taker_buy_quote_asset_volume_lag_7', 'close_lag_8', 'open_lag_8', 'high_lag_8', 'low_lag_8', 'quote_asset_volume_lag_8', 'number_of_trades_lag_8', 'taker_buy_base_asset_volume_lag_8', 'taker_buy_quote_asset_volume_lag_8', 'close_lag_9', 'open_lag_9', 'high_lag_9', 'low_lag_9', 'quote_asset_volume_lag_9', 'number_of_trades_lag_9', 'taker_buy_base_asset_volume_lag_9', 'taker_buy_quote_asset_volume_lag_9', 'close_lag_10', 'open_lag_10', 'high_lag_10', 'low_lag_10', 'quote_asset_volume_lag_10', 'number_of_trades_lag_10', 'taker_buy_base_asset_volume_lag_10', 'taker_buy_quote_asset_volume_lag_10', 'close_lag_11', 'open_lag_11', 'high_lag_11', 'low_lag_11', 'quote_asset_volume_lag_11', 'number_of_trades_lag_11', 'taker_buy_base_asset_volume_lag_11', 'taker_buy_quote_asset_volume_lag_11', 'close_lag_12', 'open_lag_12', 'high_lag_12', 'low_lag_12', 'quote_asset_volume_lag_12', 'number_of_trades_lag_12', 'taker_buy_base_asset_volume_lag_12', 'taker_buy_quote_asset_volume_lag_12', 'close_lag_13', 'open_lag_13', 'high_lag_13', 'low_lag_13', 'quote_asset_volume_lag_13', 'number_of_trades_lag_13', 'taker_buy_base_asset_volume_lag_13', 'taker_buy_quote_asset_volume_lag_13', 'close_lag_14', 'open_lag_14', 'high_lag_14', 'low_lag_14', 'quote_asset_volume_lag_14', 'number_of_trades_lag_14', 'taker_buy_base_asset_volume_lag_14', 'taker_buy_quote_asset_volume_lag_14', 'close_lag_15', 'open_lag_15', 'high_lag_15', 'low_lag_15', 'quote_asset_volume_lag_15', 'number_of_trades_lag_15', 'taker_buy_base_asset_volume_lag_15', 'taker_buy_quote_asset_volume_lag_15', 'close_lag_16', 'open_lag_16', 'high_lag_16', 'low_lag_16', 'quote_asset_volume_lag_16', 'number_of_trades_lag_16', 'taker_buy_base_asset_volume_lag_16', 'taker_buy_quote_asset_volume_lag_16', 'close_lag_17', 'open_lag_17', 'high_lag_17', 'low_lag_17', 'quote_asset_volume_lag_17', 'number_of_trades_lag_17', 'taker_buy_base_asset_volume_lag_17', 'taker_buy_quote_asset_volume_lag_17', 'close_lag_18', 'open_lag_18', 'high_lag_18', 'low_lag_18', 'quote_asset_volume_lag_18', 'number_of_trades_lag_18', 'taker_buy_base_asset_volume_lag_18', 'taker_buy_quote_asset_volume_lag_18', 'close_lag_19', 'open_lag_19', 'high_lag_19', 'low_lag_19', 'quote_asset_volume_lag_19', 'number_of_trades_lag_19', 'taker_buy_base_asset_volume_lag_19', 'taker_buy_quote_asset_volume_lag_19', 'close_lag_20', 'volume_lag_20', 'open_lag_20', 'high_lag_20', 'low_lag_20', 'quote_asset_volume_lag_20', 'number_of_trades_lag_20', 'taker_buy_base_asset_volume_lag_20', 'taker_buy_quote_asset_volume_lag_20', 'close_lag_21', 'volume_lag_21', 'open_lag_21', 'high_lag_21', 'low_lag_21', 'quote_asset_volume_lag_21', 'number_of_trades_lag_21', 'taker_buy_base_asset_volume_lag_21', 'taker_buy_quote_asset_volume_lag_21', 'close_lag_22', 'volume_lag_22', 'open_lag_22', 'high_lag_22', 'low_lag_22', 'quote_asset_volume_lag_22', 'number_of_trades_lag_22', 'taker_buy_base_asset_volume_lag_22', 'taker_buy_quote_asset_volume_lag_22', 'close_lag_23', 'volume_lag_23', 'open_lag_23', 'high_lag_23', 'low_lag_23', 'quote_asset_volume_lag_23', 'number_of_trades_lag_23', 'taker_buy_base_asset_volume_lag_23', 'taker_buy_quote_asset_volume_lag_23', 'close_lag_24', 'volume_lag_24', 'open_lag_24', 'high_lag_24', 'low_lag_24', 'quote_asset_volume_lag_24', 'number_of_trades_lag_24', 'taker_buy_base_asset_volume_lag_24', 'taker_buy_quote_asset_volume_lag_24', 'close_lag_25', 'volume_lag_25', 'open_lag_25', 'high_lag_25', 'low_lag_25', 'quote_asset_volume_lag_25', 'number_of_trades_lag_25', 'taker_buy_base_asset_volume_lag_25', 'taker_buy_quote_asset_volume_lag_25', 'close_lag_26', 'volume_lag_26', 'open_lag_26', 'high_lag_26', 'low_lag_26', 'quote_asset_volume_lag_26', 'number_of_trades_lag_26', 'taker_buy_base_asset_volume_lag_26', 'taker_buy_quote_asset_volume_lag_26', 'close_lag_27', 'volume_lag_27', 'open_lag_27', 'high_lag_27', 'low_lag_27', 'quote_asset_volume_lag_27', 'number_of_trades_lag_27', 'taker_buy_base_asset_volume_lag_27', 'taker_buy_quote_asset_volume_lag_27', 'close_lag_28', 'volume_lag_28', 'open_lag_28', 'high_lag_28', 'low_lag_28', 'quote_asset_volume_lag_28', 'number_of_trades_lag_28', 'taker_buy_base_asset_volume_lag_28', 'taker_buy_quote_asset_volume_lag_28', 'close_lag_29', 'volume_lag_29', 'open_lag_29', 'high_lag_29', 'low_lag_29', 'quote_asset_volume_lag_29', 'number_of_trades_lag_29', 'taker_buy_base_asset_volume_lag_29', 'taker_buy_quote_asset_volume_lag_29', 'close_lag_30', 'volume_lag_30', 'open_lag_30', 'high_lag_30', 'low_lag_30', 'quote_asset_volume_lag_30', 'number_of_trades_lag_30', 'taker_buy_base_asset_volume_lag_30', 'taker_buy_quote_asset_volume_lag_30', 'close_lead_0', 'RollingStdDev_5', 'RollingStdDev_10', 'RollingStdDev_15', 'RollingStdDev_20', 'RollingStdDev_25', 'RollingStdDev_30', 'RollingKurtosis_5', 'RollingKurtosis_10', 'RollingKurtosis_15', 'RollingKurtosis_20', 'RollingKurtosis_25', 'RollingKurtosis_30', 'Plus_DI', 'Minus_DI', 'ADX', 'tr_smoothed_lag_1', 'plus_dm_smoothed_lag_1', 'minus_dm_smoothed_lag_1', 'dx_smoothed_lag_1', 'PriceChangeRate_5', 'PriceChangeRate_10', 'PriceChangeRate_15', 'PriceChangeRate_20', 'PriceChangeRate_25', 'PriceChangeRate_30', 'PriceAccel_ROC_Diff_5_10', 'PriceAccel_ROC_Diff_5_15', 'PriceAccel_ROC_Diff_5_20', 'PriceAccel_ROC_Diff_5_25', 'PriceAccel_ROC_Diff_5_30', 'PriceAccel_ROC_Diff_10_15', 'PriceAccel_ROC_Diff_10_20', 'PriceAccel_ROC_Diff_10_25', 'PriceAccel_ROC_Diff_10_30', 'PriceAccel_ROC_Diff_15_20', 'PriceAccel_ROC_Diff_15_25', 'PriceAccel_ROC_Diff_15_30', 'PriceAccel_ROC_Diff_20_25', 'PriceAccel_ROC_Diff_20_30', 'PriceAccel_ROC_Diff_25_30', 'PriceMomentum_5', 'PriceMomentum_10', 'PriceMomentum_15', 'PriceMomentum_20', 'PriceMomentum_25', 'PriceMomentum_30']\n",
      "Debug: Columns at end of calculate_price_acceleration: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'symbol', 'volume_lag_0', 'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_4', 'volume_lag_5', 'volume_lag_6', 'volume_lag_7', 'volume_lag_8', 'volume_lag_9', 'volume_lag_10', 'volume_lag_11', 'volume_lag_12', 'volume_lag_13', 'volume_lag_14', 'volume_lag_15', 'volume_lag_16', 'volume_lag_17', 'volume_lag_18', 'volume_lag_19', 'Volume_RollingMean', 'Volume_RollingStdDev', 'VolumeSpike', 'close_lag_0', 'close_lag_1', 'MFI_14', 'MFI_20', 'open_lag_0', 'high_lag_0', 'low_lag_0', 'quote_asset_volume_lag_0', 'number_of_trades_lag_0', 'taker_buy_base_asset_volume_lag_0', 'taker_buy_quote_asset_volume_lag_0', 'open_lag_1', 'high_lag_1', 'low_lag_1', 'quote_asset_volume_lag_1', 'number_of_trades_lag_1', 'taker_buy_base_asset_volume_lag_1', 'taker_buy_quote_asset_volume_lag_1', 'close_lag_2', 'open_lag_2', 'high_lag_2', 'low_lag_2', 'quote_asset_volume_lag_2', 'number_of_trades_lag_2', 'taker_buy_base_asset_volume_lag_2', 'taker_buy_quote_asset_volume_lag_2', 'close_lag_3', 'open_lag_3', 'high_lag_3', 'low_lag_3', 'quote_asset_volume_lag_3', 'number_of_trades_lag_3', 'taker_buy_base_asset_volume_lag_3', 'taker_buy_quote_asset_volume_lag_3', 'close_lag_4', 'open_lag_4', 'high_lag_4', 'low_lag_4', 'quote_asset_volume_lag_4', 'number_of_trades_lag_4', 'taker_buy_base_asset_volume_lag_4', 'taker_buy_quote_asset_volume_lag_4', 'close_lag_5', 'open_lag_5', 'high_lag_5', 'low_lag_5', 'quote_asset_volume_lag_5', 'number_of_trades_lag_5', 'taker_buy_base_asset_volume_lag_5', 'taker_buy_quote_asset_volume_lag_5', 'close_lag_6', 'open_lag_6', 'high_lag_6', 'low_lag_6', 'quote_asset_volume_lag_6', 'number_of_trades_lag_6', 'taker_buy_base_asset_volume_lag_6', 'taker_buy_quote_asset_volume_lag_6', 'close_lag_7', 'open_lag_7', 'high_lag_7', 'low_lag_7', 'quote_asset_volume_lag_7', 'number_of_trades_lag_7', 'taker_buy_base_asset_volume_lag_7', 'taker_buy_quote_asset_volume_lag_7', 'close_lag_8', 'open_lag_8', 'high_lag_8', 'low_lag_8', 'quote_asset_volume_lag_8', 'number_of_trades_lag_8', 'taker_buy_base_asset_volume_lag_8', 'taker_buy_quote_asset_volume_lag_8', 'close_lag_9', 'open_lag_9', 'high_lag_9', 'low_lag_9', 'quote_asset_volume_lag_9', 'number_of_trades_lag_9', 'taker_buy_base_asset_volume_lag_9', 'taker_buy_quote_asset_volume_lag_9', 'close_lag_10', 'open_lag_10', 'high_lag_10', 'low_lag_10', 'quote_asset_volume_lag_10', 'number_of_trades_lag_10', 'taker_buy_base_asset_volume_lag_10', 'taker_buy_quote_asset_volume_lag_10', 'close_lag_11', 'open_lag_11', 'high_lag_11', 'low_lag_11', 'quote_asset_volume_lag_11', 'number_of_trades_lag_11', 'taker_buy_base_asset_volume_lag_11', 'taker_buy_quote_asset_volume_lag_11', 'close_lag_12', 'open_lag_12', 'high_lag_12', 'low_lag_12', 'quote_asset_volume_lag_12', 'number_of_trades_lag_12', 'taker_buy_base_asset_volume_lag_12', 'taker_buy_quote_asset_volume_lag_12', 'close_lag_13', 'open_lag_13', 'high_lag_13', 'low_lag_13', 'quote_asset_volume_lag_13', 'number_of_trades_lag_13', 'taker_buy_base_asset_volume_lag_13', 'taker_buy_quote_asset_volume_lag_13', 'close_lag_14', 'open_lag_14', 'high_lag_14', 'low_lag_14', 'quote_asset_volume_lag_14', 'number_of_trades_lag_14', 'taker_buy_base_asset_volume_lag_14', 'taker_buy_quote_asset_volume_lag_14', 'close_lag_15', 'open_lag_15', 'high_lag_15', 'low_lag_15', 'quote_asset_volume_lag_15', 'number_of_trades_lag_15', 'taker_buy_base_asset_volume_lag_15', 'taker_buy_quote_asset_volume_lag_15', 'close_lag_16', 'open_lag_16', 'high_lag_16', 'low_lag_16', 'quote_asset_volume_lag_16', 'number_of_trades_lag_16', 'taker_buy_base_asset_volume_lag_16', 'taker_buy_quote_asset_volume_lag_16', 'close_lag_17', 'open_lag_17', 'high_lag_17', 'low_lag_17', 'quote_asset_volume_lag_17', 'number_of_trades_lag_17', 'taker_buy_base_asset_volume_lag_17', 'taker_buy_quote_asset_volume_lag_17', 'close_lag_18', 'open_lag_18', 'high_lag_18', 'low_lag_18', 'quote_asset_volume_lag_18', 'number_of_trades_lag_18', 'taker_buy_base_asset_volume_lag_18', 'taker_buy_quote_asset_volume_lag_18', 'close_lag_19', 'open_lag_19', 'high_lag_19', 'low_lag_19', 'quote_asset_volume_lag_19', 'number_of_trades_lag_19', 'taker_buy_base_asset_volume_lag_19', 'taker_buy_quote_asset_volume_lag_19', 'close_lag_20', 'volume_lag_20', 'open_lag_20', 'high_lag_20', 'low_lag_20', 'quote_asset_volume_lag_20', 'number_of_trades_lag_20', 'taker_buy_base_asset_volume_lag_20', 'taker_buy_quote_asset_volume_lag_20', 'close_lag_21', 'volume_lag_21', 'open_lag_21', 'high_lag_21', 'low_lag_21', 'quote_asset_volume_lag_21', 'number_of_trades_lag_21', 'taker_buy_base_asset_volume_lag_21', 'taker_buy_quote_asset_volume_lag_21', 'close_lag_22', 'volume_lag_22', 'open_lag_22', 'high_lag_22', 'low_lag_22', 'quote_asset_volume_lag_22', 'number_of_trades_lag_22', 'taker_buy_base_asset_volume_lag_22', 'taker_buy_quote_asset_volume_lag_22', 'close_lag_23', 'volume_lag_23', 'open_lag_23', 'high_lag_23', 'low_lag_23', 'quote_asset_volume_lag_23', 'number_of_trades_lag_23', 'taker_buy_base_asset_volume_lag_23', 'taker_buy_quote_asset_volume_lag_23', 'close_lag_24', 'volume_lag_24', 'open_lag_24', 'high_lag_24', 'low_lag_24', 'quote_asset_volume_lag_24', 'number_of_trades_lag_24', 'taker_buy_base_asset_volume_lag_24', 'taker_buy_quote_asset_volume_lag_24', 'close_lag_25', 'volume_lag_25', 'open_lag_25', 'high_lag_25', 'low_lag_25', 'quote_asset_volume_lag_25', 'number_of_trades_lag_25', 'taker_buy_base_asset_volume_lag_25', 'taker_buy_quote_asset_volume_lag_25', 'close_lag_26', 'volume_lag_26', 'open_lag_26', 'high_lag_26', 'low_lag_26', 'quote_asset_volume_lag_26', 'number_of_trades_lag_26', 'taker_buy_base_asset_volume_lag_26', 'taker_buy_quote_asset_volume_lag_26', 'close_lag_27', 'volume_lag_27', 'open_lag_27', 'high_lag_27', 'low_lag_27', 'quote_asset_volume_lag_27', 'number_of_trades_lag_27', 'taker_buy_base_asset_volume_lag_27', 'taker_buy_quote_asset_volume_lag_27', 'close_lag_28', 'volume_lag_28', 'open_lag_28', 'high_lag_28', 'low_lag_28', 'quote_asset_volume_lag_28', 'number_of_trades_lag_28', 'taker_buy_base_asset_volume_lag_28', 'taker_buy_quote_asset_volume_lag_28', 'close_lag_29', 'volume_lag_29', 'open_lag_29', 'high_lag_29', 'low_lag_29', 'quote_asset_volume_lag_29', 'number_of_trades_lag_29', 'taker_buy_base_asset_volume_lag_29', 'taker_buy_quote_asset_volume_lag_29', 'close_lag_30', 'volume_lag_30', 'open_lag_30', 'high_lag_30', 'low_lag_30', 'quote_asset_volume_lag_30', 'number_of_trades_lag_30', 'taker_buy_base_asset_volume_lag_30', 'taker_buy_quote_asset_volume_lag_30', 'close_lead_0', 'RollingStdDev_5', 'RollingStdDev_10', 'RollingStdDev_15', 'RollingStdDev_20', 'RollingStdDev_25', 'RollingStdDev_30', 'RollingKurtosis_5', 'RollingKurtosis_10', 'RollingKurtosis_15', 'RollingKurtosis_20', 'RollingKurtosis_25', 'RollingKurtosis_30', 'Plus_DI', 'Minus_DI', 'ADX', 'tr_smoothed_lag_1', 'plus_dm_smoothed_lag_1', 'minus_dm_smoothed_lag_1', 'dx_smoothed_lag_1', 'PriceChangeRate_5', 'PriceChangeRate_10', 'PriceChangeRate_15', 'PriceChangeRate_20', 'PriceChangeRate_25', 'PriceChangeRate_30', 'PriceAccel_ROC_Diff_5_10', 'PriceAccel_ROC_Diff_5_15', 'PriceAccel_ROC_Diff_5_20', 'PriceAccel_ROC_Diff_5_25', 'PriceAccel_ROC_Diff_5_30', 'PriceAccel_ROC_Diff_10_15', 'PriceAccel_ROC_Diff_10_20', 'PriceAccel_ROC_Diff_10_25', 'PriceAccel_ROC_Diff_10_30', 'PriceAccel_ROC_Diff_15_20', 'PriceAccel_ROC_Diff_15_25', 'PriceAccel_ROC_Diff_15_30', 'PriceAccel_ROC_Diff_20_25', 'PriceAccel_ROC_Diff_20_30', 'PriceAccel_ROC_Diff_25_30', 'PriceMomentum_5', 'PriceMomentum_10', 'PriceMomentum_15', 'PriceMomentum_20', 'PriceMomentum_25', 'PriceMomentum_30', 'PriceAccel_Momentum_Diff_5_10', 'PriceAccel_Momentum_Diff_5_15', 'PriceAccel_Momentum_Diff_5_20', 'PriceAccel_Momentum_Diff_5_25', 'PriceAccel_Momentum_Diff_5_30', 'PriceAccel_Momentum_Diff_10_15', 'PriceAccel_Momentum_Diff_10_20', 'PriceAccel_Momentum_Diff_10_25', 'PriceAccel_Momentum_Diff_10_30', 'PriceAccel_Momentum_Diff_15_20', 'PriceAccel_Momentum_Diff_15_25', 'PriceAccel_Momentum_Diff_15_30', 'PriceAccel_Momentum_Diff_20_25', 'PriceAccel_Momentum_Diff_20_30', 'PriceAccel_Momentum_Diff_25_30']\n",
      "Debug Wrapper: Columns AFTER calculate_price_acceleration: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'symbol', 'volume_lag_0', 'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_4', 'volume_lag_5', 'volume_lag_6', 'volume_lag_7', 'volume_lag_8', 'volume_lag_9', 'volume_lag_10', 'volume_lag_11', 'volume_lag_12', 'volume_lag_13', 'volume_lag_14', 'volume_lag_15', 'volume_lag_16', 'volume_lag_17', 'volume_lag_18', 'volume_lag_19', 'Volume_RollingMean', 'Volume_RollingStdDev', 'VolumeSpike', 'close_lag_0', 'close_lag_1', 'MFI_14', 'MFI_20', 'open_lag_0', 'high_lag_0', 'low_lag_0', 'quote_asset_volume_lag_0', 'number_of_trades_lag_0', 'taker_buy_base_asset_volume_lag_0', 'taker_buy_quote_asset_volume_lag_0', 'open_lag_1', 'high_lag_1', 'low_lag_1', 'quote_asset_volume_lag_1', 'number_of_trades_lag_1', 'taker_buy_base_asset_volume_lag_1', 'taker_buy_quote_asset_volume_lag_1', 'close_lag_2', 'open_lag_2', 'high_lag_2', 'low_lag_2', 'quote_asset_volume_lag_2', 'number_of_trades_lag_2', 'taker_buy_base_asset_volume_lag_2', 'taker_buy_quote_asset_volume_lag_2', 'close_lag_3', 'open_lag_3', 'high_lag_3', 'low_lag_3', 'quote_asset_volume_lag_3', 'number_of_trades_lag_3', 'taker_buy_base_asset_volume_lag_3', 'taker_buy_quote_asset_volume_lag_3', 'close_lag_4', 'open_lag_4', 'high_lag_4', 'low_lag_4', 'quote_asset_volume_lag_4', 'number_of_trades_lag_4', 'taker_buy_base_asset_volume_lag_4', 'taker_buy_quote_asset_volume_lag_4', 'close_lag_5', 'open_lag_5', 'high_lag_5', 'low_lag_5', 'quote_asset_volume_lag_5', 'number_of_trades_lag_5', 'taker_buy_base_asset_volume_lag_5', 'taker_buy_quote_asset_volume_lag_5', 'close_lag_6', 'open_lag_6', 'high_lag_6', 'low_lag_6', 'quote_asset_volume_lag_6', 'number_of_trades_lag_6', 'taker_buy_base_asset_volume_lag_6', 'taker_buy_quote_asset_volume_lag_6', 'close_lag_7', 'open_lag_7', 'high_lag_7', 'low_lag_7', 'quote_asset_volume_lag_7', 'number_of_trades_lag_7', 'taker_buy_base_asset_volume_lag_7', 'taker_buy_quote_asset_volume_lag_7', 'close_lag_8', 'open_lag_8', 'high_lag_8', 'low_lag_8', 'quote_asset_volume_lag_8', 'number_of_trades_lag_8', 'taker_buy_base_asset_volume_lag_8', 'taker_buy_quote_asset_volume_lag_8', 'close_lag_9', 'open_lag_9', 'high_lag_9', 'low_lag_9', 'quote_asset_volume_lag_9', 'number_of_trades_lag_9', 'taker_buy_base_asset_volume_lag_9', 'taker_buy_quote_asset_volume_lag_9', 'close_lag_10', 'open_lag_10', 'high_lag_10', 'low_lag_10', 'quote_asset_volume_lag_10', 'number_of_trades_lag_10', 'taker_buy_base_asset_volume_lag_10', 'taker_buy_quote_asset_volume_lag_10', 'close_lag_11', 'open_lag_11', 'high_lag_11', 'low_lag_11', 'quote_asset_volume_lag_11', 'number_of_trades_lag_11', 'taker_buy_base_asset_volume_lag_11', 'taker_buy_quote_asset_volume_lag_11', 'close_lag_12', 'open_lag_12', 'high_lag_12', 'low_lag_12', 'quote_asset_volume_lag_12', 'number_of_trades_lag_12', 'taker_buy_base_asset_volume_lag_12', 'taker_buy_quote_asset_volume_lag_12', 'close_lag_13', 'open_lag_13', 'high_lag_13', 'low_lag_13', 'quote_asset_volume_lag_13', 'number_of_trades_lag_13', 'taker_buy_base_asset_volume_lag_13', 'taker_buy_quote_asset_volume_lag_13', 'close_lag_14', 'open_lag_14', 'high_lag_14', 'low_lag_14', 'quote_asset_volume_lag_14', 'number_of_trades_lag_14', 'taker_buy_base_asset_volume_lag_14', 'taker_buy_quote_asset_volume_lag_14', 'close_lag_15', 'open_lag_15', 'high_lag_15', 'low_lag_15', 'quote_asset_volume_lag_15', 'number_of_trades_lag_15', 'taker_buy_base_asset_volume_lag_15', 'taker_buy_quote_asset_volume_lag_15', 'close_lag_16', 'open_lag_16', 'high_lag_16', 'low_lag_16', 'quote_asset_volume_lag_16', 'number_of_trades_lag_16', 'taker_buy_base_asset_volume_lag_16', 'taker_buy_quote_asset_volume_lag_16', 'close_lag_17', 'open_lag_17', 'high_lag_17', 'low_lag_17', 'quote_asset_volume_lag_17', 'number_of_trades_lag_17', 'taker_buy_base_asset_volume_lag_17', 'taker_buy_quote_asset_volume_lag_17', 'close_lag_18', 'open_lag_18', 'high_lag_18', 'low_lag_18', 'quote_asset_volume_lag_18', 'number_of_trades_lag_18', 'taker_buy_base_asset_volume_lag_18', 'taker_buy_quote_asset_volume_lag_18', 'close_lag_19', 'open_lag_19', 'high_lag_19', 'low_lag_19', 'quote_asset_volume_lag_19', 'number_of_trades_lag_19', 'taker_buy_base_asset_volume_lag_19', 'taker_buy_quote_asset_volume_lag_19', 'close_lag_20', 'volume_lag_20', 'open_lag_20', 'high_lag_20', 'low_lag_20', 'quote_asset_volume_lag_20', 'number_of_trades_lag_20', 'taker_buy_base_asset_volume_lag_20', 'taker_buy_quote_asset_volume_lag_20', 'close_lag_21', 'volume_lag_21', 'open_lag_21', 'high_lag_21', 'low_lag_21', 'quote_asset_volume_lag_21', 'number_of_trades_lag_21', 'taker_buy_base_asset_volume_lag_21', 'taker_buy_quote_asset_volume_lag_21', 'close_lag_22', 'volume_lag_22', 'open_lag_22', 'high_lag_22', 'low_lag_22', 'quote_asset_volume_lag_22', 'number_of_trades_lag_22', 'taker_buy_base_asset_volume_lag_22', 'taker_buy_quote_asset_volume_lag_22', 'close_lag_23', 'volume_lag_23', 'open_lag_23', 'high_lag_23', 'low_lag_23', 'quote_asset_volume_lag_23', 'number_of_trades_lag_23', 'taker_buy_base_asset_volume_lag_23', 'taker_buy_quote_asset_volume_lag_23', 'close_lag_24', 'volume_lag_24', 'open_lag_24', 'high_lag_24', 'low_lag_24', 'quote_asset_volume_lag_24', 'number_of_trades_lag_24', 'taker_buy_base_asset_volume_lag_24', 'taker_buy_quote_asset_volume_lag_24', 'close_lag_25', 'volume_lag_25', 'open_lag_25', 'high_lag_25', 'low_lag_25', 'quote_asset_volume_lag_25', 'number_of_trades_lag_25', 'taker_buy_base_asset_volume_lag_25', 'taker_buy_quote_asset_volume_lag_25', 'close_lag_26', 'volume_lag_26', 'open_lag_26', 'high_lag_26', 'low_lag_26', 'quote_asset_volume_lag_26', 'number_of_trades_lag_26', 'taker_buy_base_asset_volume_lag_26', 'taker_buy_quote_asset_volume_lag_26', 'close_lag_27', 'volume_lag_27', 'open_lag_27', 'high_lag_27', 'low_lag_27', 'quote_asset_volume_lag_27', 'number_of_trades_lag_27', 'taker_buy_base_asset_volume_lag_27', 'taker_buy_quote_asset_volume_lag_27', 'close_lag_28', 'volume_lag_28', 'open_lag_28', 'high_lag_28', 'low_lag_28', 'quote_asset_volume_lag_28', 'number_of_trades_lag_28', 'taker_buy_base_asset_volume_lag_28', 'taker_buy_quote_asset_volume_lag_28', 'close_lag_29', 'volume_lag_29', 'open_lag_29', 'high_lag_29', 'low_lag_29', 'quote_asset_volume_lag_29', 'number_of_trades_lag_29', 'taker_buy_base_asset_volume_lag_29', 'taker_buy_quote_asset_volume_lag_29', 'close_lag_30', 'volume_lag_30', 'open_lag_30', 'high_lag_30', 'low_lag_30', 'quote_asset_volume_lag_30', 'number_of_trades_lag_30', 'taker_buy_base_asset_volume_lag_30', 'taker_buy_quote_asset_volume_lag_30', 'close_lead_0', 'RollingStdDev_5', 'RollingStdDev_10', 'RollingStdDev_15', 'RollingStdDev_20', 'RollingStdDev_25', 'RollingStdDev_30', 'RollingKurtosis_5', 'RollingKurtosis_10', 'RollingKurtosis_15', 'RollingKurtosis_20', 'RollingKurtosis_25', 'RollingKurtosis_30', 'Plus_DI', 'Minus_DI', 'ADX', 'tr_smoothed_lag_1', 'plus_dm_smoothed_lag_1', 'minus_dm_smoothed_lag_1', 'dx_smoothed_lag_1', 'PriceChangeRate_5', 'PriceChangeRate_10', 'PriceChangeRate_15', 'PriceChangeRate_20', 'PriceChangeRate_25', 'PriceChangeRate_30', 'PriceAccel_ROC_Diff_5_10', 'PriceAccel_ROC_Diff_5_15', 'PriceAccel_ROC_Diff_5_20', 'PriceAccel_ROC_Diff_5_25', 'PriceAccel_ROC_Diff_5_30', 'PriceAccel_ROC_Diff_10_15', 'PriceAccel_ROC_Diff_10_20', 'PriceAccel_ROC_Diff_10_25', 'PriceAccel_ROC_Diff_10_30', 'PriceAccel_ROC_Diff_15_20', 'PriceAccel_ROC_Diff_15_25', 'PriceAccel_ROC_Diff_15_30', 'PriceAccel_ROC_Diff_20_25', 'PriceAccel_ROC_Diff_20_30', 'PriceAccel_ROC_Diff_25_30', 'PriceMomentum_5', 'PriceMomentum_10', 'PriceMomentum_15', 'PriceMomentum_20', 'PriceMomentum_25', 'PriceMomentum_30', 'PriceAccel_Momentum_Diff_5_10', 'PriceAccel_Momentum_Diff_5_15', 'PriceAccel_Momentum_Diff_5_20', 'PriceAccel_Momentum_Diff_5_25', 'PriceAccel_Momentum_Diff_5_30', 'PriceAccel_Momentum_Diff_10_15', 'PriceAccel_Momentum_Diff_10_20', 'PriceAccel_Momentum_Diff_10_25', 'PriceAccel_Momentum_Diff_10_30', 'PriceAccel_Momentum_Diff_15_20', 'PriceAccel_Momentum_Diff_15_25', 'PriceAccel_Momentum_Diff_15_30', 'PriceAccel_Momentum_Diff_20_25', 'PriceAccel_Momentum_Diff_20_30', 'PriceAccel_Momentum_Diff_25_30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating WMAs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 288.52it/s]\n",
      "Calculating RSI: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 35.72it/s]\n",
      "Calculant Williams %R: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 180.44it/s]\n",
      "Calculant deltes OBV: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Calculando Volume ROC: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 732.71it/s]\n",
      "Calculating VolumeEMA_5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 304.35it/s]\n",
      "Calculating VolumeEMA_10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 499.62it/s]\n",
      "Calculating VolumeEMA_15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 994.62it/s]\n",
      "Calculating VolumeEMA_20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 499.68it/s]\n",
      "Calculating VolumeEMA_25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 420.44it/s]\n",
      "Calculating VolumeEMA_30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 997.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No data to plot for symbol STEEM and plot_type all_day.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "symbol = 'STEEM'\n",
    "timestamp = '2025-02-07 14:11:00'\n",
    "\n",
    "df2 = get_30_min_window(df, symbol, timestamp)\n",
    "\n",
    "df2 = generate_lags_and_leads(df2, 30, 0)\n",
    "\n",
    "indicadors_df2 = True\n",
    "\n",
    "if indicadors_df2:\n",
    "    print(\"Aplicando indicadores a df2\")\n",
    "\n",
    "    df2 = RollingStdDev(df2, indicator_periods, plot=False)\n",
    "    df2 = RollingKurtosis(df2, indicator_periods, plot=False)\n",
    "    df2 = ADX(df2, plot=False)\n",
    "    df2 = VolumeSpike(df2, column='volume_lag_', plot=False)\n",
    "\n",
    "\n",
    "    df2 = PriceChangeRate(df2, indicator_periods, plot=False)\n",
    "    df2 = PriceAcceleration(df2, indicator_periods, plot=False)\n",
    "    df2 = VPT(df2, plot=False)\n",
    "\n",
    "    df2 = SMA(df2, indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = EMA(df2, indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = WMA(df2, indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = RSI(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = StochasticOscillator(df2, periods=indicator_periods, symbol='STEEM',  plot=False)\n",
    "    df2 = calculate_macd(df2, symbol='STEEM', plot=False)\n",
    "    df2 = WilliamsR(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = ATR_row_independent(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = BollingerBands(df2, symbol='STEEM', plot=False)\n",
    "    df2 = OBV(df2, symbol='STEEM', plot=False)\n",
    "    df2 = VolumeROC(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = VolumeEMA(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = Doji(df2, doji_threshold=0.1, symbol='STEEM', plot=False)\n",
    "    df2 = Star(df2, symbol='STEEM', plot=False)\n",
    "    df2 = HammerHangingMan(df2, body_multiplier=2.5, upper_shadow_max=0.2, symbol='STEEM', plot=False)\n",
    "    df2 = Engulfing(df2, symbol='STEEM', plot=False)\n",
    "    df2 = PiercingDarkCloud(df2, penetration_threshold=0.6, symbol='STEEM', plot=False)\n",
    "    df2 = ThreeSoldiersCrows(df2, body_min_size=0.5, symbol='STEEM', plot=False)\n",
    "    df2 = RollingMedian(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = RollingStdDev(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = LiquidityGaps(df2, atr_period=20, volume_ratio_threshold=0.4, atr_threshold_multiplier=2.5, symbol='STEEM', plot=False)\n",
    "    df2 = TakerBuySellRatio(df2, symbol='STEEM', plot=False)\n",
    "    df2 = NumTradesMomentum(df2, periods=indicator_periods, symbol='STEEM', plot=False)\n",
    "    df2 = LaggedMaxDrawdown(df2, symbol='STEEM', plot=False)\n",
    "\n",
    "    df2 = MFI(df2, plot=False)\n",
    "\n",
    "\n",
    "    df2.to_csv(os.path.join(output_dir, 'df2_con_indicadores.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de columnes en df: 38\n",
      "Nombre de columnes en df2: 455\n",
      "√çndex a df: 289543\n",
      "√çndex a df2: 0\n",
      "Nombre de columnes comunes a comparar: 38\n",
      "S'han trobat 21 columnes que no coincideixen:\n",
      "  - volume_lag_1: df=1773.815, df2=5166.0, difer√®ncia=3392.185\n",
      "  - volume_lag_2: df=16700552.0, df2=3176.7, difer√®ncia=16697375.3\n",
      "  - volume_lag_3: df=10228.543, df2=5872.8, difer√®ncia=4355.7429999999995\n",
      "  - volume_lag_4: df=78255.9, df2=50572.8, difer√®ncia=27683.09999999999\n",
      "  - volume_lag_5: df=36322.0, df2=3014.1, difer√®ncia=33307.9\n",
      "  - volume_lag_6: df=3207825.0, df2=4685.1, difer√®ncia=3203139.9\n",
      "  - volume_lag_7: df=3903.5, df2=639.5, difer√®ncia=3264.0\n",
      "  - volume_lag_8: df=108864.0, df2=428.0, difer√®ncia=108436.0\n",
      "  - volume_lag_9: df=8219814041.0, df2=13152.6, difer√®ncia=8219800888.4\n",
      "  - volume_lag_10: df=662.0, df2=388.6, difer√®ncia=273.4\n",
      "  - volume_lag_11: df=108992.7, df2=9685.3, difer√®ncia=99307.4\n",
      "  - volume_lag_12: df=992036.0, df2=0.0, difer√®ncia=992036.0\n",
      "  - volume_lag_13: df=3713.11, df2=298.4, difer√®ncia=3414.71\n",
      "  - volume_lag_14: df=91841.0, df2=4244.2, difer√®ncia=87596.8\n",
      "  - volume_lag_15: df=17034.4, df2=2116.8, difer√®ncia=14917.600000000002\n",
      "  - volume_lag_16: df=71800.2, df2=1214.4, difer√®ncia=70585.8\n",
      "  - volume_lag_17: df=76166.2, df2=14123.9, difer√®ncia=62042.299999999996\n",
      "  - volume_lag_18: df=1271568.0, df2=12203.7, difer√®ncia=1259364.3\n",
      "  - volume_lag_19: df=26.81, df2=14541.8, difer√®ncia=14514.99\n",
      "  - close_lag_1: df=12.03, df2=0.1734, difer√®ncia=11.8566\n",
      "  - MFI_14: df=0.0, df2=51.206814403269476, difer√®ncia=51.206814403269476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mismatched_columns = compare_dataframes_row(df, df2, symbol_col=symbol_col, timestamp_col=timestamp_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns NOT in both df and df2 (symmetric difference):\n",
      "{'taker_buy_quote_asset_volume_lag_6', 'quote_asset_volume_lag_28', 'close_lead_0', 'quote_asset_volume_lag_19', 'taker_buy_base_asset_volume_lag_21', 'number_of_trades_lag_10', 'LaggedMaxDrawdown', 'close_lag_13', 'RollingMedian_20', 'number_of_trades_lag_3', 'RollingStdDev_25', 'PriceAccel_ROC_Diff_20_30', 'low_lag_12', 'PriceChangeRate_20', 'PriceAccel_ROC_Diff_5_20', 'taker_buy_quote_asset_volume_lag_4', 'open_lag_20', 'quote_asset_volume_lag_20', 'PriceAccel_ROC_Diff_10_20', 'high_lag_16', 'plus_dm_smoothed_lag_1', 'taker_buy_base_asset_volume_lag_1', 'VolumeEMA_20', 'VPT', 'open_lag_12', 'number_of_trades_lag_27', 'open_lag_22', 'PriceAccel_Momentum_Diff_15_25', 'PriceAccel_ROC_Diff_5_10', 'low_lag_19', 'low_lag_6', 'taker_buy_base_asset_volume_lag_25', 'volume_lag_25', 'PriceAccel_Momentum_Diff_5_30', 'PriceChangeRate_15', 'number_of_trades_lag_18', 'open_lag_29', 'WMA_30', 'low_lag_25', 'high_lag_25', 'taker_buy_quote_asset_volume_lag_21', 'EMA_15', 'Stochastic_D_30', 'high_lag_19', 'PriceAccel_Momentum_Diff_5_10', 'taker_buy_base_asset_volume_lag_4', 'PotentialLiquidityGap', 'VolumeROC_15', 'quote_asset_volume_lag_5', 'taker_buy_quote_asset_volume_lag_30', 'taker_buy_base_asset_volume_lag_27', 'number_of_trades_lag_12', 'open_lag_28', 'taker_buy_base_asset_volume_lag_18', 'low_lag_15', 'close_lag_3', 'low_lag_10', 'PriceAccel_Momentum_Diff_15_30', 'quote_asset_volume_lag_26', 'SMA_25', 'high_lag_20', 'EMA_5', 'quote_asset_volume_lag_16', 'TakerBuySellRatio', 'Stochastic_K_25', 'close_lag_2', 'open_lag_7', 'low_lag_18', 'PriceAccel_Momentum_Diff_5_15', 'WilliamsR_20', 'taker_buy_quote_asset_volume_lag_15', 'taker_buy_quote_asset_volume_lag_29', 'low_lag_16', 'VolumeROC_20', 'taker_buy_quote_asset_volume_lag_1', 'high_lag_6', 'NumTradesMomentum_5', 'volume_lag_26', 'RollingStdDev_10', 'PriceMomentum_5', 'quote_asset_volume_lag_14', 'quote_asset_volume_lag_23', 'PriceAccel_Momentum_Diff_10_20', 'taker_buy_quote_asset_volume_lag_2', 'high_lag_27', 'RollingStdDev_15', 'PriceMomentum_15', 'WMA_10', 'close_lag_18', 'OBV', 'high_lag_21', 'quote_asset_volume_lag_11', 'Minus_DI', 'close_lag_29', 'taker_buy_base_asset_volume_lag_30', 'PriceAccel_ROC_Diff_5_25', 'taker_buy_base_asset_volume_lag_12', 'taker_buy_quote_asset_volume_lag_10', 'RollingStdDev_30', 'low_lag_8', 'WMA_5', 'quote_asset_volume_lag_6', 'low_lag_26', 'close_lag_14', 'volume_lag_29', 'HangingMan', 'taker_buy_base_asset_volume_lag_8', 'EMA_20', 'SMA_5', 'number_of_trades_lag_22', 'RSI_20', 'Histogram_15_25_10', 'Stochastic_K_15', 'RollingMedian_30', 'PriceAccel_ROC_Diff_15_25', 'VolumeROC_25', 'number_of_trades_lag_4', 'close_lag_19', 'number_of_trades_lag_25', 'quote_asset_volume_lag_13', 'number_of_trades_lag_24', 'low_lag_13', 'open_lag_30', 'number_of_trades_lag_11', 'open_lag_16', 'quote_asset_volume_lag_27', 'ATR_30', 'close_lag_9', 'high_lag_24', 'quote_asset_volume_lag_18', 'number_of_trades_lag_23', 'high_lag_29', 'high_lag_11', 'PriceAccel_ROC_Diff_15_30', 'high_lag_17', 'taker_buy_base_asset_volume_lag_7', 'taker_buy_quote_asset_volume_lag_8', 'low_lag_9', 'VolumeRatio', 'RollingKurtosis_10', 'open_lag_26', 'close_lag_21', 'TakerBuyQuoteVolume', 'ATR_25', 'taker_buy_base_asset_volume_lag_5', 'BearishEngulfing', 'number_of_trades_lag_26', 'low_lag_1', 'volume_lag_22', 'close_lag_23', 'low_lag_22', 'NumTradesMomentum_20', 'Stochastic_D_15', 'low_lag_30', 'WilliamsR_15', 'WMA_25', 'PriceAccel_Momentum_Diff_25_30', 'MorningStar', 'close_lag_17', 'high_lag_1', 'low_lag_29', 'taker_buy_base_asset_volume_lag_11', 'SMA_20', 'high_lag_0', 'low_lag_27', 'PriceMomentum_10', 'high_lag_4', 'RollingKurtosis_20', 'quote_asset_volume_lag_21', 'RSI_30', 'WMA_15', 'NumTradesMomentum_30', 'WMA_20', 'Stochastic_D_5', 'taker_buy_base_asset_volume_lag_2', 'taker_buy_base_asset_volume_lag_24', 'PriceAccel_ROC_Diff_15_20', 'high_lag_2', 'PriceChangeRate_25', 'taker_buy_quote_asset_volume_lag_12', 'low_lag_21', 'close_lag_10', 'close_lag_12', 'open_lag_0', 'high_lag_9', 'DarkCloudCover', 'close_lag_25', 'PriceAccel_ROC_Diff_5_30', 'quote_asset_volume_lag_9', 'taker_buy_quote_asset_volume_lag_17', 'high_lag_22', 'quote_asset_volume_lag_30', 'low_lag_7', 'number_of_trades_lag_9', 'RollingMedian_15', 'number_of_trades_lag_29', 'MACD_15_25', 'open_lag_15', 'quote_asset_volume_lag_17', 'quote_asset_volume_lag_1', 'PriceMomentum_20', 'number_of_trades_lag_2', 'PiercingLine', 'taker_buy_base_asset_volume_lag_9', 'low_lag_11', 'EMA_30', 'VolumeEMA_5', 'high_lag_13', 'open_lag_18', 'PriceAccel_Momentum_Diff_20_25', 'PriceAccel_ROC_Diff_5_15', 'WilliamsR_30', 'number_of_trades_lag_19', 'open_lag_27', 'open_lag_5', 'open_lag_24', 'ATR_15', 'PriceChangeRate_5', 'low_lag_14', 'taker_buy_base_asset_volume_lag_29', 'quote_asset_volume_lag_29', 'close_lag_8', 'open_lag_17', 'close_lag_28', 'taker_buy_quote_asset_volume_lag_24', 'high_lag_30', 'close_lag_4', 'taker_buy_base_asset_volume_lag_13', 'high_lag_5', 'low_lag_0', 'ThreeBlackCrows', 'ATR_20_RollingMean', 'WilliamsR_25', 'number_of_trades_lag_14', 'taker_buy_quote_asset_volume_lag_20', 'VolumeROC_30', 'high_lag_23', 'Stochastic_D_10', 'PriceMomentum_25', 'number_of_trades_lag_28', 'Stochastic_D_20', 'number_of_trades_lag_15', 'number_of_trades_lag_6', 'PriceAccel_ROC_Diff_10_15', 'low_lag_4', 'taker_buy_quote_asset_volume_lag_22', 'PriceChangeRate_10', 'high_lag_26', 'close_lag_15', 'open_lag_14', 'BB_Lower_20', 'VolumeEMA_10', 'quote_asset_volume_lag_2', 'taker_buy_base_asset_volume_lag_6', 'WilliamsR_5', 'taker_buy_quote_asset_volume_lag_14', 'open_lag_1', 'SMA_30', 'close_lag_5', 'PriceAccel_Momentum_Diff_10_30', 'close_lag_20', 'high_lag_28', 'PriceAccel_ROC_Diff_10_25', 'number_of_trades_lag_7', 'open_lag_9', 'VolumeROC_5', 'high_lag_18', 'open_lag_25', 'Hammer', 'BullishEngulfing', 'volume_lag_28', 'low_lag_2', 'Stochastic_K_20', 'quote_asset_volume_lag_0', 'Stochastic_K_5', 'high_lag_14', 'PriceAccel_ROC_Diff_20_25', 'taker_buy_base_asset_volume_lag_16', 'taker_buy_base_asset_volume_lag_10', 'volume_lag_20', 'taker_buy_quote_asset_volume_lag_28', 'taker_buy_quote_asset_volume_lag_13', 'open_lag_21', 'low_lag_5', 'PriceAccel_Momentum_Diff_15_20', 'RollingKurtosis_15', 'taker_buy_quote_asset_volume_lag_19', 'taker_buy_quote_asset_volume_lag_0', 'close_lag_7', 'ATR_10', 'RollingKurtosis_5', 'BB_Upper_20', 'open_lag_4', 'number_of_trades_lag_0', 'ADX', 'close_lag_26', 'PriceAccel_Momentum_Diff_5_20', 'VolumeEMA_30', 'RSI_25', 'number_of_trades_lag_16', 'quote_asset_volume_lag_15', 'Stochastic_K_30', 'taker_buy_quote_asset_volume_lag_18', 'Signal_10', 'taker_buy_base_asset_volume_lag_23', 'NumTradesMomentum_25', 'Plus_DI', 'number_of_trades_lag_21', 'taker_buy_quote_asset_volume_lag_3', 'minus_dm_smoothed_lag_1', 'number_of_trades_lag_5', 'high_lag_3', 'quote_asset_volume_lag_12', 'Stochastic_K_10', 'taker_buy_quote_asset_volume_lag_25', 'PriceAccel_Momentum_Diff_10_25', 'quote_asset_volume_lag_7', 'taker_buy_quote_asset_volume_lag_27', 'PriceAccel_Momentum_Diff_5_25', 'taker_buy_base_asset_volume_lag_28', 'dx_smoothed_lag_1', 'open_lag_19', 'taker_buy_quote_asset_volume_lag_26', 'high_lag_8', 'number_of_trades_lag_30', 'low_lag_20', 'RollingStdDev_20', 'ATR_20', 'number_of_trades_lag_13', 'taker_buy_base_asset_volume_lag_15', 'close_lag_6', 'taker_buy_base_asset_volume_lag_20', 'close_lag_11', 'taker_buy_base_asset_volume_lag_26', 'open_lag_2', 'taker_buy_base_asset_volume_lag_19', 'volume_lag_21', 'PriceAccel_Momentum_Diff_20_30', 'TakerSellQuoteVolume', 'RollingKurtosis_25', 'low_lag_3', 'NumTradesMomentum_10', 'taker_buy_quote_asset_volume_lag_16', 'taker_buy_base_asset_volume_lag_22', 'SMA_15', 'VolumeEMA_15', 'high_lag_12', 'number_of_trades_lag_20', 'RSI_15', 'volume_lag_23', 'open_lag_23', 'open_lag_8', 'quote_asset_volume_lag_24', 'quote_asset_volume_lag_22', 'RollingMedian_25', 'volume_lag_24', 'high_lag_7', 'Doji', 'PriceAccel_ROC_Diff_25_30', 'PriceAccel_ROC_Diff_10_30', 'low_lag_24', 'SMA_10', 'BB_Middle_20', 'quote_asset_volume_lag_10', 'quote_asset_volume_lag_4', 'EMA_10', 'open_lag_6', 'volume_lag_30', 'quote_asset_volume_lag_3', 'EveningStar', 'close_lag_27', 'NumTradesMomentum_15', 'RollingMedian_10', 'taker_buy_base_asset_volume_lag_3', 'VolumeEMA_25', 'ThreeWhiteSoldiers', 'number_of_trades_lag_1', 'open_lag_10', 'open_lag_13', 'VolumeROC_10', 'taker_buy_base_asset_volume_lag_14', 'ATR_5', 'number_of_trades_lag_8', 'RSI_10', 'taker_buy_quote_asset_volume_lag_7', 'close_lag_16', 'taker_buy_quote_asset_volume_lag_23', 'quote_asset_volume_lag_8', 'tr_smoothed_lag_1', 'open_lag_11', 'open_lag_3', 'high_lag_15', 'taker_buy_base_asset_volume_lag_17', 'RollingStdDev_5', 'WilliamsR_10', 'taker_buy_quote_asset_volume_lag_5', 'PriceAccel_Momentum_Diff_10_15', 'RSI_5', 'PriceMomentum_30', 'low_lag_17', 'low_lag_28', 'low_lag_23', 'EMA_25', 'Stochastic_D_25', 'taker_buy_quote_asset_volume_lag_11', 'taker_buy_base_asset_volume_lag_0', 'volume_lag_27', 'taker_buy_quote_asset_volume_lag_9', 'RollingKurtosis_30', 'close_lag_22', 'quote_asset_volume_lag_25', 'number_of_trades_lag_17', 'RollingMedian_5', 'high_lag_10', 'close_lag_24', 'close_lag_30', 'PriceChangeRate_30'}\n"
     ]
    }
   ],
   "source": [
    "def get_columns_not_in_both(df1, df2):\n",
    " \n",
    "    cols_df1 = set(df1.columns)\n",
    "    cols_df2 = set(df2.columns)\n",
    "\n",
    "    not_in_both = cols_df1.symmetric_difference(cols_df2) # Symmetric difference\n",
    "\n",
    "    return not_in_both\n",
    "\n",
    "# Example Usage (assuming you have DataFrames named df and df2)\n",
    "columns_not_in_both = get_columns_not_in_both(df, df2)\n",
    "\n",
    "print(\"Columns NOT in both df and df2 (symmetric difference):\")\n",
    "\n",
    "print(columns_not_in_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df = pd.read_csv('df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de columnes en df: 469\n",
      "Nombre de columnes en df2: 455\n"
     ]
    }
   ],
   "source": [
    "mismatched_columns = compare_dataframes_row(df3, df2, symbol_col=symbol_col, timestamp_col=timestamp_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ATR_14_lag_10', 'ATR_14_lag_0', 'ATR_14_lag_4', 'ATR_14_RollingMean', 'ATR_14_lag_1', 'ATR_14_lag_8', 'ATR_14_lag_12', 'ATR_14_lag_6', 'ATR_14_lag_5', 'ATR_14_lag_9', 'ATR_14', 'ATR_14_lag_3', 'ATR_20_RollingMean', 'ATR_14_lag_2', 'MFI_20', 'ATR_14_lag_11', 'ATR_14_lag_13', 'ATR_14_lag_7'}\n"
     ]
    }
   ],
   "source": [
    "columns_not_in_both = get_columns_not_in_both(df2, df3)\n",
    "\n",
    "print(columns_not_in_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
