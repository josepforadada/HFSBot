{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFM_06_modeling_anomalies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, recall_score, precision_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import json\n",
    "import math\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "N_SPLITS = 5\n",
    "MODEL_DIR = './modelos_anomalias_clf'\n",
    "RESULTS_DIR = './resultados_anomalias_clf'\n",
    "RESULTS_FILE = os.path.join(RESULTS_DIR, 'resultados_cv_anomalias.json')\n",
    "BEST_MODEL_FILE = os.path.join(MODEL_DIR, 'mejor_modelo_anomalias.joblib')\n",
    "FEATURES_FILE = os.path.join(MODEL_DIR, 'columnas_relevantes_anomalias.json')\n",
    "SCALER_FILE = 'old_logs_jsons_and_joblib/escaladores.joblib'\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def mapear_prediccion_anomalia(pred):\n",
    "    return np.where(pred == -1, 1, 0)\n",
    "\n",
    "recall_sc = make_scorer(lambda yt, yp: recall_score(yt, mapear_prediccion_anomalia(yp), pos_label=1, zero_division=0))\n",
    "prec_sc = make_scorer(lambda yt, yp: precision_score(yt, mapear_prediccion_anomalia(yp), pos_label=1, zero_division=0))\n",
    "f1_sc = make_scorer(lambda yt, yp: f1_score(yt, mapear_prediccion_anomalia(yp), pos_label=1, zero_division=0))\n",
    "roc_auc_sc = make_scorer(lambda yt, ys: roc_auc_score(yt, -ys), needs_threshold=True, greater_is_better=True)\n",
    "pr_auc_sc = make_scorer(lambda yt, ys: average_precision_score(yt, -ys, pos_label=1), needs_threshold=True, greater_is_better=True)\n",
    "\n",
    "SCORERS = {'recall': recall_sc, 'precision': prec_sc, 'f1': f1_sc, 'roc_auc': roc_auc_sc, 'pr_auc': pr_auc_sc}\n",
    "PRIMARY_METRIC = 'pr_auc'\n",
    "HIGHER_BETTER = True\n",
    "\n",
    "logging.info(\"Configuración inicial lista.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Cargando datos y escaladores...\")\n",
    "datos_cargados = False\n",
    "escaladores = None\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "nombres_caracteristicas = []\n",
    "\n",
    "try:\n",
    "    df_train_orig = pd.read_csv(\"datos_divididos/datos_financieros_train_completo.csv\")\n",
    "    df_test_orig = pd.read_csv(\"datos_divididos/datos_financieros_test_completo.csv\")\n",
    "\n",
    "    y_train = df_train_orig['target'].copy()\n",
    "    y_test = df_test_orig['target'].copy()\n",
    "\n",
    "    cols_a_eliminar = ['timestamp', 'Unnamed: 0', 'symbol', 'target', 'future_max_increase_capped']\n",
    "    X_train = df_train_orig.drop(columns=[col for col in cols_a_eliminar if col in df_train_orig.columns], errors='ignore')\n",
    "    X_test = df_test_orig.drop(columns=[col for col in cols_a_eliminar if col in df_test_orig.columns], errors='ignore')\n",
    "    nombres_caracteristicas = X_train.columns.tolist()\n",
    "\n",
    "    logging.info(f\"Características iniciales: {len(nombres_caracteristicas)}\")\n",
    "\n",
    "    if not os.path.exists(SCALER_FILE):\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {SCALER_FILE}\")\n",
    "    escaladores = joblib.load(SCALER_FILE)\n",
    "    logging.info(f\"Escaladores cargados: {SCALER_FILE}\")\n",
    "\n",
    "    logging.info(f\"Shapes: X_train={X_train.shape}, y_train={y_train.shape}, X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "    logging.info(f\"Clases Train: {Counter(y_train)}\")\n",
    "    datos_cargados = True\n",
    "\n",
    "except FileNotFoundError as e_fnf:\n",
    "     logging.error(f\"{e_fnf}\")\n",
    "     datos_cargados = False\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error carga: {e}\", exc_info=False) # exc_info=False para simplificar\n",
    "    datos_cargados = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_escaladores_por_grupo(df_input, escaladores_por_simbolo, columna_grupo, nombres_cols_features):\n",
    "    \"\"\"\n",
    "    Aplica escaladores a un DataFrame por grupo.\n",
    "\n",
    "    Args:\n",
    "        df_input (pd.DataFrame): DataFrame de entrada.\n",
    "        escaladores_por_simbolo (dict): Diccionario con escaladores por grupo.\n",
    "        columna_grupo (str): Nombre de la columna para agrupar.\n",
    "        nombres_cols_features (list): Lista de nombres de columnas a escalar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con columnas escaladas.\n",
    "    \"\"\"\n",
    "    if columna_grupo not in df_input.columns:\n",
    "        raise ValueError(f\"Columna grupo '{columna_grupo}' no encontrada.\")\n",
    "\n",
    "    df_output = df_input.copy()\n",
    "    cols_a_escalar = [col for col in nombres_cols_features if col in df_output.columns]\n",
    "\n",
    "    grouped_df = df_output.groupby(columna_grupo)\n",
    "\n",
    "    for simbolo, grupo_df in tqdm(grouped_df, total=len(grouped_df.groups), desc=f\"Escalando '{columna_grupo}'\", leave=False):\n",
    "        if simbolo not in escaladores_por_simbolo:\n",
    "            logging.warning(f\"Escaladores no encontrados para '{simbolo}'. Se asigna NaN.\")\n",
    "            df_output.loc[grupo_df.index, cols_a_escalar] = np.nan\n",
    "            continue\n",
    "\n",
    "        scalers_simbolo_actual = escaladores_por_simbolo[simbolo]\n",
    "\n",
    "        for col_name in cols_a_escalar:\n",
    "            if col_name not in scalers_simbolo_actual:\n",
    "                logging.warning(f\"Escalador no encontrado para '{simbolo}' / '{col_name}'. Se asigna NaN.\")\n",
    "                df_output.loc[grupo_df.index, col_name] = np.nan\n",
    "                continue\n",
    "\n",
    "            min_val, max_val = scalers_simbolo_actual[col_name]\n",
    "            col_original_grupo = grupo_df[col_name]\n",
    "\n",
    "            if pd.isna(min_val) or pd.isna(max_val):\n",
    "                scaled_col = pd.Series(np.nan, index=col_original_grupo.index, name=col_name)\n",
    "            else:\n",
    "                denom = max_val - min_val\n",
    "                if abs(denom) < 1e-9:\n",
    "                    scaled_col = pd.Series(0.5, index=col_original_grupo.index, name=col_name)\n",
    "                else:\n",
    "                    scaled_col = (col_original_grupo - min_val) / denom\n",
    "                scaled_col = scaled_col.where(col_original_grupo.notna(), np.nan)\n",
    "\n",
    "            df_output.loc[grupo_df.index, col_name] = scaled_col\n",
    "\n",
    "    nans_restantes = df_output[cols_a_escalar].isna().sum().sum()\n",
    "    if nans_restantes > 0:\n",
    "        logging.warning(f\"{nans_restantes} NaNs restantes tras escalar. Imputando con 0.\")\n",
    "        df_output[cols_a_escalar] = df_output[cols_a_escalar].fillna(0)\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "scaling_ok = False\n",
    "X_train_norm = None\n",
    "X_test_norm = None\n",
    "X_train_normal_norm = None\n",
    "\n",
    "if datos_cargados and escaladores:\n",
    "    logging.info(\"Aplicando escaladores por grupo...\")\n",
    "    try:\n",
    "        columna_grupo = 'symbol'\n",
    "\n",
    "        if columna_grupo not in X_train.columns and columna_grupo in df_train_orig.columns:\n",
    "            X_train_temp = pd.concat([X_train, df_train_orig[[columna_grupo]]], axis=1)\n",
    "        else:\n",
    "            X_train_temp = X_train.copy()\n",
    "\n",
    "        if columna_grupo not in X_test.columns and columna_grupo in df_test_orig.columns:\n",
    "             X_test_temp = pd.concat([X_test, df_test_orig[[columna_grupo]]], axis=1)\n",
    "        else:\n",
    "             X_test_temp = X_test.copy()\n",
    "\n",
    "        X_train_norm_con_grupo = aplicar_escaladores_por_grupo(X_train_temp, escaladores, columna_grupo, nombres_caracteristicas)\n",
    "        X_test_norm_con_grupo = aplicar_escaladores_por_grupo(X_test_temp, escaladores, columna_grupo, nombres_caracteristicas)\n",
    "\n",
    "        X_train_norm = X_train_norm_con_grupo.drop(columns=[columna_grupo], errors='ignore')\n",
    "        X_test_norm = X_test_norm_con_grupo.drop(columns=[columna_grupo], errors='ignore')\n",
    "\n",
    "        X_train_normal_norm = X_train_norm[y_train == 0]\n",
    "        if X_train_normal_norm.empty:\n",
    "            raise ValueError(\"No hay datos normales (clase 0) después de escalar.\")\n",
    "\n",
    "        logging.info(\"Escalado por grupo finalizado.\")\n",
    "        logging.info(f\"Shapes escalados: X_train_norm={X_train_norm.shape}, X_test_norm={X_test_norm.shape}\")\n",
    "        logging.info(f\"Shape entreno anomalías: {X_train_normal_norm.shape}\")\n",
    "        scaling_ok = True\n",
    "\n",
    "    except ValueError as e_val:\n",
    "         logging.error(f\"Error valor: {e_val}\")\n",
    "         scaling_ok = False\n",
    "    except Exception as e_scale:\n",
    "        logging.error(f\"Error escalado: {e_scale}\", exc_info=False)\n",
    "        scaling_ok = False\n",
    "else:\n",
    "     logging.info(\"Escalado omitido por errores previos en carga.\")\n",
    "     scaling_ok = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos_anomalia = {}\n",
    "ALLOW_MULTIPROCESSING = True\n",
    "\n",
    "if scaling_ok:\n",
    "    modelos_anomalia = {\n",
    "        'IF': {\n",
    "            'model': IsolationForest,\n",
    "            'grid': [\n",
    "                {'n_estimators': 100, 'contamination': 'auto', 'max_features': 1.0},\n",
    "                {'n_estimators': 300, 'contamination': 0.03, 'max_features': 0.7},\n",
    "            ],\n",
    "            'fixed': {'random_state': 42, 'warm_start': False, 'n_jobs': -1 if ALLOW_MULTIPROCESSING else 1},\n",
    "        },\n",
    "        'OCSVM': {\n",
    "            'model': OneClassSVM,\n",
    "            'grid': [\n",
    "                {'kernel': 'rbf', 'gamma': 'scale', 'nu': 0.03},\n",
    "                {'kernel': 'rbf', 'gamma': 0.01, 'nu': 0.05},\n",
    "            ],\n",
    "            'fixed': {'max_iter': 3000, 'cache_size': 500},\n",
    "        },\n",
    "    }\n",
    "    logging.info(f\"Modelos definidos: {list(modelos_anomalia.keys())}\")\n",
    "else:\n",
    "     logging.error(\"Modelos no definidos por errores previos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_cv_anom = {}\n",
    "\n",
    "if scaling_ok and modelos_anomalia:\n",
    "    logging.info(\"Iniciando Cross-Validation...\")\n",
    "    model_bar = tqdm(modelos_anomalia.items(), desc=\"Modelos\")\n",
    "\n",
    "    for nombre_mod, config_mod in model_bar:\n",
    "        model_bar.set_description(f\"Modelo: {nombre_mod}\")\n",
    "        resultados_cv_anom[nombre_mod] = {}\n",
    "        param_bar = tqdm(config_mod['grid'], desc=f\"Params ({nombre_mod})\", leave=False)\n",
    "\n",
    "        for i, params in enumerate(param_bar):\n",
    "            param_key = f\"p_{i}\"\n",
    "            param_bar.set_postfix_str(str(params), refresh=True)\n",
    "            resultados_cv_anom[nombre_mod][param_key] = {'params': params, 'cv_scores': {}, 'status': 'Pendiente'}\n",
    "            scores_fold = {m: [] for m in SCORERS}\n",
    "            fits_t, preds_t = [], []\n",
    "            cv_error = False\n",
    "\n",
    "            cv_split = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "            fold_bar = tqdm(enumerate(cv_split.split(X_train_norm)), total=N_SPLITS, desc=\"CV Folds\", leave=False)\n",
    "\n",
    "            for fold, (idx_train, idx_val) in fold_bar:\n",
    "                fold_bar.set_description(f\"Fold {fold+1}/{N_SPLITS}\")\n",
    "                try:\n",
    "                    xt_fold = X_train_norm.iloc[idx_train]\n",
    "                    yt_fold = y_train.iloc[idx_train]\n",
    "                    xv_fold = X_train_norm.iloc[idx_val]\n",
    "                    yv_fold = y_train.iloc[idx_val]\n",
    "\n",
    "                    xt_fold_norm = xt_fold[yt_fold == 0]\n",
    "\n",
    "                    if len(xt_fold_norm) < 5:\n",
    "                        logging.warning(f\"Fold {fold+1}: Pocos datos normales ({len(xt_fold_norm)}). Omitiendo.\")\n",
    "                        for m in scores_fold: scores_fold[m].append(np.nan)\n",
    "                        continue\n",
    "\n",
    "                    m_fold = config_mod['model'](**config_mod.get('fixed', {}), **params)\n",
    "                    t0 = time.time(); m_fold.fit(xt_fold_norm); fits_t.append(time.time() - t0)\n",
    "\n",
    "                    scores_v, preds_v_raw = None, None\n",
    "                    t0 = time.time()\n",
    "                    if hasattr(m_fold, 'decision_function'):\n",
    "                        scores_v = m_fold.decision_function(xv_fold)\n",
    "                    elif hasattr(m_fold, 'score_samples'):\n",
    "                        scores_v = -m_fold.score_samples(xv_fold)\n",
    "\n",
    "                    if hasattr(m_fold, 'predict'):\n",
    "                        preds_v_raw = m_fold.predict(xv_fold)\n",
    "                    elif scores_v is not None:\n",
    "                        cont = params.get('contamination','auto'); nu=params.get('nu')\n",
    "                        pct = (nu*100) if nu else (2 if cont=='auto' else cont*100) # Simple logic for threshold\n",
    "                        try:\n",
    "                            thr = np.percentile(scores_v[~np.isnan(scores_v)], pct) # Handle potential NaNs in scores_v\n",
    "                            preds_v_raw = np.where(scores_v < thr, -1, 1)\n",
    "                        except IndexError: \n",
    "                            preds_v_raw = np.ones_like(scores_v) \n",
    "                            \n",
    "                    preds_t.append(time.time() - t0)\n",
    "\n",
    "                    for m_name, scorer in SCORERS.items():\n",
    "                        try:\n",
    "                            if getattr(scorer, '_needs_threshold', False):\n",
    "                                score = scorer._score_func(yv_fold, scores_v) if scores_v is not None else np.nan\n",
    "                            else:\n",
    "                                score = scorer._score_func(yv_fold, preds_v_raw) if preds_v_raw is not None else np.nan\n",
    "                            scores_fold[m_name].append(score)\n",
    "                        except Exception:\n",
    "                            scores_fold[m_name].append(np.nan)\n",
    "\n",
    "                except Exception as e_f:\n",
    "                    logging.error(f\"Fold {fold+1} Error: {e_f}\", exc_info=False)\n",
    "                    cv_error = True\n",
    "                    for m in scores_fold:\n",
    "                         if len(scores_fold[m]) == fold: scores_fold[m].append(np.nan)\n",
    "\n",
    "            if not cv_error:\n",
    "                summary = {}\n",
    "                for m, s_list in scores_fold.items():\n",
    "                    valid = [s for s in s_list if not pd.isna(s)]\n",
    "                    summary[f'{m}_mean'] = np.mean(valid) if valid else np.nan\n",
    "                    summary[f'{m}_std'] = np.std(valid) if valid else np.nan\n",
    "                resultados_cv_anom[nombre_mod][param_key]['cv_scores'] = {k: (round(v, 5) if not pd.isna(v) else None) for k,v in summary.items()}\n",
    "                resultados_cv_anom[nombre_mod][param_key]['t_fit_m'] = round(np.mean(fits_t), 3) if fits_t else np.nan\n",
    "                resultados_cv_anom[nombre_mod][param_key]['t_pred_m'] = round(np.mean(preds_t), 3) if preds_t else np.nan\n",
    "                resultados_cv_anom[nombre_mod][param_key]['status'] = 'OK'\n",
    "            else:\n",
    "                resultados_cv_anom[nombre_mod][param_key]['status'] = 'Error CV'\n",
    "                resultados_cv_anom[nombre_mod][param_key]['cv_scores'] = {f'{m}_mean': np.nan for m in SCORERS} # Ensure scores dict exists even on error\n",
    "\n",
    "            del m_fold; gc.collect()\n",
    "\n",
    "    logging.info(\"Cross-Validation finalizada.\")\n",
    "else:\n",
    "    logging.warning(\"Cross-Validation no ejecutada por errores previos.\")\n",
    "\n",
    "def json_serializer(obj):\n",
    "    \"\"\"Función auxiliar para serializar tipos numpy/pandas a JSON.\"\"\"\n",
    "    if isinstance(obj, (np.number, np.bool_)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, type):\n",
    "        return str(obj)\n",
    "    if pd.isna(obj):\n",
    "        return None\n",
    "    return str(obj)\n",
    "\n",
    "logging.info(f\"Guardando resultados CV: {RESULTS_FILE}\")\n",
    "try:\n",
    "    with open(RESULTS_FILE, 'w') as f:\n",
    "        json.dump(resultados_cv_anom, f, indent=2, default=json_serializer)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error guardando JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -np.inf if HIGHER_BETTER else np.inf\n",
    "best_cfg = None\n",
    "best_name = None\n",
    "best_param_key = None\n",
    "best_cv_scores = None\n",
    "\n",
    "logging.info(f\"Buscando mejor configuración basada en '{PRIMARY_METRIC}'...\")\n",
    "\n",
    "if not resultados_cv_anom:\n",
    "    logging.warning(\"No hay resultados de CV para seleccionar.\")\n",
    "else:\n",
    "    for name, p_dict in resultados_cv_anom.items():\n",
    "        for p_key, res in p_dict.items():\n",
    "            if res.get('status') == 'OK' and 'cv_scores' in res:\n",
    "                score = res['cv_scores'].get(f'{PRIMARY_METRIC}_mean')\n",
    "                if score is not None and not pd.isna(score):\n",
    "                    is_better = (HIGHER_BETTER and score > best_score) or \\\n",
    "                                (not HIGHER_BETTER and score < best_score)\n",
    "                    if is_better:\n",
    "                        best_score = score\n",
    "                        best_cfg = res.get('params')\n",
    "                        best_name = name\n",
    "                        best_param_key = p_key\n",
    "                        best_cv_scores = res.get('cv_scores')\n",
    "                        logging.info(f\"  Nuevo mejor: {best_name} ({best_param_key}), Score: {best_score:.5f}\")\n",
    "\n",
    "    if best_cfg is not None:\n",
    "        logging.info(f\"Mejor Modelo: {best_name}\")\n",
    "        logging.info(f\"  Parámetros: {best_cfg}\")\n",
    "        logging.info(f\"  Score CV ({PRIMARY_METRIC}): {best_score:.5f}\")\n",
    "        logging.info(f\"  Scores CV Detalle: {best_cv_scores}\")\n",
    "    else:\n",
    "        logging.warning(\"No se encontró una configuración válida como la mejor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved = False\n",
    "final_model = None\n",
    "test_metrics = {}\n",
    "\n",
    "if scaling_ok and best_cfg is not None and best_name is not None:\n",
    "    logging.info(f\"Re-entrenando mejor modelo ({best_name}) con datos normales...\")\n",
    "\n",
    "    cfg_best = modelos_anomalia.get(best_name)\n",
    "    if cfg_best:\n",
    "        model_class = cfg_best.get('model')\n",
    "        final_params = {**cfg_best.get('fixed', {}), **best_cfg}\n",
    "\n",
    "        if model_class:\n",
    "            final_model = model_class(**final_params)\n",
    "\n",
    "            try:\n",
    "                if X_train_normal_norm.isna().any().any():\n",
    "                    logging.warning(\"NaNs detectados en datos de entrenamiento final. Imputando con 0.\")\n",
    "                    X_train_normal_norm = X_train_normal_norm.fillna(0)\n",
    "\n",
    "                t_start_fit = time.time()\n",
    "                final_model.fit(X_train_normal_norm)\n",
    "                t_fit = time.time() - t_start_fit\n",
    "                logging.info(f\"Entrenamiento final completado ({t_fit:.2f}s).\")\n",
    "\n",
    "                logging.info(\"Evaluando modelo final en conjunto de test...\")\n",
    "                scores_t, preds_raw_t = None, None\n",
    "\n",
    "                if X_test_norm.isna().any().any():\n",
    "                    logging.warning(\"NaNs detectados en datos de test. Imputando con 0.\")\n",
    "                    X_test_norm = X_test_norm.fillna(0)\n",
    "\n",
    "                if hasattr(final_model, 'decision_function'):\n",
    "                    scores_t = final_model.decision_function(X_test_norm)\n",
    "                elif hasattr(final_model, 'score_samples'):\n",
    "                    scores_t = -final_model.score_samples(X_test_norm)\n",
    "\n",
    "                if hasattr(final_model, 'predict'):\n",
    "                    preds_raw_t = final_model.predict(X_test_norm)\n",
    "                elif scores_t is not None:\n",
    "                    cont = best_cfg.get('contamination','auto'); nu=best_cfg.get('nu')\n",
    "                    pct = (nu*100) if nu else (2 if cont=='auto' else cont*100)\n",
    "                    try:\n",
    "                        thr = np.percentile(scores_t[~np.isnan(scores_t)], pct)\n",
    "                        preds_raw_t = np.where(scores_t < thr, -1, 1)\n",
    "                    except IndexError:\n",
    "                         preds_raw_t = np.ones_like(scores_t)\n",
    "\n",
    "\n",
    "                if scores_t is not None:\n",
    "                    try:\n",
    "                        test_metrics['roc_auc'] = round(roc_auc_score(y_test, -scores_t), 5)\n",
    "                    except Exception: pass\n",
    "                    try:\n",
    "                        test_metrics['pr_auc'] = round(average_precision_score(y_test, -scores_t, pos_label=1), 5)\n",
    "                    except Exception: pass\n",
    "\n",
    "                if preds_raw_t is not None:\n",
    "                    preds_map_t = mapear_prediccion_anomalia(preds_raw_t)\n",
    "                    try:\n",
    "                        test_metrics['recall'] = round(recall_score(y_test, preds_map_t, pos_label=1, zero_division=0), 5)\n",
    "                    except Exception: pass\n",
    "                    try:\n",
    "                        test_metrics['precision'] = round(precision_score(y_test, preds_map_t, pos_label=1, zero_division=0), 5)\n",
    "                    except Exception: pass\n",
    "                    try:\n",
    "                        test_metrics['f1'] = round(f1_score(y_test, preds_map_t, pos_label=1, zero_division=0), 5)\n",
    "                    except Exception: pass\n",
    "\n",
    "                    print(f\"\\nMétricas Test: {test_metrics}\")\n",
    "                    print(\"\\nMatriz de Confusión Test:\")\n",
    "                    print(confusion_matrix(y_test, preds_map_t, labels=[0, 1]))\n",
    "                    print(\"\\nReporte de Clasificación Test:\")\n",
    "                    print(classification_report(y_test, preds_map_t, labels=[0, 1], target_names=['Normal', 'Anomalia'], zero_division=0))\n",
    "                elif test_metrics:\n",
    "                    print(f\"\\nMétricas Test (basadas en score): {test_metrics}\")\n",
    "                else:\n",
    "                    print(\"\\nNo se pudieron calcular métricas de test.\")\n",
    "\n",
    "\n",
    "                logging.info(f\"Guardando modelo final: {BEST_MODEL_FILE}\")\n",
    "                try:\n",
    "                  \n",
    "                    try:\n",
    "                        if hasattr(final_model, 'feature_names_in_') and nombres_caracteristicas:\n",
    "                           setattr(final_model, 'feature_names_in_', nombres_caracteristicas)\n",
    "                    except AttributeError:\n",
    "                        pass \n",
    "\n",
    "                    joblib.dump(final_model, BEST_MODEL_FILE)\n",
    "                    logging.info(\"Modelo guardado correctamente.\")\n",
    "                    model_saved = True\n",
    "                except Exception as e_save:\n",
    "                    logging.error(f\"Error guardando modelo: {e_save}\")\n",
    "\n",
    "            except Exception as e_final:\n",
    "                logging.error(f\"Error en re-entrenamiento/evaluación: {e_final}\", exc_info=False)\n",
    "        else:\n",
    "            logging.error(f\"Clase de modelo no encontrada para {best_name}\")\n",
    "    else:\n",
    "        logging.error(f\"Configuración no encontrada para {best_name}\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\"Re-entrenamiento no ejecutado: faltan datos escalados o mejor configuración.\")\n",
    "\n",
    "\n",
    "if 'model_saved' not in locals():\n",
    "    model_saved = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Analizando importancia de características (si aplica)...\")\n",
    "features_dict = {}\n",
    "NUM_TOP = 15\n",
    "\n",
    "if model_saved and final_model is not None and nombres_caracteristicas:\n",
    "    logging.info(f\"Calculando importancia para: {best_name}\")\n",
    "    try:\n",
    "        imp = None\n",
    "        feature_names = nombres_caracteristicas # Usar la lista guardada\n",
    "\n",
    "        if hasattr(final_model, 'feature_importances_'):\n",
    "            imp = final_model.feature_importances_\n",
    "            imp_type = 'importance'\n",
    "        elif isinstance(final_model, OneClassSVM) and getattr(final_model, 'kernel', '') == 'linear' and hasattr(final_model, 'coef_'):\n",
    "             imp = np.abs(final_model.coef_[0])\n",
    "             imp_type = 'abs_coef_proxy'\n",
    "        else:\n",
    "             features_dict = {\"info\": \"Importancia no disponible para este modelo.\"}\n",
    "\n",
    "        if imp is not None:\n",
    "             # Asegurarse de que la longitud coincida\n",
    "             if len(imp) == len(feature_names):\n",
    "                 df_imp = pd.DataFrame({'feature': feature_names, imp_type: imp})\n",
    "                 df_imp = df_imp.sort_values(imp_type, ascending=False).head(NUM_TOP)\n",
    "                 features_dict = df_imp.round(5).to_dict('records')\n",
    "                 print(f\"\\nTop {len(features_dict)} Características (Proxy):\")\n",
    "                 for f in features_dict:\n",
    "                     print(f\"  - {f}\")\n",
    "             else:\n",
    "                 features_dict = {\"error\": f\"Discrepancia en número de importancias ({len(imp)}) y características ({len(feature_names)}).\"}\n",
    "                 logging.error(features_dict[\"error\"])\n",
    "\n",
    "        elif \"info\" in features_dict:\n",
    "             print(f\"  {features_dict['info']}\")\n",
    "        elif \"error\" in features_dict:\n",
    "             print(f\"  Error calculando importancia.\")\n",
    "\n",
    "\n",
    "        if features_dict: \n",
    "            try:\n",
    "                with open(FEATURES_FILE, 'w') as f:\n",
    "                    json.dump(features_dict, f, indent=2, default=json_serializer)\n",
    "                logging.info(f\"Información de importancia guardada: {FEATURES_FILE}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error guardando JSON de importancia: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error analizando importancia: {e}\", exc_info=False)\n",
    "else:\n",
    "    logging.warning(\"Análisis de importancia omitido: modelo no guardado, no cargado o faltan nombres de características.\")\n",
    "\n",
    "logging.info(\"Proceso completado.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
