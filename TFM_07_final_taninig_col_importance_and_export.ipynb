{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFM_07_final_taninig_col_importance_and_export.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importaciones completadas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import joblib\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.12f}'.format(x))\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"Importaciones completadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:38:54,236 - INFO - [2339540163.py:10] - Iniciando script de entrenamiento para el mejor GBR. Log en: training_best_gbr.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configurado.\n"
     ]
    }
   ],
   "source": [
    "LOG_FILENAME = 'training_best_gbr.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILENAME, mode='w', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logging.info(f\"Iniciando script de entrenamiento para el mejor GBR. Log en: {LOG_FILENAME}\")\n",
    "print(\"Logging configurado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:32,839 - INFO - [1538478532.py:4] - Archivos CSV cargados correctamente.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos CSV cargados.\n",
      "Train shape: (85932, 442)\n",
      "Test shape: (21483, 442)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_train = pd.read_csv(\"datos_divididos/datos_financieros_train_completo.csv\")\n",
    "    df_test = pd.read_csv(\"datos_divididos/datos_financieros_test_completo.csv\")\n",
    "    logging.info(\"Archivos CSV cargados correctamente.\")\n",
    "    print(\"Archivos CSV cargados.\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Error al cargar los archivos CSV: {e}. Asegúrate de que los archivos están en la ruta correcta.\")\n",
    "    print(f\"Error carga archivo: {e}\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    logging.error(f\"Ocurrió un error inesperado al cargar los datos: {e}\")\n",
    "    print(f\"Error carga datos: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:32,912 - INFO - [2472782200.py:1] - Iniciando limpieza inicial...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpieza inicial datos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:33,316 - INFO - [2472782200.py:10] - Columnas eliminadas de train: ['timestamp']\n",
      "2025-04-06 18:39:33,493 - INFO - [2472782200.py:14] - Columnas eliminadas de test: ['timestamp']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols eliminadas train: ['timestamp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:33,503 - INFO - [2472782200.py:17] - Reseteando índices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols eliminadas test: ['timestamp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:34,018 - INFO - [2472782200.py:20] - Índices reseteados.\n",
      "2025-04-06 18:39:34,023 - INFO - [2472782200.py:23] - Comprobando y eliminando duplicados...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índices reseteados.\n",
      "Comprobando duplicados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:45,405 - INFO - [2472782200.py:26] - Duplicados encontrados en train: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados train: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:47,328 - INFO - [2472782200.py:34] - Duplicados encontrados en test: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados test: 0\n",
      "Limpieza inicial completada.\n",
      "Train shape final: (85932, 441)\n",
      "Test shape final: (21483, 441)\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Iniciando limpieza inicial...\")\n",
    "print(\"Limpieza inicial datos.\")\n",
    "\n",
    "drop_cols_initial = ['timestamp', 'Unnamed: 0']\n",
    "cols_to_drop_train = [col for col in drop_cols_initial if col in df_train.columns]\n",
    "cols_to_drop_test = [col for col in drop_cols_initial if col in df_test.columns]\n",
    "\n",
    "if cols_to_drop_train:\n",
    "    df_train = df_train.drop(columns=cols_to_drop_train)\n",
    "    logging.info(f\"Columnas eliminadas de train: {cols_to_drop_train}\")\n",
    "    print(f\"Cols eliminadas train: {cols_to_drop_train}\")\n",
    "if cols_to_drop_test:\n",
    "    df_test = df_test.drop(columns=cols_to_drop_test)\n",
    "    logging.info(f\"Columnas eliminadas de test: {cols_to_drop_test}\")\n",
    "    print(f\"Cols eliminadas test: {cols_to_drop_test}\")\n",
    "\n",
    "logging.info(\"Reseteando índices...\")\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "logging.info(\"Índices reseteados.\")\n",
    "print(\"Índices reseteados.\")\n",
    "\n",
    "logging.info(\"Comprobando y eliminando duplicados...\")\n",
    "print(\"Comprobando duplicados.\")\n",
    "duplicados_train = df_train.duplicated().sum()\n",
    "logging.info(f\"Duplicados encontrados en train: {duplicados_train}\")\n",
    "print(f\"Duplicados train: {duplicados_train}\")\n",
    "if duplicados_train > 0:\n",
    "    df_train.drop_duplicates(inplace=True)\n",
    "    logging.info(\"Duplicados eliminados en train.\")\n",
    "    print(f\"Duplicados eliminados train. Shape: {df_train.shape}\")\n",
    "\n",
    "duplicados_test = df_test.duplicated().sum()\n",
    "logging.info(f\"Duplicados encontrados en test: {duplicados_test}\")\n",
    "print(f\"Duplicados test: {duplicados_test}\")\n",
    "if duplicados_test > 0:\n",
    "    df_test.drop_duplicates(inplace=True)\n",
    "    logging.info(\"Duplicados eliminados en test.\")\n",
    "    print(f\"Duplicados eliminados test. Shape: {df_test.shape}\")\n",
    "\n",
    "print(\"Limpieza inicial completada.\")\n",
    "print(f\"Train shape final: {df_train.shape}\")\n",
    "print(f\"Test shape final: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Iniciando limpieza inicial...\")\n",
    "print(\"Limpieza inicial datos.\")\n",
    "\n",
    "# Columnas a eliminar inicialmente\n",
    "# AÑADIMOS 'PotentialLiquidityGap' A LA LISTA\n",
    "drop_cols_initial = ['timestamp', 'Unnamed: 0', 'PotentialLiquidityGap']\n",
    "cols_to_drop_train = [col for col in drop_cols_initial if col in df_train.columns]\n",
    "cols_to_drop_test = [col for col in drop_cols_initial if col in df_test.columns]\n",
    "\n",
    "if cols_to_drop_train:\n",
    "    df_train = df_train.drop(columns=cols_to_drop_train)\n",
    "    logging.info(f\"Columnas eliminadas de train: {cols_to_drop_train}\")\n",
    "    print(f\"Cols eliminadas train: {cols_to_drop_train}\")\n",
    "if cols_to_drop_test:\n",
    "    df_test = df_test.drop(columns=cols_to_drop_test)\n",
    "    logging.info(f\"Columnas eliminadas de test: {cols_to_drop_test}\")\n",
    "    print(f\"Cols eliminadas test: {cols_to_drop_test}\")\n",
    "\n",
    "# Resetear índices (sin cambios)\n",
    "logging.info(\"Reseteando índices...\")\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "logging.info(\"Índices reseteados.\")\n",
    "print(\"Índices reseteados.\")\n",
    "\n",
    "# Eliminar duplicados (sin cambios)\n",
    "logging.info(\"Comprobando y eliminando duplicados...\")\n",
    "print(\"Comprobando duplicados.\")\n",
    "duplicados_train = df_train.duplicated().sum()\n",
    "logging.info(f\"Duplicados encontrados en train: {duplicados_train}\")\n",
    "print(f\"Duplicados train: {duplicados_train}\")\n",
    "if duplicados_train > 0:\n",
    "    df_train.drop_duplicates(inplace=True)\n",
    "    logging.info(\"Duplicados eliminados en train.\")\n",
    "    print(f\"Duplicados eliminados train. Shape: {df_train.shape}\")\n",
    "\n",
    "duplicados_test = df_test.duplicated().sum()\n",
    "logging.info(f\"Duplicados encontrados en test: {duplicados_test}\")\n",
    "print(f\"Duplicados test: {duplicados_test}\")\n",
    "if duplicados_test > 0:\n",
    "    df_test.drop_duplicates(inplace=True)\n",
    "    logging.info(\"Duplicados eliminados en test.\")\n",
    "    print(f\"Duplicados eliminados test. Shape: {df_test.shape}\")\n",
    "\n",
    "print(\"Limpieza inicial completada.\")\n",
    "print(f\"Train shape final: {df_train.shape}\")\n",
    "print(f\"Test shape final: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones normalización definidas.\n"
     ]
    }
   ],
   "source": [
    "def normalize_column_within_group(col: pd.Series, margen: float = 0.2):\n",
    "    \"\"\"Calcula límites escalador.\"\"\"\n",
    "    min_val = col.min()\n",
    "    max_val = col.max()\n",
    "    if pd.isna(min_val) or pd.isna(max_val):\n",
    "        return col, (np.nan, np.nan)\n",
    "\n",
    "    col_range = max_val - min_val\n",
    "    if abs(col_range) < 1e-9:\n",
    "        min_ext = min_val - 0.0001 if min_val != max_val else min_val\n",
    "        max_ext = max_val + 0.0001 if min_val != max_val else max_val\n",
    "        denominador = max_ext - min_ext\n",
    "        if abs(denominador) < 1e-9:\n",
    "             return col, (np.nan, np.nan)\n",
    "    else:\n",
    "        min_ext = min_val - margen * col_range\n",
    "        max_ext = max_val + margen * col_range\n",
    "        denominador = max_ext - min_ext\n",
    "        if abs(denominador) < 1e-9:\n",
    "             return col, (np.nan, np.nan)\n",
    "\n",
    "    return col, (min_ext, max_ext)\n",
    "\n",
    "\n",
    "def normalizar_datos_ml(df_train: pd.DataFrame,\n",
    "                        df_test: pd.DataFrame,\n",
    "                        columna_simbolo: str,\n",
    "                        margen: float = 0.2):\n",
    "    \"\"\"Normaliza datos por grupo.\"\"\"\n",
    "    logging.info(f\"Normalizando datos para ML por grupo '{columna_simbolo}'...\")\n",
    "\n",
    "    if columna_simbolo not in df_train.columns or columna_simbolo not in df_test.columns:\n",
    "        error_msg = f\"La columna '{columna_simbolo}' no existe en ambos DataFrames.\"\n",
    "        logging.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    train_original_idx = df_train.index\n",
    "    test_original_idx = df_test.index\n",
    "\n",
    "    logging.info(\"Calculando escaladores en el conjunto de train...\")\n",
    "    escaladores = {}\n",
    "    columnas_numericas_train = df_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    objetivos = ['target','future_max_increase_capped']\n",
    "    cols_to_exclude = [columna_simbolo] + objetivos\n",
    "    columnas_a_normalizar = [col for col in columnas_numericas_train if col not in cols_to_exclude]\n",
    "\n",
    "    if not columnas_a_normalizar:\n",
    "        logging.warning(\"No hay columnas numéricas en df_train para calcular escaladores (excluyendo símbolo y objetivos).\")\n",
    "        return df_train.copy(), df_test.copy(), {}\n",
    "\n",
    "    logging.info(f\"Columnas a normalizar: {len(columnas_a_normalizar)}\")\n",
    "\n",
    "    grouped_train = df_train.groupby(columna_simbolo)\n",
    "    for simbolo, grupo_df_train in tqdm(grouped_train, total=len(grouped_train.groups), desc=\"Calculando escaladores\"):\n",
    "        escaladores_simbolo = {}\n",
    "        for col_name in columnas_a_normalizar:\n",
    "            if col_name in grupo_df_train.columns:\n",
    "                try:\n",
    "                    _ , escalador_params = normalize_column_within_group(grupo_df_train[col_name], margen)\n",
    "                    escaladores_simbolo[col_name] = escalador_params\n",
    "                except Exception as e_calc:\n",
    "                    logging.error(f\"Error calculando escalador para {simbolo}/{col_name}: {e_calc}\")\n",
    "                    escaladores_simbolo[col_name] = (np.nan, np.nan)\n",
    "            else:\n",
    "                 escaladores_simbolo[col_name] = (np.nan, np.nan)\n",
    "        escaladores[simbolo] = escaladores_simbolo\n",
    "    logging.info(\"Escaladores calculados.\")\n",
    "\n",
    "    logging.info(\"Aplicando escaladores...\")\n",
    "    def _aplicar_escaladores_interna(df_input: pd.DataFrame, escaladores_dict: dict, cols_norm: list, desc: str) -> pd.DataFrame:\n",
    "        df_output = df_input.copy()\n",
    "        grouped_df = df_output.groupby(columna_simbolo)\n",
    "\n",
    "        for simbolo, grupo_df in tqdm(grouped_df, total=len(grouped_df.groups), desc=desc):\n",
    "            if simbolo not in escaladores_dict:\n",
    "                logging.warning(f\"Símbolo '{simbolo}' encontrado en {desc} pero no tiene escaladores. Sus datos numéricos no se normalizarán.\")\n",
    "                continue\n",
    "\n",
    "            scalers_for_symbol = escaladores_dict[simbolo]\n",
    "\n",
    "            for col_name in cols_norm:\n",
    "                if col_name in grupo_df.columns:\n",
    "                    if col_name in scalers_for_symbol:\n",
    "                        min_ext, max_ext = scalers_for_symbol[col_name]\n",
    "\n",
    "                        if pd.isna(min_ext) or pd.isna(max_ext):\n",
    "                            df_output.loc[grupo_df.index, col_name] = np.nan\n",
    "                            logging.debug(f\"Aplicando NaN a {simbolo}/{col_name} debido a escaladores NaN.\")\n",
    "                            continue\n",
    "\n",
    "                        denominador = max_ext - min_ext\n",
    "                        col_original = grupo_df[col_name]\n",
    "\n",
    "                        if abs(denominador) < 1e-9:\n",
    "                            scaled_val = 0.5 if min_ext != max_ext else 0.0\n",
    "                            scaled_col = pd.Series(scaled_val, index=col_original.index, name=col_name)\n",
    "                            logging.debug(f\"Aplicando {scaled_val} a {simbolo}/{col_name} debido a denominador ~0.\")\n",
    "                        else:\n",
    "                            scaled_col = (col_original - min_ext) / denominador\n",
    "\n",
    "                        df_output.loc[grupo_df.index, col_name] = scaled_col.where(col_original.notna(), np.nan)\n",
    "                    else:\n",
    "                        logging.warning(f\"Escalador no encontrado para {simbolo}/{col_name} en diccionario, aunque la columna está en la lista. Se dejará sin normalizar.\")\n",
    "\n",
    "        return df_output\n",
    "\n",
    "    df_train_normalizado = _aplicar_escaladores_interna(df_train, escaladores, columnas_a_normalizar, \"Normalizando Train\")\n",
    "    logging.info(\"Conjunto de Train normalizado.\")\n",
    "\n",
    "    df_test_normalizado = _aplicar_escaladores_interna(df_test, escaladores, columnas_a_normalizar, \"Normalizando Test\")\n",
    "    logging.info(\"Conjunto de Test normalizado.\")\n",
    "\n",
    "    logging.info(\"Normalización completada.\")\n",
    "    df_train_normalizado.index = train_original_idx\n",
    "    df_test_normalizado.index = test_original_idx\n",
    "    return df_train_normalizado, df_test_normalizado, escaladores\n",
    "\n",
    "\n",
    "def guardar_escaladores(escaladores, nombre_archivo):\n",
    "    \"\"\"Guarda escaladores en joblib.\"\"\"\n",
    "    try:\n",
    "        joblib.dump(escaladores, nombre_archivo)\n",
    "        logging.info(f\"Escaladores guardados en {nombre_archivo}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error guardando escaladores en {nombre_archivo}: {e}\")\n",
    "\n",
    "def convert_value_for_json(value):\n",
    "    \"\"\"Convierte valores a JSON.\"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "         return [convert_value_for_json(v) for v in value]\n",
    "    if isinstance(value, (np.number, np.bool_)):\n",
    "        if np.isnan(value): return None\n",
    "        return value.item()\n",
    "    if isinstance(value, float) and math.isnan(value):\n",
    "         return None\n",
    "    return value\n",
    "\n",
    "def guardar_escaladores_joblib_json(escaladores, base_nombre_archivo):\n",
    "    \"\"\"Guarda escaladores joblib y JSON.\"\"\"\n",
    "    joblib_filename = f\"{base_nombre_archivo}.joblib\"\n",
    "    guardar_escaladores(escaladores, joblib_filename)\n",
    "\n",
    "    json_filename = f\"{base_nombre_archivo}.json\"\n",
    "    json_compatible_scalers = {}\n",
    "    try:\n",
    "        for simbolo, cols_scalers in escaladores.items():\n",
    "            json_compatible_scalers[str(simbolo)] = {}\n",
    "            for col, (min_e, max_e) in cols_scalers.items():\n",
    "                 json_min = convert_value_for_json(min_e)\n",
    "                 json_max = convert_value_for_json(max_e)\n",
    "                 json_compatible_scalers[str(simbolo)][str(col)] = [json_min, json_max]\n",
    "\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_compatible_scalers, f, indent=4, ensure_ascii=False)\n",
    "        logging.info(f\"Escaladores guardados en formato JSON: {json_filename}\")\n",
    "\n",
    "    except Exception as e_json:\n",
    "        logging.error(f\"Error guardando escaladores en JSON ({json_filename}): {e_json}\")\n",
    "\n",
    "print(\"Funciones normalización definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:47,537 - INFO - [439320242.py:4] - Aplicando normalización con margen=0.2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando normalización (margen=0.2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:39:47,937 - INFO - [2376216886.py:30] - Normalizando datos para ML por grupo 'symbol'...\n",
      "2025-04-06 18:39:47,941 - INFO - [2376216886.py:40] - Calculando escaladores en el conjunto de train...\n",
      "2025-04-06 18:39:48,406 - INFO - [2376216886.py:51] - Columnas a normalizar: 438\n",
      "Calculando escaladores: 100%|██████████| 77/77 [00:28<00:00,  2.70it/s]\n",
      "2025-04-06 18:40:16,981 - INFO - [2376216886.py:67] - Escaladores calculados.\n",
      "2025-04-06 18:40:16,986 - INFO - [2376216886.py:69] - Aplicando escaladores...\n",
      "Normalizando Train: 100%|██████████| 77/77 [02:00<00:00,  1.57s/it]\n",
      "2025-04-06 18:42:18,519 - INFO - [2376216886.py:108] - Conjunto de Train normalizado.\n",
      "Normalizando Test: 100%|██████████| 77/77 [01:42<00:00,  1.33s/it]\n",
      "2025-04-06 18:44:01,123 - INFO - [2376216886.py:111] - Conjunto de Test normalizado.\n",
      "2025-04-06 18:44:01,125 - INFO - [2376216886.py:113] - Normalización completada.\n",
      "2025-04-06 18:44:01,272 - INFO - [439320242.py:16] - Normalización completada en 253.73 segundos.\n",
      "2025-04-06 18:44:01,275 - INFO - [439320242.py:19] - Guardando escaladores...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalización: 253.73s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:04,897 - INFO - [2376216886.py:123] - Escaladores guardados en escaladores_gbr_final.joblib\n",
      "2025-04-06 18:44:06,329 - INFO - [2376216886.py:155] - Escaladores guardados en formato JSON: escaladores_gbr_final.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaladores guardados: escaladores_gbr_final.[joblib/json]\n",
      "Muestra train normalizado:\n",
      "            open           high            low          close         volume  quote_asset_volume  number_of_trades  taker_buy_base_asset_volume  taker_buy_quote_asset_volume symbol    close_lag_1   volume_lag_1     open_lag_1     high_lag_1      low_lag_1  quote_asset_volume_lag_1  number_of_trades_lag_1  taker_buy_base_asset_volume_lag_1  taker_buy_quote_asset_volume_lag_1    close_lag_2   volume_lag_2     open_lag_2     high_lag_2      low_lag_2  quote_asset_volume_lag_2  number_of_trades_lag_2  taker_buy_base_asset_volume_lag_2  taker_buy_quote_asset_volume_lag_2    close_lag_3   volume_lag_3     open_lag_3     high_lag_3      low_lag_3  quote_asset_volume_lag_3  number_of_trades_lag_3  taker_buy_base_asset_volume_lag_3  taker_buy_quote_asset_volume_lag_3    close_lag_4   volume_lag_4     open_lag_4     high_lag_4      low_lag_4  quote_asset_volume_lag_4  number_of_trades_lag_4  taker_buy_base_asset_volume_lag_4  taker_buy_quote_asset_volume_lag_4    close_lag_5   volume_lag_5  \\\n",
      "0 0.248376623377 0.250830564784 0.257799671593 0.255216693419 0.244043762056      0.239535820860    0.218335787923               0.246412522539                0.241876799698    ACE 0.255216693419 0.192003142302 0.264610389610 0.250830564784 0.257799671593            0.189826848856          0.175994108984                     0.145616484245                      0.145498292121 0.263242375602 0.174715479088 0.264610389610 0.250830564784 0.266009852217            0.173326922663          0.163107511046                     0.142857142857                      0.142857142857 0.279293739968 0.175662839697 0.264610389610 0.259136212625 0.274220032841            0.174269887808          0.175994108984                     0.205152527850                      0.202555782401 0.271268057785 0.148078467477 0.264610389610 0.259136212625 0.274220032841            0.147851213278          0.152061855670                     0.153060366515                      0.152623871613 0.263242375602 0.233144945339   \n",
      "1 0.197978870005 0.200275608636 0.200460829493 0.195512820513 0.150386901037      0.150043853002    0.159118907998               0.146220416653                0.146071632261   ALGO 0.202380952381 0.159352729306 0.204869085898 0.202572347267 0.205069124424            0.158629768381          0.165278667521                     0.161862437803                      0.161050250228 0.204670329670 0.157569410990 0.209462563160 0.207165824529 0.209677419355            0.156930187125          0.161829202188                     0.150114904420                      0.149804129686 0.209249084249 0.156202013124 0.209462563160 0.207165824529 0.211981566820            0.155628381264          0.159858079141                     0.154053186927                      0.153582287569 0.213827838828 0.153433097230 0.207165824529 0.211759301791 0.211981566820            0.152972491045          0.163061154093                     0.152526859152                      0.152116921809 0.209249084249 0.153360969800   \n",
      "2 0.176339285714 0.198660714286 0.187500000000 0.197802197802 0.159981301029      0.158971776439    0.161266568483               0.171289359414                0.169650687811  ALICE 0.197802197802 0.142857142857 0.187500000000 0.187500000000 0.198660714286            0.142857142857          0.142857142857                     0.142857142857                      0.142857142857 0.197802197802 0.144885030493 0.209821428571 0.209821428571 0.198660714286            0.144764397878          0.157584683358                     0.143301602426                      0.143276705339 0.197802197802 0.142857142857 0.187500000000 0.187500000000 0.198660714286            0.142857142857          0.142857142857                     0.142857142857                      0.142857142857 0.197802197802 0.142857142857 0.187500000000 0.187500000000 0.198660714286            0.142857142857          0.142857142857                     0.142857142857                      0.142857142857 0.197802197802 0.162185570996   \n",
      "\n",
      "      open_lag_5     high_lag_5      low_lag_5  quote_asset_volume_lag_5  number_of_trades_lag_5  taker_buy_base_asset_volume_lag_5  taker_buy_quote_asset_volume_lag_5    close_lag_6   volume_lag_6     open_lag_6     high_lag_6      low_lag_6  quote_asset_volume_lag_6  number_of_trades_lag_6  taker_buy_base_asset_volume_lag_6  taker_buy_quote_asset_volume_lag_6    close_lag_7   volume_lag_7     open_lag_7     high_lag_7      low_lag_7  quote_asset_volume_lag_7  number_of_trades_lag_7  taker_buy_base_asset_volume_lag_7  taker_buy_quote_asset_volume_lag_7    close_lag_8   volume_lag_8     open_lag_8     high_lag_8      low_lag_8  quote_asset_volume_lag_8  number_of_trades_lag_8  taker_buy_base_asset_volume_lag_8  taker_buy_quote_asset_volume_lag_8    close_lag_9   volume_lag_9     open_lag_9     high_lag_9      low_lag_9  quote_asset_volume_lag_9  number_of_trades_lag_9  taker_buy_base_asset_volume_lag_9  taker_buy_quote_asset_volume_lag_9   close_lag_10  volume_lag_10    open_lag_10  \\\n",
      "0 0.264610389610 0.250830564784 0.257799671593            0.229140419408          0.210972017673                     0.217529525287                      0.214252346577 0.271268057785 0.177698497483 0.288961038961 0.284053156146 0.274220032841            0.176260001647          0.185198821797                     0.154817069346                      0.154350837359 0.295345104334 0.179948478927 0.297077922078 0.284053156146 0.290640394089            0.178446113497          0.198085419735                     0.166787005537                      0.165857779952 0.303370786517 0.202955331447 0.313311688312 0.300664451827 0.298850574713            0.200664940622          0.196244477172                     0.160871271219                      0.160222717191 0.319422150883 0.194634236245 0.321428571429 0.308970099668 0.323481116585            0.192759542922          0.183357879234                     0.143055668628                      0.143048533791 0.327447833066 0.255600560744 0.337662337662   \n",
      "1 0.216352779054 0.211759301791 0.214285714286            0.152913616458          0.165525057902                     0.148898383877                      0.148645715625 0.220695970696 0.150280653445 0.230133210841 0.225539733578 0.225806451613            0.149975251562          0.157886956093                     0.150983500089                      0.150656312814 0.234432234432 0.146931694805 0.230133210841 0.232429949472 0.237327188940            0.146769158388          0.151727196570                     0.145847350353                      0.145731875367 0.234432234432 0.161922107040 0.239320165365 0.239320165365 0.239631336406            0.161194283705          0.172423988567                     0.170840359415                      0.169789244680 0.241300366300 0.151697288000 0.246210381259 0.243913642627 0.246543778802            0.151369667940          0.161336421426                     0.147076847116                      0.146924128449 0.250457875458 0.168102175286 0.248507119890   \n",
      "2 0.198660714286 0.209821428571 0.198660714286            0.161044722671          0.172312223859                     0.143236622957                      0.143215365795 0.208791208791 0.157565616672 0.209821428571 0.209821428571 0.209821428571            0.156721434240          0.161266568483                     0.142857142857                      0.142857142857 0.230769230769 0.144103937831 0.220982142857 0.220982142857 0.232142857143            0.144034590324          0.146539027982                     0.142857142857                      0.142857142857 0.230769230769 0.157884163979 0.232142857143 0.232142857143 0.232142857143            0.157048582977          0.157584683358                     0.143170043993                      0.143153425196 0.230769230769 0.152517232513 0.243303571429 0.243303571429 0.232142857143            0.151995086787          0.168630338733                     0.145302970074                      0.145176619394 0.252747252747 0.147456322357 0.243303571429   \n",
      "\n",
      "     high_lag_10     low_lag_10  quote_asset_volume_lag_10  number_of_trades_lag_10  taker_buy_base_asset_volume_lag_10  taker_buy_quote_asset_volume_lag_10   close_lag_11  volume_lag_11    open_lag_11    high_lag_11     low_lag_11  quote_asset_volume_lag_11  number_of_trades_lag_11  taker_buy_base_asset_volume_lag_11  taker_buy_quote_asset_volume_lag_11   close_lag_12  volume_lag_12    open_lag_12    high_lag_12     low_lag_12  quote_asset_volume_lag_12  number_of_trades_lag_12  taker_buy_base_asset_volume_lag_12  taker_buy_quote_asset_volume_lag_12   close_lag_13  volume_lag_13    open_lag_13    high_lag_13     low_lag_13  quote_asset_volume_lag_13  number_of_trades_lag_13  taker_buy_base_asset_volume_lag_13  taker_buy_quote_asset_volume_lag_13   close_lag_14  volume_lag_14    open_lag_14    high_lag_14     low_lag_14  quote_asset_volume_lag_14  number_of_trades_lag_14  taker_buy_base_asset_volume_lag_14  taker_buy_quote_asset_volume_lag_14   close_lag_15  volume_lag_15  \\\n",
      "0 0.325581395349 0.331691297209             0.251690599758           0.218335787923                      0.201857667366                       0.199940627078 0.335473515249 0.204503965822 0.321428571429 0.317275747508 0.331691297209             0.202358198326           0.212812960236                      0.245221367913                       0.241726337720 0.319422150883 0.173080114517 0.321428571429 0.308970099668 0.323481116585             0.171999473144           0.166789396171                      0.142857142857                       0.142857142857 0.327447833066 0.192878783709 0.337662337662 0.325581395349 0.331691297209             0.191157889987           0.183357879234                      0.152693344082                       0.152373747024 0.335473515249 0.244056271219 0.313311688312 0.317275747508 0.323481116585             0.240503918669           0.205449189985                      0.321543683010                       0.315419888897 0.319422150883 0.256479537928   \n",
      "1 0.248507119890 0.253456221198             0.167192952784           0.180801261519                      0.164644214780                       0.163869679450 0.250457875458 0.177963333870 0.241616903996 0.246210381259 0.248847926267             0.176675071410           0.163307544473                      0.200744056405                       0.198641371500 0.241300366300 0.143351237348 0.248507119890 0.243913642627 0.246543778802             0.143329397663           0.150248854285                      0.142857142857                       0.142857142857 0.250457875458 0.143724399621 0.246210381259 0.250803858521 0.253456221198             0.143691097041           0.153451929237                      0.144232592413                       0.144184115724 0.248168498168 0.147445224832 0.239320165365 0.243913642627 0.246543778802             0.147271874335           0.155669442665                      0.149864961023                       0.149608209317 0.241300366300 0.153397681367   \n",
      "2 0.243303571429 0.254464285714             0.147213856839           0.150220913108                      0.142857142857                       0.142857142857 0.241758241758 0.150188008087 0.232142857143 0.232142857143 0.243303571429             0.149790895422           0.150220913108                      0.157249795355                       0.156485373171 0.241758241758 0.145406132337 0.232142857143 0.232142857143 0.243303571429             0.145268053683           0.150220913108                      0.142857142857                       0.142857142857 0.252747252747 0.159690351650 0.232142857143 0.243303571429 0.243303571429             0.158778809143           0.161266568483                      0.143279009564                       0.143257216060 0.252747252747 0.167981746995 0.220982142857 0.243303571429 0.232142857143             0.166621498857           0.172312223859                      0.187603204891                       0.185234781393 0.219780219780 0.158650693895   \n",
      "\n",
      "     open_lag_15    high_lag_15     low_lag_15  quote_asset_volume_lag_15  number_of_trades_lag_15  taker_buy_base_asset_volume_lag_15  taker_buy_quote_asset_volume_lag_15   close_lag_16  volume_lag_16    open_lag_16    high_lag_16     low_lag_16  quote_asset_volume_lag_16  number_of_trades_lag_16  taker_buy_base_asset_volume_lag_16  taker_buy_quote_asset_volume_lag_16   close_lag_17  volume_lag_17    open_lag_17    high_lag_17     low_lag_17  quote_asset_volume_lag_17  number_of_trades_lag_17  taker_buy_base_asset_volume_lag_17  taker_buy_quote_asset_volume_lag_17   close_lag_18  volume_lag_18    open_lag_18    high_lag_18     low_lag_18  quote_asset_volume_lag_18  number_of_trades_lag_18  taker_buy_base_asset_volume_lag_18  taker_buy_quote_asset_volume_lag_18   close_lag_19  volume_lag_19    open_lag_19    high_lag_19     low_lag_19  quote_asset_volume_lag_19  number_of_trades_lag_19  taker_buy_base_asset_volume_lag_19  taker_buy_quote_asset_volume_lag_19   close_lag_20  \\\n",
      "0 0.321428571429 0.308970099668 0.323481116585             0.252397643605           0.223858615611                      0.302940650471                       0.297294210323 0.319422150883 0.245561540495 0.329545454545 0.317275747508 0.315270935961             0.241907014175           0.205449189985                      0.145296173758                       0.145208517188 0.335473515249 0.227153056280 0.329545454545 0.317275747508 0.331691297209             0.224237778532           0.220176730486                      0.161401785464                       0.160777925534 0.335473515249 0.144663465989 0.329545454545 0.317275747508 0.339901477833             0.144601344762           0.150220913108                      0.142857142857                       0.142857142857 0.343499197432 0.162449827832 0.337662337662 0.325581395349 0.339901477833             0.161790404619           0.174153166421                      0.143637899671                       0.143612531383 0.343499197432   \n",
      "1 0.239320165365 0.239320165365 0.241935483871             0.152995394574           0.157886956093                      0.155990415323                       0.155497901463 0.241300366300 0.165053387687 0.250803858521 0.248507119890 0.244239631336             0.164220013068           0.168728132854                      0.155701700675                       0.155222599616 0.252747252747 0.156053007355 0.259990813045 0.255397335783 0.255760368664             0.155583237445           0.165771448283                      0.146819670291                       0.146682713132 0.261904761905 0.155189205798 0.257694074414 0.257694074414 0.262672811060             0.154763979738           0.170206475139                      0.153629605384                       0.153260941932 0.259615384615 0.148430390508 0.273771244832 0.269177767570 0.264976958525             0.148242993106           0.173409550091                      0.142943919451                       0.142941304647 0.273351648352   \n",
      "2 0.220982142857 0.220982142857 0.220982142857             0.157749763886           0.161266568483                      0.143224626747                       0.143204575262 0.230769230769 0.192008951587 0.232142857143 0.232142857143 0.220982142857             0.189291918746           0.201767304860                      0.180861534989                       0.178787853604 0.241758241758 0.153888370921 0.243303571429 0.243303571429 0.243303571429             0.153301962292           0.164948453608                      0.142857142857                       0.142857142857 0.252747252747 0.142857142857 0.243303571429 0.243303571429 0.254464285714             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857 0.252747252747 0.148989280354 0.265625000000 0.265625000000 0.254464285714             0.148670373576           0.157584683358                      0.143204033254                       0.143187120568 0.263736263736   \n",
      "\n",
      "   volume_lag_20    open_lag_20    high_lag_20     low_lag_20  quote_asset_volume_lag_20  number_of_trades_lag_20  taker_buy_base_asset_volume_lag_20  taker_buy_quote_asset_volume_lag_20   close_lag_21  volume_lag_21    open_lag_21    high_lag_21     low_lag_21  quote_asset_volume_lag_21  number_of_trades_lag_21  taker_buy_base_asset_volume_lag_21  taker_buy_quote_asset_volume_lag_21   close_lag_22  volume_lag_22    open_lag_22    high_lag_22     low_lag_22  quote_asset_volume_lag_22  number_of_trades_lag_22  taker_buy_base_asset_volume_lag_22  taker_buy_quote_asset_volume_lag_22   close_lag_23  volume_lag_23    open_lag_23    high_lag_23     low_lag_23  quote_asset_volume_lag_23  number_of_trades_lag_23  taker_buy_base_asset_volume_lag_23  taker_buy_quote_asset_volume_lag_23   close_lag_24  volume_lag_24    open_lag_24    high_lag_24     low_lag_24  quote_asset_volume_lag_24  number_of_trades_lag_24  taker_buy_base_asset_volume_lag_24  taker_buy_quote_asset_volume_lag_24  \\\n",
      "0 0.143268277346 0.337662337662 0.325581395349 0.348111658456             0.143254610079           0.148379970545                      0.142857142857                       0.142857142857 0.343499197432 0.143033939027 0.337662337662 0.325581395349 0.348111658456             0.143028061825           0.148379970545                      0.142962244736                       0.142958829774 0.343499197432 0.174717980921 0.345779220779 0.333887043189 0.348111658456             0.173690823492           0.161266568483                      0.144583816580                       0.144527713636 0.351524879615 0.178349807901 0.353896103896 0.342192691030 0.356321839080             0.177250860373           0.164948453608                      0.142857142857                       0.142857142857 0.359550561798 0.166565342447 0.353896103896 0.342192691030 0.356321839080             0.165831466596           0.185198821797                      0.166344910333                       0.165635725369   \n",
      "1 0.147102295613 0.271474506201 0.266881028939 0.274193548387             0.146963595129           0.166017838664                      0.148537360243                       0.148353918653 0.273351648352 0.147066015948 0.271474506201 0.271474506201 0.276497695853             0.146930008807           0.152959148475                      0.144180688656                       0.144137869476 0.273351648352 0.152765810527 0.278364722095 0.273771244832 0.274193548387             0.152443853363           0.161582811807                      0.155644119945                       0.155231198822 0.280219780220 0.150050018429 0.292145153881 0.289848415250 0.285714285714             0.149835963625           0.158872517617                      0.143975506617                       0.143943159157 0.293956043956 0.161739844911 0.289848415250 0.287551676619 0.292626728111             0.161180619911           0.167003400187                      0.153301962919                       0.152995069829   \n",
      "2 0.145422629992 0.254464285714 0.254464285714 0.265625000000             0.145291100957           0.157584683358                      0.147893951427                       0.147641066317 0.263736263736 0.147514064149 0.254464285714 0.254464285714 0.265625000000             0.147275310131           0.157584683358                      0.152000054063                       0.151541012177 0.263736263736 0.170503240384 0.254464285714 0.254464285714 0.254464285714             0.169084759029           0.175994108984                      0.150344777064                       0.149968842348 0.263736263736 0.190741483626 0.265625000000 0.265625000000 0.265625000000             0.188291489349           0.179675994109                      0.142857142857                       0.142857142857 0.285714285714 0.143031284767 0.265625000000 0.276785714286 0.276785714286             0.143022733316           0.150220913108                      0.143024889856                       0.143016954946   \n",
      "\n",
      "    close_lag_25  volume_lag_25    open_lag_25    high_lag_25     low_lag_25  quote_asset_volume_lag_25  number_of_trades_lag_25  taker_buy_base_asset_volume_lag_25  taker_buy_quote_asset_volume_lag_25   close_lag_26  volume_lag_26    open_lag_26    high_lag_26     low_lag_26  quote_asset_volume_lag_26  number_of_trades_lag_26  taker_buy_base_asset_volume_lag_26  taker_buy_quote_asset_volume_lag_26   close_lag_27  volume_lag_27    open_lag_27    high_lag_27     low_lag_27  quote_asset_volume_lag_27  number_of_trades_lag_27  taker_buy_base_asset_volume_lag_27  taker_buy_quote_asset_volume_lag_27   close_lag_28  volume_lag_28    open_lag_28    high_lag_28     low_lag_28  quote_asset_volume_lag_28  number_of_trades_lag_28  taker_buy_base_asset_volume_lag_28  taker_buy_quote_asset_volume_lag_28   close_lag_29  volume_lag_29    open_lag_29    high_lag_29     low_lag_29  quote_asset_volume_lag_29  number_of_trades_lag_29  taker_buy_base_asset_volume_lag_29  \\\n",
      "0 0.359550561798 0.288313689778 0.337662337662 0.342192691030 0.339901477833             0.283687414876           0.277245949926                      0.240344974395                       0.237284611550 0.335473515249 0.174773021238 0.305194805195 0.317275747508 0.315270935961             0.173620421776           0.203608247423                      0.191092232070                       0.189405616342 0.319422150883 0.154393926885 0.313311688312 0.300664451827 0.323481116585             0.153970673224           0.168630338733                      0.164731679908                       0.163945528781 0.311396468700 0.270786684580 0.288961038961 0.292358803987 0.290640394089             0.265736285114           0.255154639175                      0.312246337450                       0.305816488738 0.295345104334 0.197597239978 0.272727272727 0.275747508306 0.274220032841             0.195311147936           0.227540500736                      0.244073588665   \n",
      "1 0.293956043956 0.173938018610 0.285254937988 0.292145153881 0.292626728111             0.173025185248           0.181786823042                      0.163363181778                       0.162765333794 0.289377289377 0.148255470693 0.276067983463 0.282958199357 0.276497695853             0.148085321201           0.165771448283                      0.147404398551                       0.147265600903 0.277930402930 0.145655428004 0.278364722095 0.278364722095 0.278801843318             0.145567739412           0.161336421426                      0.146878873014                       0.146752151231 0.282509157509 0.153248243697 0.273771244832 0.280661460726 0.267281105991             0.152914851771           0.183018774947                      0.155258896750                       0.154872534564 0.275641025641 0.150316069309 0.255397335783 0.269177767570 0.255760368664             0.150065852215           0.170945646282                      0.150916823094   \n",
      "2 0.274725274725 0.144155467543 0.265625000000 0.265625000000 0.276785714286             0.144090787630           0.146539027982                      0.142857142857                       0.142857142857 0.274725274725 0.218252646631 0.243303571429 0.265625000000 0.254464285714             0.214408234020           0.183357879234                      0.290529484304                       0.283157048219 0.263736263736 0.144031225946 0.254464285714 0.254464285714 0.265625000000             0.143971032304           0.153902798233                      0.142857142857                       0.142857142857 0.274725274725 0.207466439290 0.243303571429 0.265625000000 0.254464285714             0.204218049384           0.179675994109                      0.253121103856                       0.247734991106 0.252747252747 0.142857142857 0.243303571429 0.243303571429 0.254464285714             0.142857142857           0.142857142857                      0.142857142857   \n",
      "\n",
      "   taker_buy_quote_asset_volume_lag_29   close_lag_30  volume_lag_30    open_lag_30    high_lag_30     low_lag_30  quote_asset_volume_lag_30  number_of_trades_lag_30  taker_buy_base_asset_volume_lag_30  taker_buy_quote_asset_volume_lag_30          SMA_5         SMA_10         SMA_15         SMA_20         SMA_25         SMA_30          EMA_5         EMA_10         EMA_15         EMA_20         EMA_25         EMA_30          WMA_5         WMA_10         WMA_15         WMA_20         WMA_25         WMA_30          RSI_5         RSI_10         RSI_15         RSI_20         RSI_25         RSI_30  Stochastic_K_5  Stochastic_D_5  Stochastic_K_10  Stochastic_D_10  Stochastic_K_15  Stochastic_D_15  Stochastic_K_20  Stochastic_D_20  Stochastic_K_25  Stochastic_D_25  Stochastic_K_30  Stochastic_D_30     MACD_15_25      Signal_10  Histogram_15_25_10    WilliamsR_5   WilliamsR_10   WilliamsR_15   WilliamsR_20   WilliamsR_25   WilliamsR_30          ATR_5         ATR_10         ATR_15  \\\n",
      "0                       0.239931181969 0.287319422151 0.857142857143 0.272727272727 0.300664451827 0.274220032841             0.857142857143           0.553387334315                      0.857142857143                       0.857142857143 0.250673854447 0.262755102041 0.277758561052 0.283509700176 0.290739411342 0.288913030358 0.246349206349 0.251657457851 0.263082740659 0.269714423843 0.274211500833 0.273907067823 0.259577795152 0.275292158737 0.297628548626 0.304766314954 0.313317228006 0.307651049907 0.428650793651 0.252857142857 0.332280978690 0.300724637681 0.294243070362 0.386777719938  0.142857142857  0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857 0.288209025146 0.277187853729      0.289156265092 0.142857142857 0.142857142857 0.142857142857 0.142857142857 0.142857142857 0.142857142857 0.333996243509 0.378410956750 0.374084200832   \n",
      "1                       0.150650214131 0.259615384615 0.152730394663 0.271474506201 0.273771244832 0.264976958525             0.152408185240           0.166510619425                      0.148478157520                       0.148300506837 0.194514886823 0.202080023111 0.205819001226 0.207838814693 0.214687711578 0.219190252570 0.189780127208 0.191444282209 0.197034961823 0.200255915567 0.204267547613 0.206563817140 0.201512275570 0.212789624814 0.222117973403 0.223720580416 0.232881133356 0.238738042586 0.285835351090 0.194341372913 0.301587301587 0.207655516096 0.169230769231 0.316865417376  0.142857142857  0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857   0.142857142857 0.336281976467 0.364922380552      0.336812482784 0.142857142857 0.142857142857 0.142857142857 0.142857142857 0.142857142857 0.142857142857 0.226304323746 0.221480982557 0.207936583523   \n",
      "2                       0.142857142857 0.252747252747 0.152378326335 0.243303571429 0.243303571429 0.254464285714             0.151876376217           0.164948453608                      0.151386847812                       0.150946205884 0.184065934066 0.185430463576 0.190530058177 0.189522150324 0.195691932534 0.200598802395 0.180809356827 0.177374564051 0.180502468450 0.182481408041 0.186973471597 0.189321765748 0.186836518047 0.197280928381 0.210454220599 0.206616001492 0.213190267165 0.214370882397 0.142857142857 0.142857142857 0.429000000000 0.274325162645 0.291531997414 0.351940776311  0.380952380952  0.380952380952   0.261904761905   0.261904761905   0.261904761905   0.261904761905   0.232142857143   0.232142857143   0.222222222222   0.222222222222   0.222222222222   0.222222222222 0.350988119994 0.470465881014      0.351985049727 0.380952380952 0.261904761905 0.261904761905 0.232142857143 0.222222222222 0.222222222222 0.269002695418 0.326071862286 0.341753788468   \n",
      "\n",
      "          ATR_20         ATR_25         ATR_30    BB_Upper_20    BB_Lower_20            OBV    VolumeROC_5   VolumeROC_10   VolumeROC_15   VolumeROC_20   VolumeROC_25   VolumeROC_30    VolumeEMA_5   VolumeEMA_10   VolumeEMA_15   VolumeEMA_20   VolumeEMA_25   VolumeEMA_30           Doji         Hammer  BullishEngulfing  BearishEngulfing    MorningStar    EveningStar   PiercingLine  DarkCloudCover  ThreeWhiteSoldiers  ThreeBlackCrows  RollingMedian_5  RollingMedian_10  RollingMedian_15  RollingMedian_20  RollingMedian_25  RollingMedian_30  RollingStdDev_5  RollingStdDev_10  RollingStdDev_15  RollingStdDev_20  RollingStdDev_25  RollingStdDev_30  RollingKurtosis_5  RollingKurtosis_10  RollingKurtosis_15  RollingKurtosis_20  RollingKurtosis_25  RollingKurtosis_30        Plus_DI       Minus_DI            ADX  tr_smoothed_lag_1  plus_dm_smoothed_lag_1  minus_dm_smoothed_lag_1  Volume_RollingMean  Volume_RollingStdDev    VolumeSpike    VolumeRatio         ATR_14  PotentialLiquidityGap  \\\n",
      "0 0.359667233342 0.337212662231 0.350545191326 0.304054634699 0.256775923381 0.325847272904 0.143472861550 0.143204346907 0.143298598928 0.229165113981 0.143242381446 0.142902412093 0.230124181640 0.287685048567 0.344664639424 0.410055304005 0.359188165868 0.430010489354 0.857142857143 0.142857142857    0.142857142857    0.142857142857 0.142857142857 0.142857142857 0.142857142857  0.142857142857      0.142857142857   0.142857142857   0.252100840336    0.253401360544    0.280550774527    0.299651567944    0.296564195298    0.287522603978   0.256805532975    0.349828705691    0.462581868641    0.382306457328    0.380942299140    0.359119066641     0.301038062284      0.303180752559      0.160575118537      0.166661004601      0.213781932917      0.222520734999 0.142857142857 0.142857142857 0.142857142857     0.238095238095          0.142857142857           0.142857142857      0.475517512138        0.275299106770 0.142857142857 0.799465928427 0.374334038839                    NaN   \n",
      "1 0.208727247644 0.201709316807 0.205727896990 0.210017327189 0.214590945467 0.370592702417 0.153899987188 0.144671031176 0.150121221942 0.177854441728 0.144086593673 0.149684736088 0.166992970375 0.169160281105 0.171042761227 0.175474843241 0.167879097932 0.174116985759 0.142857142857 0.142857142857    0.142857142857    0.142857142857 0.142857142857 0.142857142857 0.142857142857  0.142857142857      0.142857142857   0.142857142857   0.192199248120    0.193447362081    0.216494845361    0.219387755102    0.203479177649    0.216081790550   0.215554183569    0.299584515006    0.337893700814    0.298891854560    0.347260408528    0.352067737209     0.416621658115      0.218020166693      0.166634679369      0.212579030205      0.236720082818      0.206381721445 0.142857142857 0.428571428571 0.857142857143     0.220077220077          0.142857142857           0.197802197802      0.182000481181        0.168639025374 0.142857142857 0.195594249303 0.212712631997                    NaN   \n",
      "2 0.343114493993 0.336036051226 0.342204949511 0.192934521425 0.201146477552 0.264363216978 0.145010179416 0.146083929223 0.144097398199 0.144353943282 0.163915337328 0.145154274227 0.149566517018 0.174339044391 0.179197239477 0.213750918509 0.233325014693 0.280466641207 0.142857142857 0.142857142857    0.142857142857    0.142857142857 0.142857142857 0.142857142857            NaN             NaN      0.142857142857   0.142857142857   0.177419354839    0.172619047619    0.203389830508    0.186335403727    0.193877551020    0.200772200772   0.142857142857    0.331525194213    0.398668364144    0.366624545130    0.391423742463    0.358854291674     0.428571428571      0.209821428571      0.170847658104      0.174731254962      0.200889415023      0.209604133051 0.142857142857 0.142857142857 0.142857142857     0.238095238095          0.142857142857           0.142857142857      0.227317671859        0.176909196353 0.142857142857 0.797475643577 0.335231742468                    NaN   \n",
      "\n",
      "   TakerSellQuoteVolume  TakerBuySellRatio  NumTradesMomentum_5  NumTradesMomentum_10  NumTradesMomentum_15  NumTradesMomentum_20  NumTradesMomentum_25  NumTradesMomentum_30  LaggedMaxDrawdown  PriceChangeRate_5  PriceChangeRate_10  PriceChangeRate_15  PriceChangeRate_20  PriceChangeRate_25  PriceChangeRate_30  PriceAccel_ROC_Diff_5_10  PriceAccel_ROC_Diff_5_15  PriceAccel_ROC_Diff_5_20  PriceAccel_ROC_Diff_5_25  PriceAccel_ROC_Diff_5_30  PriceAccel_ROC_Diff_10_15  PriceAccel_ROC_Diff_10_20  PriceAccel_ROC_Diff_10_25  PriceAccel_ROC_Diff_10_30  PriceAccel_ROC_Diff_15_20  PriceAccel_ROC_Diff_15_25  PriceAccel_ROC_Diff_15_30  PriceAccel_ROC_Diff_20_25  PriceAccel_ROC_Diff_20_30  PriceAccel_ROC_Diff_25_30  PriceMomentum_5  PriceMomentum_10  PriceMomentum_15  PriceMomentum_20  PriceMomentum_25  PriceMomentum_30  PriceAccel_Momentum_Diff_5_10  PriceAccel_Momentum_Diff_5_15  PriceAccel_Momentum_Diff_5_20  PriceAccel_Momentum_Diff_5_25  PriceAccel_Momentum_Diff_5_30  \\\n",
      "0        0.237199579744     0.143609965906       0.488732118362        0.431946006749        0.488732118362        0.527173913043        0.467446688054        0.315304722712     0.429876063410     0.433171128946      0.291145690919      0.339393000257      0.296834808978      0.312044476502      0.419883216293            0.277810668893            0.330104100799            0.311283493856            0.283948387816            0.433712731550             0.477402595900             0.423156751338             0.398616430538             0.513968976838             0.388832856429             0.364878489556             0.517000530801             0.411524287907             0.594238671703             0.658580438487   0.445887445887    0.307692307692    0.352941176471    0.311688311688    0.327188940092    0.430875576037                 0.294372294372                 0.344322344322                 0.324929971989                 0.298701298701                 0.442396313364   \n",
      "1        0.155950594015     0.147062692824       0.455806990269        0.425121204009        0.479111667529        0.465904394545        0.471279651101        0.459951089495     0.466291198013     0.421976282361      0.327552697056      0.371666153496      0.331879878032      0.330846518129      0.388475030165            0.343224030578            0.383967232988            0.337969742733            0.320873331960            0.405706929858             0.489066692675             0.403615452206             0.377013291773             0.449332728362             0.368486644457             0.332956505320             0.426049724885             0.402683885790             0.488740046585             0.563723688228   0.438867438867    0.346193952033    0.387873754153    0.351190476190    0.350230414747    0.404865876482                 0.361647361647                 0.398331595412                 0.354651162791                 0.340029761905                 0.419354838710   \n",
      "2        0.147911178153     0.155832984097       0.485560261841        0.486847430190        0.500000000000        0.481409001957        0.494270435447        0.505985634477     0.452679460286     0.487684729064      0.368206477251      0.438537264944      0.339783201160      0.342730877952      0.402945071138            0.320467264778            0.451453845884            0.349783608600            0.319430622376            0.381840234599             0.588689629791             0.481118680417             0.441069415401             0.475393531994             0.351332454909             0.370354463855             0.419243177852             0.454415704267             0.537559300066             0.555817351096   0.500000000000    0.380952380952    0.445887445887    0.352941176471    0.357142857143    0.421602787456                 0.337662337662                 0.460317460317                 0.359307359307                 0.331932773109                 0.392857142857   \n",
      "\n",
      "   PriceAccel_Momentum_Diff_10_15  PriceAccel_Momentum_Diff_10_20  PriceAccel_Momentum_Diff_10_25  PriceAccel_Momentum_Diff_10_30  PriceAccel_Momentum_Diff_15_20  PriceAccel_Momentum_Diff_15_25  PriceAccel_Momentum_Diff_15_30  PriceAccel_Momentum_Diff_20_25  PriceAccel_Momentum_Diff_20_30  PriceAccel_Momentum_Diff_25_30            VPT         MFI_14  future_max_increase_capped  target  \n",
      "0                  0.489177489177                  0.435897435897                  0.408963585434                  0.519480519481                  0.402597402597                  0.380952380952                  0.521008403361                  0.424242424242                  0.600732600733                  0.662337662338 0.325847272904 0.252767271590              0.001203369434       0  \n",
      "1                  0.503217503218                  0.419186652763                  0.392026578073                  0.462797619048                  0.387387387387                  0.351407716371                  0.437707641196                  0.419562419562                  0.502606882169                  0.574002574003 0.396252516282 0.366699087507              0.000000000000       0  \n",
      "2                  0.597402597403                  0.486772486772                  0.445887445887                  0.478991596639                  0.370129870130                  0.380952380952                  0.424242424242                  0.467532467532                  0.539682539683                  0.564935064935 0.238034309813 0.207467583255              0.001543209877       0  \n",
      "Muestra test normalizado:\n",
      "            open           high            low          close         volume  quote_asset_volume  number_of_trades  taker_buy_base_asset_volume  taker_buy_quote_asset_volume symbol    close_lag_1   volume_lag_1     open_lag_1     high_lag_1      low_lag_1  quote_asset_volume_lag_1  number_of_trades_lag_1  taker_buy_base_asset_volume_lag_1  taker_buy_quote_asset_volume_lag_1    close_lag_2   volume_lag_2     open_lag_2     high_lag_2      low_lag_2  quote_asset_volume_lag_2  number_of_trades_lag_2  taker_buy_base_asset_volume_lag_2  taker_buy_quote_asset_volume_lag_2    close_lag_3   volume_lag_3     open_lag_3     high_lag_3      low_lag_3  quote_asset_volume_lag_3  number_of_trades_lag_3  taker_buy_base_asset_volume_lag_3  taker_buy_quote_asset_volume_lag_3    close_lag_4   volume_lag_4     open_lag_4     high_lag_4      low_lag_4  quote_asset_volume_lag_4  number_of_trades_lag_4  taker_buy_base_asset_volume_lag_4  taker_buy_quote_asset_volume_lag_4    close_lag_5   volume_lag_5  \\\n",
      "0 0.438496462887 0.435519528371 0.450147106898 0.442106376229 0.189554603975      0.187702732275    0.182515734115               0.168180218756                0.167212529788  SPELL 0.436819287300 0.159322951350 0.437968535530 0.424465733235 0.440339980386            0.158649763683          0.175618587809                     0.149815222388                      0.149540659441 0.435761869515 0.271644047655 0.450110864745 0.436572270765 0.436526097853            0.266417428779          0.250193982240                     0.215548872849                      0.212648981913 0.449508300730 0.185099195321 0.461725266603 0.450257921887 0.448512585812            0.183496037244          0.223467540305                     0.167399414189                      0.166479536188 0.461668605266 0.154743782059 0.461725266603 0.454995262659 0.473575242454            0.154320041551          0.163548581774                     0.161342709582                      0.160702906494 0.460611187480 0.219342668767   \n",
      "1 0.471042471042 0.488354037267 0.484126984127 0.490450725745 0.176605948958      0.175448725761    0.174261986182               0.172484587959                0.173519022055     IO 0.467532467532 0.153038886717 0.451737451737 0.461180124224 0.468253968254            0.152635857363          0.155070137483                     0.147291171077                      0.147422197461 0.456073338426 0.150263108782 0.455598455598 0.449534161491 0.464285714286            0.149963554106          0.150533882337                     0.144019442564                      0.144052330709 0.459893048128 0.145627629392 0.451737451737 0.457298136646 0.468253968254            0.145518935790          0.152976481262                     0.144476862339                      0.144524047753 0.444614209320 0.151189435865 0.444015444015 0.449534161491 0.460317460317            0.150848175832          0.156465908298                     0.144366438904                      0.144409084573 0.440794499618 0.159915356783   \n",
      "2 0.600446428571 0.622767857143 0.611607142857 0.604395604396 0.157561543177      0.157476316237    0.175994108984               0.151732538587                0.151701018520  ALICE 0.604395604396 0.157336788092 0.589285714286 0.600446428571 0.600446428571            0.157239635527          0.150220913108                     0.163222908090                      0.163117277797 0.604395604396 0.144302928061 0.600446428571 0.600446428571 0.611607142857            0.144293822227          0.150220913108                     0.145695646004                      0.145680923654 0.604395604396 0.170327876425 0.600446428571 0.600446428571 0.611607142857            0.170154860457          0.227540500736                     0.158371840833                      0.158291371375 0.604395604396 0.161127175165 0.578125000000 0.600446428571 0.589285714286            0.160964845506          0.164948453608                     0.177551580803                      0.177280447009 0.549450549451 0.142857142857   \n",
      "\n",
      "      open_lag_5     high_lag_5      low_lag_5  quote_asset_volume_lag_5  number_of_trades_lag_5  taker_buy_base_asset_volume_lag_5  taker_buy_quote_asset_volume_lag_5    close_lag_6   volume_lag_6     open_lag_6     high_lag_6      low_lag_6  quote_asset_volume_lag_6  number_of_trades_lag_6  taker_buy_base_asset_volume_lag_6  taker_buy_quote_asset_volume_lag_6    close_lag_7   volume_lag_7     open_lag_7     high_lag_7      low_lag_7  quote_asset_volume_lag_7  number_of_trades_lag_7  taker_buy_base_asset_volume_lag_7  taker_buy_quote_asset_volume_lag_7    close_lag_8   volume_lag_8     open_lag_8     high_lag_8      low_lag_8  quote_asset_volume_lag_8  number_of_trades_lag_8  taker_buy_base_asset_volume_lag_8  taker_buy_quote_asset_volume_lag_8    close_lag_9   volume_lag_9     open_lag_9     high_lag_9      low_lag_9  quote_asset_volume_lag_9  number_of_trades_lag_9  taker_buy_base_asset_volume_lag_9  taker_buy_quote_asset_volume_lag_9   close_lag_10  volume_lag_10    open_lag_10  \\\n",
      "0 0.460141484532 0.453416149068 0.466492317751            0.216537452831          0.195016811794                     0.151109955447                      0.150820593109 0.457438934123 0.170810109618 0.475979305248 0.462364459417 0.468671679198            0.169821450741          0.174756444521                     0.152089589595                      0.151760461757 0.469599238659 0.155711366259 0.479146869391 0.466049057796 0.481203007519            0.155270450054          0.175187516165                     0.166425974936                      0.165646770266 0.481759543196 0.154918062493 0.482842360891 0.471839140962 0.493734335840            0.154522672213          0.162255366842                     0.155410000224                      0.155013196289 0.481759543196 0.167842438937 0.497624326893 0.483945678492 0.494279176201            0.167039733094          0.182946805759                     0.146470669369                      0.146357547945 0.500264354446 0.159735715532 0.509766656108   \n",
      "1 0.413127413127 0.434006211180 0.428571428571            0.159177374885          0.160304278038                     0.163076914526                      0.163584786097 0.414056531704 0.166121100456 0.416988416988 0.414596273292 0.420634920635            0.165030646082          0.171121501849                     0.161789240795                      0.162193109138 0.417876241406 0.150152108037 0.416988416988 0.410714285714 0.424603174603            0.149807578439          0.151929653151                     0.143814352746                      0.143834733296 0.421695951108 0.151112485156 0.424710424710 0.418478260870 0.428571428571            0.150731524670          0.151929653151                     0.147925981757                      0.148037549522 0.425515660810 0.153836298227 0.428571428571 0.426242236025 0.432539682540            0.153344309561          0.155070137483                     0.144383436668                      0.144420783514 0.429335370512 0.145348276726 0.432432432432   \n",
      "2 0.544642857143 0.544642857143 0.555803571429            0.142857142857          0.142857142857                     0.142857142857                      0.142857142857 0.549450549451 0.170110657306 0.555803571429 0.555803571429 0.555803571429            0.169772699219          0.253313696613                     0.142857142857                      0.142857142857 0.571428571429 0.143630597715 0.566964285714 0.566964285714 0.578125000000            0.143622360289          0.146539027982                     0.144375663065                      0.144361171058 0.571428571429 0.162328346995 0.566964285714 0.566964285714 0.578125000000            0.162120975337          0.205449189985                     0.142857142857                      0.142857142857 0.571428571429 0.162489351884 0.566964285714 0.566964285714 0.578125000000            0.162280265496          0.216494845361                     0.142857142857                      0.142857142857 0.571428571429 0.142857142857 0.566964285714   \n",
      "\n",
      "     high_lag_10     low_lag_10  quote_asset_volume_lag_10  number_of_trades_lag_10  taker_buy_base_asset_volume_lag_10  taker_buy_quote_asset_volume_lag_10   close_lag_11  volume_lag_11    open_lag_11    high_lag_11     low_lag_11  quote_asset_volume_lag_11  number_of_trades_lag_11  taker_buy_base_asset_volume_lag_11  taker_buy_quote_asset_volume_lag_11   close_lag_12  volume_lag_12    open_lag_12    high_lag_12     low_lag_12  quote_asset_volume_lag_12  number_of_trades_lag_12  taker_buy_base_asset_volume_lag_12  taker_buy_quote_asset_volume_lag_12   close_lag_13  volume_lag_13    open_lag_13    high_lag_13     low_lag_13  quote_asset_volume_lag_13  number_of_trades_lag_13  taker_buy_base_asset_volume_lag_13  taker_buy_quote_asset_volume_lag_13   close_lag_14  volume_lag_14    open_lag_14    high_lag_14     low_lag_14  quote_asset_volume_lag_14  number_of_trades_lag_14  taker_buy_base_asset_volume_lag_14  taker_buy_quote_asset_volume_lag_14   close_lag_15  volume_lag_15  \\\n",
      "0 0.499210443205 0.513348588863             0.159248890505           0.179067160962                      0.170725993163                       0.169956423628 0.507137570054 0.254301453361 0.510822510823 0.516054321508 0.508445025607             0.251198869541           0.255366841969                      0.427690521580                       0.420170492174 0.512953367876 0.206123330472 0.491817125963 0.508685124750 0.500272420181             0.204298964314           0.213983964135                      0.312782245833                       0.308088366984 0.489161467696 0.163972287979 0.485481997677 0.478155595326 0.491010134031             0.163287879955           0.179067160962                      0.176390906060                       0.175342886937 0.485989214339 0.307570113387 0.456973920389 0.482892936099 0.459409393048             0.301860607004           0.320027588585                      0.567828949801                       0.553801572411 0.457967643016 0.147842774935   \n",
      "1 0.426242236025 0.444444444444             0.145239292615           0.144950799079                      0.144869392836                       0.144919320780 0.433155080214 0.145746579101 0.432432432432 0.430124223602 0.448412698413             0.145621145444           0.145299741782                      0.146753011920                       0.146849948230 0.436974789916 0.149069506711 0.432432432432 0.430124223602 0.448412698413             0.148799862794           0.149138111522                      0.148213765080                       0.148347022428 0.433155080214 0.145280674947 0.428571428571 0.430124223602 0.444444444444             0.145174814780           0.149835996929                      0.145550730244                       0.145617085279 0.425515660810 0.149122017325 0.413127413127 0.418478260870 0.424603174603             0.148835631721           0.153674366669                      0.147597254913                       0.147703354714 0.414056531704 0.145885152070   \n",
      "2 0.566964285714 0.578125000000             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857 0.571428571429 0.142857142857 0.566964285714 0.566964285714 0.578125000000             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857 0.571428571429 0.142857142857 0.566964285714 0.566964285714 0.578125000000             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857 0.571428571429 0.151477982108 0.566964285714 0.566964285714 0.578125000000             0.151386168694           0.153902798233                      0.159782395154                       0.159620868901 0.549450549451 0.142857142857 0.544642857143 0.544642857143 0.555803571429             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857 0.549450549451 0.170392746833   \n",
      "\n",
      "     open_lag_15    high_lag_15     low_lag_15  quote_asset_volume_lag_15  number_of_trades_lag_15  taker_buy_base_asset_volume_lag_15  taker_buy_quote_asset_volume_lag_15   close_lag_16  volume_lag_16    open_lag_16    high_lag_16     low_lag_16  quote_asset_volume_lag_16  number_of_trades_lag_16  taker_buy_base_asset_volume_lag_16  taker_buy_quote_asset_volume_lag_16   close_lag_17  volume_lag_17    open_lag_17    high_lag_17     low_lag_17  quote_asset_volume_lag_17  number_of_trades_lag_17  taker_buy_base_asset_volume_lag_17  taker_buy_quote_asset_volume_lag_17   close_lag_18  volume_lag_18    open_lag_18    high_lag_18     low_lag_18  quote_asset_volume_lag_18  number_of_trades_lag_18  taker_buy_base_asset_volume_lag_18  taker_buy_quote_asset_volume_lag_18   close_lag_19  volume_lag_19    open_lag_19    high_lag_19     low_lag_19  quote_asset_volume_lag_19  number_of_trades_lag_19  taker_buy_base_asset_volume_lag_19  taker_buy_quote_asset_volume_lag_19   close_lag_20  \\\n",
      "0 0.460669411889 0.449731550690 0.469761359922             0.147662780193           0.157944650401                      0.149348463599                       0.149117128356 0.461139896373 0.168589494359 0.472283813747 0.459732603432 0.470306200283             0.167673612202           0.178205017674                      0.188149630984                       0.186596688425 0.474357618695 0.150869422547 0.474395523176 0.462364459417 0.485016890051             0.150596275916           0.155358220536                      0.148243834169                       0.148065641817 0.473300200909 0.153233952039 0.466476612818 0.463417201811 0.479023646072             0.152875310863           0.160531080266                      0.170801526633                       0.169861807122 0.466426985302 0.154273090226 0.463309048675 0.460785345826 0.475209763539             0.153872390698           0.172170014656                      0.166423461578                       0.165623700377 0.468541820873   \n",
      "1 0.420849420849 0.422360248447 0.428571428571             0.145747126333           0.159606392630                      0.144164078654                       0.144194320337 0.421695951108 0.145134414025 0.440154440154 0.434006211180 0.432539682540             0.145032836920           0.155768022891                      0.143917331905                       0.143942766226 0.444614209320 0.145324572206 0.444015444015 0.441770186335 0.456349206349             0.145222577158           0.148091283411                      0.146044782063                       0.146130944384 0.448433919022 0.144856057945 0.437888198758 0.437205651491 0.456349206349             0.144773713012           0.146695512597                      0.145101064925                       0.145162364093 0.436293436293 0.143524215100 0.444015444015 0.445652173913 0.451845906902             0.143497177589           0.145997627190                      0.143638915963                       0.143660582616 0.440794499618   \n",
      "2 0.544642857143 0.566964285714 0.555803571429             0.170027908102           0.175994108984                      0.143049482086                       0.143047646495 0.571428571429 0.147705518413 0.578125000000 0.578125000000 0.578125000000             0.147654086991           0.153902798233                      0.142857142857                       0.142857142857 0.571428571429 0.221863087064 0.578125000000 0.578125000000 0.578125000000             0.221125863285           0.466863033873                      0.143442957763                       0.143438217803 0.593406593407 0.143250336960 0.589285714286 0.589285714286 0.589285714286             0.143247072548           0.157584683358                      0.143334592002                       0.143331422245 0.582417582418 0.145471715606 0.578125000000 0.578125000000 0.589285714286             0.145447662814           0.153902798233                      0.147990320978                       0.147948787283 0.571428571429   \n",
      "\n",
      "   volume_lag_20    open_lag_20    high_lag_20     low_lag_20  quote_asset_volume_lag_20  number_of_trades_lag_20  taker_buy_base_asset_volume_lag_20  taker_buy_quote_asset_volume_lag_20   close_lag_21  volume_lag_21    open_lag_21    high_lag_21     low_lag_21  quote_asset_volume_lag_21  number_of_trades_lag_21  taker_buy_base_asset_volume_lag_21  taker_buy_quote_asset_volume_lag_21   close_lag_22  volume_lag_22    open_lag_22    high_lag_22     low_lag_22  quote_asset_volume_lag_22  number_of_trades_lag_22  taker_buy_base_asset_volume_lag_22  taker_buy_quote_asset_volume_lag_22   close_lag_23  volume_lag_23    open_lag_23    high_lag_23     low_lag_23  quote_asset_volume_lag_23  number_of_trades_lag_23  taker_buy_base_asset_volume_lag_23  taker_buy_quote_asset_volume_lag_23   close_lag_24  volume_lag_24    open_lag_24    high_lag_24     low_lag_24  quote_asset_volume_lag_24  number_of_trades_lag_24  taker_buy_base_asset_volume_lag_24  taker_buy_quote_asset_volume_lag_24  \\\n",
      "0 0.152102462977 0.451166719459 0.454995262659 0.461588754495             0.151766751076           0.172170014656                      0.164073622851                       0.163326058466 0.450565718515 0.155373649851 0.448527082673 0.440256869144 0.456140350877             0.154893335729           0.162686438486                      0.160857012113                       0.160184420428 0.454266680766 0.150248292079 0.446943300602 0.440256869144 0.458319712324             0.149967271112           0.161393223554                      0.159355776654                       0.158741471522 0.445278629587 0.167357691908 0.433745116672 0.431308558796 0.437615778577             0.166351330990           0.180791447539                      0.204327333821                       0.201882222362 0.435233160622 0.184385659656 0.444303663816 0.431834929993 0.442519341833             0.182703197480           0.190275023709                      0.164748061071                       0.163901939992   \n",
      "1 0.148189520898 0.436293436293 0.437888198758 0.452380952381             0.147964187591           0.150184939633                      0.145302246498                       0.145365645162 0.436974789916 0.146249736605 0.428571428571 0.434006211180 0.440476190476             0.146102944118           0.149487054226                      0.146696776615                       0.146792592747 0.429335370512 0.147461680611 0.428571428571 0.426242236025 0.444444444444             0.147259274350           0.148440226115                      0.146046581096                       0.146124549675 0.425515660810 0.156575724485 0.428571428571 0.422360248447 0.428571428571             0.155944929988           0.156814851001                      0.144815421831                       0.144860411030 0.433155080214 0.142776339032 0.440154440154 0.434006211180 0.448412698413             0.142774093542           0.143903970968                      0.142870356448                       0.142870685104   \n",
      "2 0.142857142857 0.566964285714 0.566964285714 0.578125000000             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857 0.571428571429 0.153638665679 0.555803571429 0.566964285714 0.566964285714             0.153514415207           0.164948453608                      0.164024454833                       0.163803918897 0.560439560440 0.154602454590 0.555803571429 0.555803571429 0.566964285714             0.154460326560           0.164948453608                      0.153320636812                       0.153205582444 0.549450549451 0.154612332816 0.544642857143 0.544642857143 0.555803571429             0.154453032472           0.164948453608                      0.145905179799                       0.145867237619 0.560439560440 0.142857142857 0.555803571429 0.555803571429 0.566964285714             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857   \n",
      "\n",
      "    close_lag_25  volume_lag_25    open_lag_25    high_lag_25     low_lag_25  quote_asset_volume_lag_25  number_of_trades_lag_25  taker_buy_base_asset_volume_lag_25  taker_buy_quote_asset_volume_lag_25   close_lag_26  volume_lag_26    open_lag_26    high_lag_26     low_lag_26  quote_asset_volume_lag_26  number_of_trades_lag_26  taker_buy_base_asset_volume_lag_26  taker_buy_quote_asset_volume_lag_26   close_lag_27  volume_lag_27    open_lag_27    high_lag_27     low_lag_27  quote_asset_volume_lag_27  number_of_trades_lag_27  taker_buy_base_asset_volume_lag_27  taker_buy_quote_asset_volume_lag_27   close_lag_28  volume_lag_28    open_lag_28    high_lag_28     low_lag_28  quote_asset_volume_lag_28  number_of_trades_lag_28  taker_buy_base_asset_volume_lag_28  taker_buy_quote_asset_volume_lag_28   close_lag_29  volume_lag_29    open_lag_29    high_lag_29     low_lag_29  quote_asset_volume_lag_29  number_of_trades_lag_29  taker_buy_base_asset_volume_lag_29  \\\n",
      "0 0.440520249551 0.148991487045 0.441136099673 0.434466785977 0.450691947259             0.148750872056           0.154927148892                      0.151055201424                       0.150740212487 0.441577667336 0.160171992719 0.440080244958 0.428150331614 0.446333224365             0.159476601319           0.176480731098                      0.166006846039                       0.165106019193 0.441048958443 0.161063954472 0.434273044029 0.428676702811 0.442519341833             0.160325537400           0.170445728080                      0.152186229957                       0.151816393291 0.434704451729 0.159076059072 0.452750501531 0.439730497947 0.444698703280             0.158422014143           0.164410725063                      0.147681027957                       0.147501208704 0.453209262980 0.198453819170 0.462781121318 0.456574376250 0.459954233410             0.196421426574           0.199758599879                      0.197444433765   \n",
      "1 0.440794499618 0.149607881225 0.432432432432 0.441770186335 0.448412698413             0.149323441315           0.150533882337                      0.147140176335                       0.147256576363 0.433155080214 0.159852382399 0.444015444015 0.437888198758 0.448412698413             0.159126763928           0.155419080187                      0.143488882773                       0.143505975296 0.448433919022 0.163660651253 0.436293436293 0.445652173913 0.452380952381             0.162803899556           0.163793705074                      0.168748956948                       0.169462984737 0.440794499618 0.146312058127 0.451737451737 0.445652173913 0.456349206349             0.146163896543           0.148440226115                      0.144252758598                       0.144290080673 0.452253628724 0.150782989540 0.451737451737 0.449534161491 0.456349206349             0.150455565914           0.155070137483                      0.147571075874   \n",
      "2 0.560439560440 0.146856602060 0.555803571429 0.555803571429 0.566964285714             0.146808205281           0.153902798233                      0.150709261902                       0.150622921660 0.549450549451 0.148045044219 0.555803571429 0.555803571429 0.555803571429             0.147980289804           0.164948453608                      0.150367569863                       0.150284986795 0.549450549451 0.155688652025 0.544642857143 0.544642857143 0.555803571429             0.155514765952           0.150220913108                      0.142857142857                       0.142857142857 0.549450549451 0.153813622289 0.555803571429 0.555803571429 0.555803571429             0.153666266649           0.157584683358                      0.144374263508                       0.144357581570 0.549450549451 0.142857142857 0.544642857143 0.544642857143 0.555803571429             0.142857142857           0.142857142857                      0.142857142857   \n",
      "\n",
      "   taker_buy_quote_asset_volume_lag_29   close_lag_30  volume_lag_30    open_lag_30    high_lag_30     low_lag_30  quote_asset_volume_lag_30  number_of_trades_lag_30  taker_buy_base_asset_volume_lag_30  taker_buy_quote_asset_volume_lag_30          SMA_5         SMA_10         SMA_15         SMA_20         SMA_25         SMA_30          EMA_5         EMA_10         EMA_15         EMA_20         EMA_25         EMA_30          WMA_5         WMA_10         WMA_15         WMA_20         WMA_25         WMA_30          RSI_5         RSI_10         RSI_15         RSI_20         RSI_25         RSI_30  Stochastic_K_5  Stochastic_D_5  Stochastic_K_10  Stochastic_D_10  Stochastic_K_15  Stochastic_D_15  Stochastic_K_20  Stochastic_D_20  Stochastic_K_25  Stochastic_D_25  Stochastic_K_30  Stochastic_D_30     MACD_15_25      Signal_10  Histogram_15_25_10    WilliamsR_5   WilliamsR_10   WilliamsR_15   WilliamsR_20   WilliamsR_25   WilliamsR_30          ATR_5         ATR_10         ATR_15  \\\n",
      "0                       0.195538242936 0.462197314159 0.168171986978 0.443247809101 0.451837035477 0.454505829792             0.167225493893           0.178636089318                      0.216669290411                       0.214003421581 0.448931542481 0.464018223864 0.481381364815 0.485190709567 0.484307340301 0.481722003329 0.441370626546 0.453890383658 0.466355154072 0.473549078650 0.478763031064 0.484178126212 0.451521310080 0.471170433072 0.494785190621 0.489989894533 0.480153216643 0.472802668398 0.303128371090 0.235107979383 0.474241373533 0.421736228689 0.491514729286 0.439606350210  0.412891986063  0.412891986063   0.304483837331   0.304483837331   0.254689754690   0.254689754690   0.254689754690   0.254689754690   0.254689754690   0.254689754690   0.254689754690   0.254689754690 0.393472838096 0.396777075384      0.394529407648 0.412891986063 0.304483837331 0.254689754690 0.254689754690 0.254689754690 0.254689754690 0.225830720509 0.235479522194 0.270771399461   \n",
      "1                       0.147703707241 0.444614209320 0.148335832050 0.436293436293 0.441770186335 0.448412698413             0.148101056134           0.154023309373                      0.146828726416                       0.146936778366 0.477343878728 0.462576793285 0.456460343610 0.453794642857 0.450748196792 0.450012045290 0.479357294911 0.469837815749 0.463645536613 0.461880961634 0.458846170761 0.455758766912 0.472359058566 0.449486763911 0.448438739453 0.449075001670 0.447014984367 0.447881181411 0.809638752053 0.719986263736 0.775910364146 0.646080897447 0.622890213819 0.588440073165  0.802197802198  0.802197802198   0.826086956522   0.826086956522   0.826086956522   0.826086956522   0.826086956522   0.826086956522   0.826086956522   0.826086956522   0.826086956522   0.826086956522 0.612356296949 0.601358680674      0.612955922872 0.802197802198 0.826086956522 0.826086956522 0.826086956522 0.826086956522 0.826086956522 0.344181922495 0.350840461534 0.315986378461   \n",
      "2                       0.142857142857 0.549450549451 0.142857142857 0.544642857143 0.544642857143 0.555803571429             0.142857142857           0.142857142857                      0.142857142857                       0.142857142857 0.607600732601 0.588694418165 0.583225597931 0.586485813838 0.587888640520 0.585970915312 0.605906823775 0.595922498823 0.592712624093 0.591397943804 0.591589521791 0.589249842046 0.607673642705 0.578670958452 0.575835781784 0.580601920925 0.580571490505 0.574412202667 0.856875267590 0.653628117914 0.699246031746 0.531418179119 0.570207570208 0.641530296329  0.500000000000  0.500000000000   0.653061224490   0.653061224490   0.653061224490   0.653061224490   0.653061224490   0.653061224490   0.653061224490   0.653061224490   0.653061224490   0.653061224490 0.562436395714 0.557540804287      0.563675694551 0.500000000000 0.653061224490 0.653061224490 0.653061224490 0.653061224490 0.653061224490 0.302425876011 0.342702818307 0.328258893760   \n",
      "\n",
      "          ATR_20         ATR_25         ATR_30    BB_Upper_20    BB_Lower_20            OBV    VolumeROC_5   VolumeROC_10   VolumeROC_15   VolumeROC_20   VolumeROC_25   VolumeROC_30    VolumeEMA_5   VolumeEMA_10   VolumeEMA_15   VolumeEMA_20   VolumeEMA_25   VolumeEMA_30           Doji         Hammer  BullishEngulfing  BearishEngulfing    MorningStar    EveningStar   PiercingLine  DarkCloudCover  ThreeWhiteSoldiers  ThreeBlackCrows  RollingMedian_5  RollingMedian_10  RollingMedian_15  RollingMedian_20  RollingMedian_25  RollingMedian_30  RollingStdDev_5  RollingStdDev_10  RollingStdDev_15  RollingStdDev_20  RollingStdDev_25  RollingStdDev_30  RollingKurtosis_5  RollingKurtosis_10  RollingKurtosis_15  RollingKurtosis_20  RollingKurtosis_25  RollingKurtosis_30        Plus_DI       Minus_DI            ADX  tr_smoothed_lag_1  plus_dm_smoothed_lag_1  minus_dm_smoothed_lag_1  Volume_RollingMean  Volume_RollingStdDev    VolumeSpike    VolumeRatio         ATR_14  PotentialLiquidityGap  \\\n",
      "0 0.281455165517 0.291429494412 0.298202802093 0.464852557552 0.498382077832 0.614922978950 0.147732184721 0.146438014012 0.200365548121 0.190780558545 0.185243322363 0.149300844500 0.249889078511 0.232308504364 0.289151932938 0.299816825075 0.275809634119 0.275616012072 0.142857142857 0.142857142857    0.142857142857    0.142857142857 0.142857142857 0.142857142857 0.142857142857  0.142857142857      0.142857142857   0.142857142857   0.437266123707    0.463907323321    0.480389826480    0.482804796397    0.481336201442    0.478081485301   0.227262001163    0.266816830701    0.322289550008    0.285267061333    0.253359061500    0.245416292604                NaN                 NaN                 NaN                 NaN      0.395100894801      0.377896958505 0.767857142857 0.142857142857 0.857142857143     0.213324791800          0.244897959184           0.142857142857      0.350029724776        0.325503647045 0.142857142857 0.438366605316 0.258003277633                    NaN   \n",
      "1 0.305129651454 0.292944491868 0.290074275807 0.448740878660 0.450545015326 0.356025512411 0.148012065476 0.169072319918 0.191410693731 0.156570718344 0.167184172855 0.149192807876 0.160801001347 0.172131866850 0.160430056545 0.151258121543 0.151857949262 0.159359746348 0.142857142857 0.142857142857    0.142857142857    0.142857142857 0.142857142857 0.142857142857 0.142857142857  0.142857142857      0.142857142857   0.142857142857   0.478991596639    0.464177160226    0.448979591837    0.448337825696    0.443130118289    0.448653582839   0.391917158484    0.416936076277    0.302082709668    0.268413252629    0.233899488233    0.205924510232     0.558214285714      0.276955960175      0.360006180391      0.395717160616      0.467453595340      0.490125964292 0.857142857143 0.142857142857 0.857142857143     0.309523809524          0.309523809524           0.142857142857      0.162664436484        0.164460478011 0.857142857143 0.491620524683 0.318320968071                    NaN   \n",
      "2 0.324126412356 0.309257928962 0.305316893249 0.557709735886 0.592817742834 0.264363216978 0.145287324358 0.143723788070 0.143467992570 0.143081388755 0.148727185586 0.144134368226 0.180771333103 0.196327007435 0.181599621155 0.230513460358 0.213143209101 0.204902581149 0.857142857143 0.142857142857    0.142857142857    0.142857142857 0.142857142857 0.142857142857            NaN             NaN      0.142857142857   0.142857142857   0.603686635945    0.595238095238    0.578692493947    0.571428571429    0.576530612245    0.580437580438   0.142857142857    0.427253327229    0.369822789683    0.331230928561    0.298651924160    0.271162937418     0.428571428571      0.195083209006      0.202173824915      0.207173139803      0.222058041470      0.246355865272 0.857142857143 0.142857142857 0.857142857143     0.238095238095          0.244897959184           0.142857142857      0.252603156091        0.208405786108 0.142857142857 0.834453297695 0.337585261389                    NaN   \n",
      "\n",
      "   TakerSellQuoteVolume  TakerBuySellRatio  NumTradesMomentum_5  NumTradesMomentum_10  NumTradesMomentum_15  NumTradesMomentum_20  NumTradesMomentum_25  NumTradesMomentum_30  LaggedMaxDrawdown  PriceChangeRate_5  PriceChangeRate_10  PriceChangeRate_15  PriceChangeRate_20  PriceChangeRate_25  PriceChangeRate_30  PriceAccel_ROC_Diff_5_10  PriceAccel_ROC_Diff_5_15  PriceAccel_ROC_Diff_5_20  PriceAccel_ROC_Diff_5_25  PriceAccel_ROC_Diff_5_30  PriceAccel_ROC_Diff_10_15  PriceAccel_ROC_Diff_10_20  PriceAccel_ROC_Diff_10_25  PriceAccel_ROC_Diff_10_30  PriceAccel_ROC_Diff_15_20  PriceAccel_ROC_Diff_15_25  PriceAccel_ROC_Diff_15_30  PriceAccel_ROC_Diff_20_25  PriceAccel_ROC_Diff_20_30  PriceAccel_ROC_Diff_25_30  PriceMomentum_5  PriceMomentum_10  PriceMomentum_15  PriceMomentum_20  PriceMomentum_25  PriceMomentum_30  PriceAccel_Momentum_Diff_5_10  PriceAccel_Momentum_Diff_5_15  PriceAccel_Momentum_Diff_5_20  PriceAccel_Momentum_Diff_5_25  PriceAccel_Momentum_Diff_5_30  \\\n",
      "0        0.197545518416     0.143138607986       0.488532652386        0.493265913709        0.534120425030        0.518871955562        0.509823182711        0.501829156759     0.544549711274     0.324216573351      0.335996024427      0.383156666103      0.371716406324      0.441627402046      0.412670994982            0.275830188230            0.447538136253            0.396848568554            0.442036277514            0.437254402971             0.471766437470             0.501301628414             0.507967854067             0.472865880174             0.346284266260             0.476804909306             0.404081356190             0.440225854453             0.457152835976             0.320834452972   0.333087874325    0.342409548676    0.389133378470    0.378684807256    0.450627072221    0.425876010782                 0.283996072656                 0.449645654606                 0.401828029790                 0.445200302343                 0.446302436212   \n",
      "1        0.165066759659     0.150615737224       0.481090143481        0.498316246352        0.488729779899        0.507247952453        0.500999382200        0.502682502683     0.787023273666     0.663975818764      0.651269115094      0.674050238220      0.629170890417      0.592856985846      0.597341715164            0.551129199490            0.581725448375            0.536587244821            0.548483703030            0.525249177820             0.563439732480             0.503325520629             0.514332785921             0.523801658528             0.432786998356             0.469135554576             0.479794310647             0.517297269681             0.518843022920             0.505374648907   0.669789227166    0.666666666667    0.673469387755    0.635467980296    0.610483042138    0.615763546798                 0.552693208431                 0.595238095238                 0.537414965986                 0.555418719212                 0.538540596095   \n",
      "2        0.163458035330     0.143907894819       0.508663842896        0.498988263861        0.500000000000        0.497064579256        0.498090145149        0.525937749401     0.669005102041     0.650187034061      0.588821606559      0.594445721476      0.537002449623      0.547218277157      0.580362241984            0.424121427090            0.507560547422            0.442717283917            0.453363044900            0.476484614765             0.552652989458             0.508844034723             0.508372948299             0.518243263078             0.421096393616             0.482134900630             0.487627344188             0.520688500041             0.563772293793             0.521487797797   0.662337662338    0.592592592593    0.597402597403    0.542016806723    0.553571428571    0.595818815331                 0.435064935065                 0.513227513228                 0.445887445887                 0.457983193277                 0.482142857143   \n",
      "\n",
      "   PriceAccel_Momentum_Diff_10_15  PriceAccel_Momentum_Diff_10_20  PriceAccel_Momentum_Diff_10_25  PriceAccel_Momentum_Diff_10_30  PriceAccel_Momentum_Diff_15_20  PriceAccel_Momentum_Diff_15_25  PriceAccel_Momentum_Diff_15_30  PriceAccel_Momentum_Diff_20_25  PriceAccel_Momentum_Diff_20_30  PriceAccel_Momentum_Diff_25_30            VPT         MFI_14  future_max_increase_capped  target  \n",
      "0                  0.474226804124                  0.500932487878                  0.510155721056                  0.470899470899                  0.351497299951                  0.475755315181                  0.407752200406                  0.441089837997                  0.456173069750                  0.325724104075 0.668022864585 0.438961519973              0.003388278388       0  \n",
      "1                  0.564402810304                  0.515873015873                  0.517006802721                  0.530788177340                  0.435597189696                  0.484126984127                  0.482993197279                  0.517564402810                  0.531746031746                  0.505854800937 0.309905434773 0.555566541266              0.000000000000       0  \n",
      "2                  0.564935064935                  0.513227513228                  0.510822510823                  0.521008403361                  0.435064935065                  0.486772486772                  0.489177489177                  0.532467532468                  0.566137566138                  0.532467532468 0.238034309813 0.497607964404              0.002919708029       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:07,060 - WARNING - [439320242.py:32] - Columnas completamente NaN en train normalizado: ['PotentialLiquidityGap']\n",
      "2025-04-06 18:44:07,067 - WARNING - [439320242.py:35] - Columnas completamente NaN en test normalizado: ['PotentialLiquidityGap']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Cols NaN train: ['PotentialLiquidityGap']\n",
      "WARN: Cols NaN test: ['PotentialLiquidityGap']\n"
     ]
    }
   ],
   "source": [
    "columna_simbolo = 'symbol'\n",
    "margen_normalizacion = 0.2\n",
    "\n",
    "logging.info(f\"Aplicando normalización con margen={margen_normalizacion}...\")\n",
    "print(f\"Aplicando normalización (margen={margen_normalizacion})...\")\n",
    "start_norm = time.time()\n",
    "\n",
    "df_train_normalizado, df_test_normalizado, escaladores = normalizar_datos_ml(\n",
    "    df_train.copy(),\n",
    "    df_test.copy(),\n",
    "    columna_simbolo,\n",
    "    margen=margen_normalizacion\n",
    ")\n",
    "\n",
    "end_norm = time.time()\n",
    "logging.info(f\"Normalización completada en {end_norm - start_norm:.2f} segundos.\")\n",
    "print(f\"Normalización: {end_norm - start_norm:.2f}s.\")\n",
    "\n",
    "logging.info(\"Guardando escaladores...\")\n",
    "nombre_base_escaladores = 'escaladores_gbr_final'\n",
    "guardar_escaladores_joblib_json(escaladores, nombre_base_escaladores)\n",
    "print(f\"Escaladores guardados: {nombre_base_escaladores}.[joblib/json]\")\n",
    "\n",
    "print(\"Muestra train normalizado:\")\n",
    "print(df_train_normalizado.head(3))\n",
    "print(\"Muestra test normalizado:\")\n",
    "print(df_test_normalizado.head(3))\n",
    "\n",
    "nan_cols_train = df_train_normalizado.isna().all()\n",
    "nan_cols_test = df_test_normalizado.isna().all()\n",
    "if nan_cols_train.any():\n",
    "    logging.warning(f\"Columnas completamente NaN en train normalizado: {nan_cols_train[nan_cols_train].index.tolist()}\")\n",
    "    print(f\"WARN: Cols NaN train: {nan_cols_train[nan_cols_train].index.tolist()}\")\n",
    "if nan_cols_test.any():\n",
    "     logging.warning(f\"Columnas completamente NaN en test normalizado: {nan_cols_test[nan_cols_test].index.tolist()}\")\n",
    "     print(f\"WARN: Cols NaN test: {nan_cols_test[nan_cols_test].index.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:07,170 - INFO - [4266143611.py:1] - Preparando conjuntos de datos finales para el modelo GBR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando datos finales X/Z...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:07,551 - INFO - [4266143611.py:15] - Dimensiones preparadas:\n",
      "2025-04-06 18:44:07,554 - INFO - [4266143611.py:16] -   X_train_main: (85932, 438)\n",
      "2025-04-06 18:44:07,653 - INFO - [4266143611.py:17] -   Z_train_main: (85932,)\n",
      "2025-04-06 18:44:07,657 - INFO - [4266143611.py:18] -   X_test_main: (21483, 438)\n",
      "2025-04-06 18:44:07,662 - INFO - [4266143611.py:19] -   Z_test_main: (21483,)\n",
      "2025-04-06 18:44:07,665 - INFO - [4266143611.py:28] - Dimensiones de X/Z verificadas correctamente.\n",
      "2025-04-06 18:44:07,669 - INFO - [4266143611.py:34] - Liberando memoria de DataFrames intermedios...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones datos:\n",
      "Train: X=(85932, 438) Z=(85932,)\n",
      "Test: X=(21483, 438) Z=(21483,)\n",
      "Consistencia dimensiones ok.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:08,425 - INFO - [4266143611.py:37] - DataFrames intermedios eliminados de memoria.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria intermedia liberada.\n",
      "NaNs X_train: 137251\n",
      "NaNs Z_train: 0\n",
      "NaNs X_test: 34334\n",
      "NaNs Z_test: 0\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Preparando conjuntos de datos finales para el modelo GBR...\")\n",
    "print(\"Preparando datos finales X/Z...\")\n",
    "\n",
    "objetivo_clasificacion = 'target'\n",
    "objetivo_regresion = 'future_max_increase_capped'\n",
    "\n",
    "cols_to_drop_for_X = [columna_simbolo, objetivo_clasificacion, objetivo_regresion]\n",
    "\n",
    "X_train_main = df_train_normalizado.drop(columns=[col for col in cols_to_drop_for_X if col in df_train_normalizado.columns], axis=1)\n",
    "Z_train_main = df_train[objetivo_regresion].copy()\n",
    "\n",
    "X_test_main = df_test_normalizado.drop(columns=[col for col in cols_to_drop_for_X if col in df_test_normalizado.columns], axis=1)\n",
    "Z_test_main = df_test[objetivo_regresion].copy()\n",
    "\n",
    "logging.info(f\"Dimensiones preparadas:\")\n",
    "logging.info(f\"  X_train_main: {X_train_main.shape}\")\n",
    "logging.info(f\"  Z_train_main: {Z_train_main.shape}\")\n",
    "logging.info(f\"  X_test_main: {X_test_main.shape}\")\n",
    "logging.info(f\"  Z_test_main: {Z_test_main.shape}\")\n",
    "print(\"Dimensiones datos:\")\n",
    "print(f\"Train: X={X_train_main.shape} Z={Z_train_main.shape}\")\n",
    "print(f\"Test: X={X_test_main.shape} Z={Z_test_main.shape}\")\n",
    "\n",
    "try:\n",
    "    assert X_train_main.shape[0] == Z_train_main.shape[0], \"Discrepancia en filas de entrenamiento\"\n",
    "    assert X_test_main.shape[0] == Z_test_main.shape[0], \"Discrepancia en filas de test\"\n",
    "    assert X_train_main.shape[1] == X_test_main.shape[1], \"Discrepancia en número de características entre train y test\"\n",
    "    logging.info(\"Dimensiones de X/Z verificadas correctamente.\")\n",
    "    print(\"Consistencia dimensiones ok.\")\n",
    "except AssertionError as e:\n",
    "    logging.error(f\"Error de aserción en dimensiones: {e}\")\n",
    "    print(f\"Error dimensiones: {e}\")\n",
    "\n",
    "logging.info(\"Liberando memoria de DataFrames intermedios...\")\n",
    "del df_train_normalizado, df_test_normalizado, df_train, df_test, escaladores\n",
    "gc.collect()\n",
    "logging.info(\"DataFrames intermedios eliminados de memoria.\")\n",
    "print(\"Memoria intermedia liberada.\")\n",
    "\n",
    "print(f\"NaNs X_train: {X_train_main.isna().sum().sum()}\")\n",
    "print(f\"NaNs Z_train: {Z_train_main.isna().sum()}\")\n",
    "print(f\"NaNs X_test: {X_test_main.isna().sum().sum()}\")\n",
    "print(f\"NaNs Z_test: {Z_test_main.isna().sum()}\")\n",
    "if Z_train_main.isna().any() or Z_test_main.isna().any():\n",
    "     print(\"WARN: NaNs en Z_train/Z_test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:09,941 - INFO - [564147948.py:12] - Hiperparámetros seleccionados para GradientBoostingRegressor: {'n_estimators': 3000, 'max_depth': 9, 'learning_rate': 0.005, 'subsample': 0.5, 'random_state': 42, 'min_samples_split': 2, 'min_samples_leaf': 1, 'loss': 'squared_error'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros GBR:\n",
      "  n_estimators: 3000\n",
      "  max_depth: 9\n",
      "  learning_rate: 0.005\n",
      "  subsample: 0.5\n",
      "  random_state: 42\n",
      "  min_samples_split: 2\n",
      "  min_samples_leaf: 1\n",
      "  loss: squared_error\n"
     ]
    }
   ],
   "source": [
    "best_gbr_params = {\n",
    "    'n_estimators': 3000,\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.005,\n",
    "    'subsample': 0.5,\n",
    "    'random_state': 42,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'loss': 'squared_error'\n",
    "}\n",
    "\n",
    "logging.info(f\"Hiperparámetros seleccionados para GradientBoostingRegressor: {best_gbr_params}\")\n",
    "print(\"Hiperparámetros GBR:\")\n",
    "for param, value in best_gbr_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:10,095 - INFO - [3285868171.py:1] - Instanciando el modelo GradientBoostingRegressor...\n",
      "2025-04-06 18:44:10,100 - INFO - [3285868171.py:5] - Modelo instanciado correctamente.\n",
      "2025-04-06 18:44:10,103 - INFO - [3285868171.py:11] - Iniciando entrenamiento del modelo GBR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBR instanciado (3000 est).\n",
      "Entrenando GBR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:44:11,126 - ERROR - [3285868171.py:22] - Error de valor durante el entrenamiento: Input X contains NaN.\n",
      "GradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error entrenamiento: Input X contains NaN.\n",
      "GradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m      \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m     25\u001b[0m          \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosible causa: NaNs en input.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     28\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError inesperado durante el entrenamiento del modelo GBR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m start_train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mgbr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_train_main\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     end_train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     18\u001b[0m     training_duration \u001b[38;5;241m=\u001b[39m end_train_time \u001b[38;5;241m-\u001b[39m start_train_time\n",
      "File \u001b[1;32md:\\Projects\\TFM - Crypto\\crypto_trading\\tfmenv\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\TFM - Crypto\\crypto_trading\\tfmenv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:416\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_state()\n\u001b[0;32m    412\u001b[0m \u001b[38;5;66;03m# Check input\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# Since check_array converts both X and y to the same dtype, but the\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# trees use different types for X and y, checking them separately.\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    418\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m sample_weight_is_none \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    422\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32md:\\Projects\\TFM - Crypto\\crypto_trading\\tfmenv\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Projects\\TFM - Crypto\\crypto_trading\\tfmenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32md:\\Projects\\TFM - Crypto\\crypto_trading\\tfmenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32md:\\Projects\\TFM - Crypto\\crypto_trading\\tfmenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\TFM - Crypto\\crypto_trading\\tfmenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "logging.info(\"Instanciando el modelo GradientBoostingRegressor...\")\n",
    "try:\n",
    "    gbr_model = GradientBoostingRegressor(**best_gbr_params)\n",
    "    print(f\"GBR instanciado ({best_gbr_params['n_estimators']} est).\")\n",
    "    logging.info(\"Modelo instanciado correctamente.\")\n",
    "except TypeError as e:\n",
    "    logging.error(f\"Error al instanciar GBR. Verifica los nombres de los parámetros: {e}\")\n",
    "    print(f\"Error instanciación GBR: {e}\")\n",
    "    raise e\n",
    "\n",
    "logging.info(\"Iniciando entrenamiento del modelo GBR...\")\n",
    "print(\"Entrenando GBR...\")\n",
    "start_train_time = time.time()\n",
    "\n",
    "try:\n",
    "    gbr_model.fit(X_train_main, Z_train_main)\n",
    "    end_train_time = time.time()\n",
    "    training_duration = end_train_time - start_train_time\n",
    "    logging.info(f\"Modelo GBR entrenado exitosamente en {training_duration:.2f} segundos.\")\n",
    "    print(f\"Entrenamiento: {training_duration:.2f}s.\")\n",
    "except ValueError as e:\n",
    "     logging.error(f\"Error de valor durante el entrenamiento: {e}\")\n",
    "     print(f\"Error entrenamiento: {e}\")\n",
    "     if \"Input contains NaN\" in str(e):\n",
    "         print(\"Posible causa: NaNs en input.\")\n",
    "     raise e\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error inesperado durante el entrenamiento del modelo GBR: {e}\")\n",
    "    print(f\"Error inesperado entrenamiento: {e}\")\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    _ = gbr_model.feature_importances_\n",
    "    logging.info(\"Verificación post-entrenamiento: Atributo 'feature_importances_' encontrado.\")\n",
    "except AttributeError:\n",
    "    logging.error(\"Error post-entrenamiento: El modelo no parece haber sido entrenado correctamente (falta 'feature_importances_').\")\n",
    "    print(\"Error: Modelo no parece entrenado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Evaluando el modelo GBR en el conjunto de test...\")\n",
    "print(\"Evaluando en test...\")\n",
    "\n",
    "if not hasattr(gbr_model, 'feature_importances_'):\n",
    "     print(\"Evaluación saltada: modelo no entrenado.\")\n",
    "     logging.error(\"Evaluación saltada porque el modelo no parece entrenado.\")\n",
    "else:\n",
    "    try:\n",
    "        start_pred_time = time.time()\n",
    "        Z_pred = gbr_model.predict(X_test_main)\n",
    "        end_pred_time = time.time()\n",
    "        prediction_duration = end_pred_time - start_pred_time\n",
    "        logging.info(f\"Predicción en test completada en {prediction_duration:.2f} segundos.\")\n",
    "        print(f\"Predicción: {prediction_duration:.2f}s.\")\n",
    "\n",
    "        mse = mean_squared_error(Z_test_main, Z_pred)\n",
    "        mae = mean_absolute_error(Z_test_main, Z_pred)\n",
    "        rmse = math.sqrt(mse)\n",
    "        r2 = r2_score(Z_test_main, Z_pred)\n",
    "\n",
    "        logging.info(f\"Métricas de evaluación en Test:\")\n",
    "        logging.info(f\"  MSE: {mse:.8f}\")\n",
    "        logging.info(f\"  MAE: {mae:.8f}\")\n",
    "        logging.info(f\"  RMSE: {rmse:.8f}\")\n",
    "        logging.info(f\"  R2 Score: {r2:.8f}\")\n",
    "\n",
    "        print(\"Métricas Test:\")\n",
    "        print(f\"  MSE: {mse:.8f}\")\n",
    "        print(f\"  MAE: {mae:.8f}\")\n",
    "        print(f\"  RMSE:{rmse:.8f}\")\n",
    "        print(f\"  R2: {r2:.8f}\")\n",
    "\n",
    "        print(\"Comparación Log Test (n_splits=3):\")\n",
    "        print(f\"  MSE : Log=0.01291 | Calc={mse:.5f}\")\n",
    "        print(f\"  MAE : Log=0.08891 | Calc={mae:.5f}\")\n",
    "        print(f\"  RMSE: Log=0.11362 | Calc={rmse:.5f}\")\n",
    "        print(f\"  R2  : Log=0.96424 | Calc={r2:.5f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error durante la evaluación del modelo GBR: {e}\")\n",
    "        print(f\"Error evaluación: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Calculando y mostrando la importancia de las características...\")\n",
    "print(\"Importancia Características:\")\n",
    "\n",
    "if not hasattr(gbr_model, 'feature_importances_'):\n",
    "     print(\"Importancia no calculada: modelo no entrenado.\")\n",
    "     logging.error(\"Cálculo de importancia saltado porque el modelo no parece entrenado.\")\n",
    "else:\n",
    "    try:\n",
    "        importances = gbr_model.feature_importances_\n",
    "        feature_names = X_train_main.columns\n",
    "\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        })\n",
    "\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        logging.info(f\"Importancia de características calculada para {len(feature_names)} características.\")\n",
    "\n",
    "        n_top_features = 30\n",
    "        print(f\"Top {n_top_features} Características:\")\n",
    "        print(feature_importance_df.head(n_top_features).to_string())\n",
    "\n",
    "        csv_filename_importance = 'gbr_feature_importances.csv'\n",
    "        try:\n",
    "            feature_importance_df.to_csv(csv_filename_importance, index=False)\n",
    "            logging.info(f\"Importancia de todas las características guardada en '{csv_filename_importance}'\")\n",
    "            print(f\"Importancias guardadas: '{csv_filename_importance}'\")\n",
    "        except Exception as e_csv:\n",
    "            logging.error(f\"No se pudo guardar el CSV de importancias: {e_csv}\")\n",
    "            print(f\"Error guardando CSV importancias: {e_csv}\")\n",
    "\n",
    "    except AttributeError:\n",
    "        msg = \"El modelo entrenado no tiene el atributo 'feature_importances_'. Esto es inesperado.\"\n",
    "        logging.error(msg)\n",
    "        print(msg)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inesperado al obtener la importancia de las características: {e}\")\n",
    "        print(f\"Error inesperado importancia: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'best_gradient_boosting_regressor_3000est.joblib'\n",
    "logging.info(f\"Intentando guardar el modelo entrenado en '{model_filename}'...\")\n",
    "print(f\"Guardando modelo: '{model_filename}'...\")\n",
    "\n",
    "if 'gbr_model' in locals() and hasattr(gbr_model, 'predict'):\n",
    "    try:\n",
    "        joblib.dump(gbr_model, model_filename)\n",
    "        logging.info(\"Modelo guardado exitosamente.\")\n",
    "        print(\"Modelo guardado.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar el modelo en '{model_filename}': {e}\")\n",
    "        print(f\"Error guardando modelo: {e}\")\n",
    "else:\n",
    "     msg = \"El objeto 'gbr_model' no existe o no está entrenado. No se puede guardar.\"\n",
    "     logging.error(msg)\n",
    "     print(f\"Error: {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optional_cv = False\n",
    "\n",
    "if run_optional_cv:\n",
    "    logging.info(\"Iniciando re-evaluación opcional con Cross-Validation (n_splits=3)...\")\n",
    "    print(\"Ejecutando CV opcional (n_splits=3)...\")\n",
    "\n",
    "    n_splits_cv = 3\n",
    "    cv_splitter = TimeSeriesSplit(n_splits=n_splits_cv)\n",
    "\n",
    "    gbr_cv_model = GradientBoostingRegressor(**best_gbr_params)\n",
    "    logging.info(\"Instancia de GBR para CV creada.\")\n",
    "\n",
    "    scoring_reg = {\n",
    "        'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "        'mae': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "        'rmse': make_scorer(lambda y, p: np.sqrt(mean_squared_error(y, p)), greater_is_better=False),\n",
    "        'r2': 'r2'\n",
    "    }\n",
    "    logging.info(f\"Scoring para CV definido: {list(scoring_reg.keys())}\")\n",
    "\n",
    "    try:\n",
    "        start_cv_time = time.time()\n",
    "        cv_results = cross_validate(gbr_cv_model, X_train_main, Z_train_main,\n",
    "                                    cv=cv_splitter, scoring=scoring_reg,\n",
    "                                    n_jobs=-1, return_train_score=False,\n",
    "                                    error_score='raise')\n",
    "        end_cv_time = time.time()\n",
    "        cv_duration = end_cv_time - start_cv_time\n",
    "        logging.info(f\"Cross-Validation completada en {cv_duration:.2f} segundos.\")\n",
    "        print(f\"CV completada: {cv_duration:.2f}s.\")\n",
    "\n",
    "        print(f\"Resultados CV Mean (n_splits={n_splits_cv}):\")\n",
    "        cv_scores_summary = {}\n",
    "        log_cv_comparison = {}\n",
    "\n",
    "        for metric_name in scoring_reg.keys():\n",
    "            cv_key = f'test_{metric_name}'\n",
    "            if cv_key in cv_results:\n",
    "                scores = cv_results[cv_key]\n",
    "                if metric_name in ['mse', 'mae', 'rmse']:\n",
    "                    scores = -scores\n",
    "\n",
    "                mean_score = np.nanmean(scores) if np.any(~np.isnan(scores)) else np.nan\n",
    "                std_score = np.nanstd(scores) if np.any(~np.isnan(scores)) else np.nan\n",
    "\n",
    "                cv_scores_summary[metric_name] = mean_score\n",
    "                cv_scores_summary[metric_name + '_std'] = std_score\n",
    "                print(f\"  {metric_name.upper()}: {mean_score:.8f} (std: {std_score:.8f})\")\n",
    "                logging.info(f\"  CV {metric_name}: {mean_score:.8f} +/- {std_score:.8f}\")\n",
    "            else:\n",
    "                 logging.warning(f\"Métrica CV '{cv_key}' no encontrada en los resultados.\")\n",
    "                 print(f\"  {metric_name.upper()}: No encontrada\")\n",
    "\n",
    "        log_cv_values = {'mse': 0.01619, 'mae': 0.10118, 'rmse': 0.12632, 'r2': 0.95523}\n",
    "        print(\"Comparación Log CV (n_splits=3):\")\n",
    "        for metric, log_val in log_cv_values.items():\n",
    "             calc_val = cv_scores_summary.get(metric, np.nan)\n",
    "             print(f\"  {metric.upper()}: Log={log_val:.5f} | Calc={calc_val:.5f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error durante el Cross-Validation opcional: {e}\")\n",
    "        print(f\"Error CV opcional: {e}\")\n",
    "\n",
    "else:\n",
    "    logging.info(\"Saltando re-evaluación opcional con Cross-Validation.\")\n",
    "    print(\"CV opcional desactivada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Script finalizado.\")\n",
    "print(\"===========================\")\n",
    "print(\"Proceso completado.\")\n",
    "print(\"===========================\")\n",
    "print(f\"Modelo: {model_filename if 'model_filename' in locals() else 'No guardado'}\")\n",
    "print(f\"Log: {LOG_FILENAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
